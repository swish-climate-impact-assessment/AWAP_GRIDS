#+TITLE:AWAP GRIDS overview 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage{verbatim}
-----

* Introduction
NCEPH holds Australian Bureau of Meteorology data for all stations from 1990 to 2010 \cite{NationalClimateCentreoftheBureauofMeteorology:2005}.
The aim of this project is to download the Australian Water Availability Project (AWAP) gridded datasets \cite{Jones2009}.  In particular we want the vapour pressure data from 2010 so that we don't have to buy it again.  We want to compare it with the station data to see if they are close.
** init code
#+name:R-init-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  source('~/tools/disentangle/src/newnode.r')
  nodes <- newnode(name='main.r', newgraph = T,
   inputs = 'R-init')
  
#+end_src

#+name: R-init
#+begin_src R  :session *shell* :exports none :eval no :tangle run/overview.r
   
  rootdir <- getwd()
  wd <- '~/data/AWAP_GRIDS'
  # source(dir('run',pattern = 'tools', full.names=T))
  # file.remove(dir('run',full.names=T))
  setwd(wd)
  dir()
  
  
#+end_src
** TODO deprecated init code
#dbx <- 'I:/My Dropbox/data/AWAP_GRIDS'
#source('~/my dropbox/tools/transformations.r')
# I? or maybe actually want to use c drive for large data downloads on work PC?
#file.copy(file.path(dbxwd,'overview.r'), file.path(wd,'overview.r'), overwrite=T)
#dir.create('run')
** go
#+name:go-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  
  nodes <- newnode(name='go',
   inputs='main.r')
  
#+end_src


#+name: go
#+begin_src R  :session *shell* :exports none :eval no :tangle run/go.r
  source(dir('run',pattern = 'tools', full.names=T))
  # source(dir('run',pattern = 'load', full.names=T))
  # source(dir('run',pattern = 'check', full.names=T))
      
#+end_src



I am also testing the use of R in Emacs orgmode \cite{Schulte}.
* Tests
#+name: R-test
#+begin_src R  :session *R* :tangle run/overview.r :exports results :eval no
  print('helloworld2')
  
#+end_src

#+results: R-test
: helloworld2
=helloworld

* Tools
** tools
#+name:R-tools-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='tools', clearpage = F, ttype='report', nosectionheading = T,
 i=c('go', 'vars', 'get_data_range', 'read.asciigrid2','grid2csv'),
 o = 'tools',append = T,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

#+name: R-tools
#+begin_src R :session *shell* :tangle run/tools.r :exports none :eval no
  setwd('~/data/AWAP_GRIDS')
  if(!require(maptools))
    install.packages('maptools',repos='http://cran.csiro.au'); require(maptools)
  if(!require(uncompress))
    install.packages('uncompress',repos='http://cran.csiro.au'); require(uncompress)
  source('./run/connect2postgres.r')
  delphe <- connect2postgres(hostip='115.146.94.209',user='gislibrary',db='pgisdb')
  # uncomment in run file so that can be sourced?
  source('./run/load2postgres.r')
  source('~/tools/disentangle/src/df2ddi.r')
  
#+end_src

** vars
\begin{comment}
this is a test
\end{comment}
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='variable names', clearpage = F, ttype='report', nosectionheading = T,
 o = 'vars',
 notes='
 At the BoM website the urls for our files can be like the following combinations:
 rain                http://www.bom.gov.au/web03/ncc/www/awap/   rainfall/totals/daily/    grid/0.05/history/nat/2010120120101201.grid.Z
 tmax                http://www.bom.gov.au/web03/ncc/www/awap/   temperature/maxave/daily/ grid/0.05/history/nat/2012020620120206.grid.Z
 tmin                http://www.bom.gov.au/web03/ncc/www/awap/   temperature/minave/daily/ grid/0.05/history/nat/2012020620120206.grid.Z
 vapour pressure 9am http://www.bom.gov.au/web03/ncc/www/awap/   vprp/vprph09/daily/       grid/0.05/history/nat/2012020620120206.grid.Z
 vapour pressure 3pm http://www.bom.gov.au/web03/ncc/www/awap/   vprp/vprph15/daily/       grid/0.05/history/nat/2012020620120206.grid.Z
 solar               http://www.bom.gov.au/web03/ncc/www/awap/   solar/solarave/daily/     grid/0.05/history/nat/2012020720120207.grid.Z
 NDVI                http://reg.bom.gov.au/web03/ncc/www/awap/   ndvi/ndviave/month/       grid/history/nat/2012010120120131.grid.Z
 ',echoCode = FALSE,
 code=NA)



#+end_src
#+name: R-vars
#+begin_src R :session *R* :tangle run/tools.r :exports results :eval no
 vars<-c('variable,measure,timestep
 rainfall,totals,daily
 temperature,maxave,daily
 temperature,minave,daily
 vprp,vprph09,daily
 vprp,vprph15,daily
 solar,solarave,daily
 ndvi,ndviave,month
 ')
 vars<-read.csv(textConnection(vars))
#+end_src

#+results: R-vars
| rainfall    | totals   | daily |
| temperature | maxave   | daily |
| temperature | minave   | daily |
| vprp        | vprph09  | daily |
| vprp        | vprph15  | daily |
| solar       | solarave | daily |
| ndvi        | ndviave  | month |
** get data range
A function to get the data for a met variable on a day is called for a range of dates
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='get data range', clearpage = F, ttype='report', nosectionheading = T,
 o = c('get_data_range'),i='get_data',
 notes='',echoCode = FALSE,
 code=NA)



#+end_src
#+name:get_data_range
#+begin_src R :session *R* :tangle run/tools.r :exports none :eval no
# newnode get_data
 get_data<-function(variable,measure,timestep,startdate,enddate){
  url="http://www.bom.gov.au/web03/ncc/www/awap/{variable}/{measure}/{timestep}/grid/0.05/history/nat/{startdate}{enddate}.grid.Z" 
  url=gsub("{variable}",variable,url,fixed=TRUE)
  url=gsub("{measure}",measure,url,fixed=TRUE)
  url=gsub("{timestep}",timestep,url,fixed=TRUE)
  url=gsub("{startdate}",startdate,url,fixed=TRUE)
  url=gsub("{enddate}",enddate,url,fixed=TRUE)
  download.file(url,sprintf("%s_%s%s.grid.Z",measure,startdate,enddate),mode="wb")
  }

# newnode get_data_range
 get_data_range<-function(variable,measure,timestep,startdate,enddate){
  thisdate<-startdate
  while (thisdate<=enddate){
   get_data(variable,measure,timestep,format(as.POSIXct(thisdate),"%Y%m%d"),format(as.POSIXct(thisdate),"%Y%m%d"))
   thisdate<-thisdate+as.double(as.difftime(1,units="days"),units="secs")
   }
 }
#+end_src
** read asciigrid2
A function to get the data from a grid to a long table
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='read.asciigrid2', clearpage = F, ttype='report', nosectionheading = T,
 o = c('read.asciigrid2'),
 notes='',echoCode = FALSE,
 code=NA)
#+end_src
#+name:read.asciigrid2
#+begin_src R :session *R* :tangle run/tools.r :exports none :eval no
#Modified from maptools package
#Reads only the specified number of data items, ignoring BOM's wierd footer
 read.asciigrid2<-function (fname, as.image = FALSE, plot.image = FALSE, colname = fname, proj4string = CRS(as.character(NA))) {
  t = file(fname, "r")
  l5 = readLines(t, n = 6)
  l5s = strsplit(l5, "\\s+", perl = T)
  xllcenter = yllcenter = xllcorner = yllcorner = as.numeric(NA)
  for (i in 1:6) {
     fieldname = casefold(l5s[[i]][1])
     if (length(grep("ncols", fieldname))) 
         ncols = as.numeric(l5s[[i]][2])
     if (length(grep("nrows", fieldname))) 
         nrows = as.numeric(l5s[[i]][2])
     if (length(grep("xllcorner", fieldname))) 
         xllcorner = as.numeric(l5s[[i]][2])
     if (length(grep("yllcorner", fieldname))) 
         yllcorner = as.numeric(l5s[[i]][2])
     if (length(grep("xllcenter", fieldname))) 
         xllcenter = as.numeric(l5s[[i]][2])
     if (length(grep("yllcenter", fieldname))) 
         yllcenter = as.numeric(l5s[[i]][2])
     if (length(grep("cellsize", fieldname))) 
         cellsize = as.numeric(l5s[[i]][2])
     if (length(grep("nodata_value", fieldname))) 
         nodata.value = as.numeric(l5s[[i]][2])
 }
 if (is.na(xllcorner) && !is.na(xllcenter)) 
     xllcorner = xllcenter - 0.5 * cellsize
 else xllcenter = xllcorner + 0.5 * cellsize
 if (is.na(yllcorner) && !is.na(yllcenter)) 
     yllcorner = yllcenter - 0.5 * cellsize
 else yllcenter = yllcorner + 0.5 * cellsize
 map = scan(t, as.numeric(0), quiet = TRUE,nmax=nrows*ncols)
 close(t)
 if (length(as.vector(map)) != nrows * ncols) 
     stop("dimensions of map do not match that of header")
 map[map == nodata.value] = NA
 if (as.image) {
     img = matrix(map, ncols, nrows)[, nrows:1]
     img = list(z = img, x = xllcorner + cellsize * ((1:ncols) - 
         0.5), y = yllcorner + cellsize * ((1:nrows) - 0.5))
     if (plot.image) {
         image(img, asp = 1)
         return(invisible(img))
     }
     else return(img)
 }
 df = data.frame(map)
 names(df) = colname
 grid = GridTopology(c(xllcenter, yllcenter), rep(cellsize, 
     2), c(ncols, nrows))
 SpatialGridDataFrame(grid, data = df, proj4string = proj4string)
 }
#+end_src

** grid2csv
#+name:grid2csv-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='grid2csv', clearpage = F, ttype='report', nosectionheading = T,
 o = 'grid2csv',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

#+name:grid2csv
#+begin_src R :session *R* :tangle run/tools.r :exports none :eval no
# filename must be in format generated by get_data: variable_{startdate}{enddate}
 grid2csv<-function(filename){
	variable<-strsplit(filename,"_")[[1]][1]
	year<-as.numeric(substr(strsplit(filename,"_")[[1]][2],1,4))
	month<-as.numeric(substr(strsplit(filename,"_")[[1]][2],5,6))
	day<-as.numeric(substr(strsplit(filename,"_")[[1]][2],7,8))
	csv_filename<-sub("grid","csv",filename)
	d<-read.asciigrid2(filename)
	#image(d)
	e<-as.data.frame(d)
	names(e)<-c(variable,"long","lat")
	e$year<-year
	e$month<-month
	e$day<-day
	write.csv(e,csv_filename,row.names=FALSE,na="")
 }

#+end_src

* Load
DONE IN dlAWAP_GRIDS.r on ncephMacPro
** zip as nceph (used GUI) and load to brains /ResearchData/AWAP_GRIDS
#+name:load to brains
#+begin_src sh :session *shell* :tangle no :exports none :eval no
################################################################
# name:load to brains
ssh root@115.146.93.225
cd /home/ResearchData
mkdir AWAP_GRIDS
chmod 0777 /home/ResearchData/AWAP_GRIDS
# from local
scp temperature.zip root@115.146.93.225:/home/ResearchData/AWAP_GRIDS
# on remove server
unzip temperature.zip
#+end_src

** DEPRECATED
*** TASKS
**** TODO SET UP TO DO YEAR/MONTH
*** download
#+name:download-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  newnode(dsc='download', clearpage = F, ttype='report', nosectionheading = T,
   o = 'data/{year}/{month}', i=c('tools', 'foundMissings'),
   notes='',echoCode = FALSE,
   code=NA)
#+end_src

#+name: download
#+begin_src R :session *shell* :tangle run/load.r :exports none :eval no
    dir.create('data')      
    setwd('data')
    # tmax
    # i <- 2
    # # vars[i,]
    # get_data_range(variable=vars[i,1],measure =vars[i,2],timestep=vars[i,3],startdate=as.POSIXct("2010-01-30"), enddate=as.POSIXct("2010-01-31"))
    # # vp DONT DO TOO MANY DOWNLOADS, PERHAPS A YEAR/MONTH AT A TIME, THEN CONVERTS/DELETES, THEN MORE DOWNLOADS
  
  
  yy <- '2010'
  leapyear<- ifelse( yy %in% c('1988', '1992', '1996', '2000', '2004', '2008', '2012'), T, F)
  # http://en.wikipedia.org/wiki/List_of_leap_years
  dir.create(yy)
  setwd(yy)
  strt=Sys.time()
  for(mm in as.character(1)){
   print(mm)
  # mm <- as.character(1)
   dir.create(mm)
   setwd(mm)
   for(i in 4:5){
   # i <- 5
   variable<-gsub(' ','',vars[i,1])
   measure<-gsub(' ','',vars[i,2])
   timestep<-gsub(' ','',vars[i,3])
   maxdate <- ifelse(mm %in% c(9,4,6,11), 30, 31)
   if(mm == 2 & leapyear == F){maxdate <- 28}
   if(mm == 2 & leapyear == T){maxdate <- 29}
   get_data_range(variable=variable,measure =measure,timestep=timestep,
    startdate=as.POSIXct(paste(yy,"-",mm,"-01",sep="")),
    enddate=as.POSIXct(paste(yy,"-",mm,"-",maxdate,sep=""))
    )
   }
   setwd(file.path(wd,"data",yy))
  }
  setwd(file.path(wd,"data"))
  end=Sys.time()
  print(end-strt)
  
 #+end_src
*** uncompress and transform to long csv

#+name:uncompress-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='uncompress-newnode', clearpage = F, ttype='report', nosectionheading = T,
 i='data/{year}/{month}', o = c('grids','csvs'),append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

#+name: uncompress
#+begin_src R :session *shell* :tangle run/load.r :exports results :eval no
  setwd(file.path(wd,'data',yy))
  strt=Sys.time()
  for(mm in c(1)){
  # mm <- '1'
  mm <- as.character(mm)
  print(mm)
  setwd(mm)
  files <- dir(pattern='.grid.Z')
  
  for (f in files) {
  # f <- files[1]
   print(f)
   handle <- file(f, "rb")
   data <- readBin(handle, "raw", 99999999)
   close(handle)
   uncomp_data <- uncompress(data)
   handle <- file(gsub('.Z','',f), "wb")
   writeBin(uncomp_data, handle)
   close(handle)
   # newnode convert to long csvfor (f in dir(pattern=".grid$")) {
   grid2csv(gsub('.Z','',f))
   # clean up
   file.remove(f)
   }
  setwd(file.path(wd,'data'))
  }
  endd=Sys.time()
  print(endd-strt)
  # 49 sec
  setwd(wd)
  
#+end_src
* CHECK
#+name:check-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='check', clearpage = F, ttype='report', nosectionheading = T,
 i='grids', o = 'fig1.jpg',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

#+name:check
#+begin_src R :session *shell* :tangle run/check.r :exports none :eval no
  # newnode CHECK 
  # newnode check grid
  files <- dir('data', pattern='.grid')
  f <- files[1]
  print(f)
  # to select a differnt one
    
  d <- read.asciigrid2(file.path('data',f))
  str(d)
  # compare with http://www.bom.gov.au/jsp/awap/vprp/archive.jsp?colour=colour&map=vprph15&year=2010&month=12&day=30&period=daily&area=nat
  # far out that colour scheme is dodgy!
  image(d, col = rainbow(19))
  dev.copy(jpeg, 'fig1.jpg')
  dev.off()
  # newnode check csv
  read.table(file.path('data',sub("grid","csv",f)), nrows = 10, sep=',', header=T)
    
  
#+end_src
The grid for a particular day is shown in \ref{fig:fig1.jpg}
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{fig1.jpg}
\caption{fig1.jpg}
\label{fig:fig1.jpg}
\end{figure}
* deprecated Load csv to delphe
** tested Rpostgresql, decide to use COPY instead
#+name:loadCsv2delphe-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='loadCsv2delphe test', clearpage = F, ttype='transformations', nosectionheading = T,
 i='csvs',o = 'test. too slow',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

#+name:loadCsv2delphe
#+begin_src R :session *R* :tangle run/load.r :exports none :eval no
 dir()
 dbSendUpdate(delphe,
 'CREATE TABLE awap_grids.vprph_master (
 lat numeric,
 long numeric,
 yy int4,
 mm int4,
 dd int4,
 hh int4,
 val numeric,
 constraint vprph_master_pkey primary key (lat, long, yy, mm, dd, hh)
 )
 ')
 
 files <- dir('data', pattern='.csv')
 f <- files[1]
 print(f)
 # to select a differnt one
   
 d <- read.csv(file.path('data',f))
 st <- Sys.time()
 dbWriteTable(delphe, 'awap_grids_indat',d)
 en <- Sys.time()
 print(en-st)  
 # 20 mins
#+end_src

** use COPY instead
*** TASKS
**** TODO REMOVE newnode add grids?
*** code
#+name:loadCsv2delpheUsingCOPY-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  
  newnode(dsc='loadCsv2delpheUsingCOPY-newnode', clearpage = F, ttype='transformations', nosectionheading = T,
   i ='csvs', o = c('awap_grids.vprph_master','check4duplicates','check4missings'),append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA, TASK = '
  ,**** TODO NEED TO REMOVE GRID POLYGONS
   ')
#+end_src

#+name:loadCsv2delpheUsingCOPY
#+begin_src R :session *R* :tangle run/load.r :exports none :eval no
       
   setwd('data')
   # mm <- '1'
   setwd(mm)
   load2postgres(gsub('.grid','.csv',f),'awap_grids','awap_grids_indat', pguser='ivan_hanigan',db='delphe',ip='130.56.102.41')
   # this creates the file sqlquery.txt and should be passed to the psql.exe with COPY
   # but firest make a table for it to go into
   yy <- '2011'
   dbSendUpdate(delphe,
   # cat(
   paste('CREATE TABLE awap_grids.vprph_',yy,' (
   lat numeric,
   long numeric,
   yy int4,
   mm int4,
   dd int4,
   hh int4,
   val numeric,
   constraint vprph_',yy,'_pkey primary key (lat, long, yy, mm, dd, hh),
   constraint month_range check (yy = ',yy,')
   )
   INHERITS (awap_grids.vprph_master)
   ',sep='')
   )
   # test the copy and insert
  
  
   st <- Sys.time()
   shell(paste("type sqlquery.txt \"",gsub('.grid','.csv',f),"\" | \"i:\\my dropbox\\tools\\pgutils\\psql\" -h 130.56.102.41 -U ivan_hanigan -d delphe",sep=""))
   en <- Sys.time()
   print(en-st)    
     # # 9 sec from work pc, 3.4 mins over vpn, remember to make pgadmin remember your password
   # unfortunatly emacs nogo with the shell() bit of this so done in plain R console
  
   # newnode subset to gridcells that have stations
   # first make station grid
   dbSendUpdate(delphe,"select long, lat into awap_grids.awap_grid_05 from awap_grids.awap_grids_indat")
   dbGetQuery(delphe,"SELECT AddGeometryColumn(\'awap_grids\', \'awap_grid_05\', \'the_geom\', 4283, \'POLYGON\', 2);")
   # newnode add grid
  # TASK THIS SEEMS TO HAVE CREATED THE WRONG GRID LINES.  MIGHT DELETE THIS?
  '
  ,**** TODO TASK remove grid
  '
   dbSendUpdate(delphe,
   "UPDATE awap_grids.awap_grid_05 SET the_geom=GeomFromText('POLYGON((
   '|| long-0.05 || ' '|| lat-0.05 ||',
   '|| long-0.05 || ' '|| lat+0.05 ||',
   '|| long+0.05 || ' '|| lat+0.05 ||',
   '|| long+0.05 || ' '|| lat-0.05 ||',
   '|| long-0.05 || ' '|| lat-0.05 ||'
   ))' ,4283);
   alter table awap_grids.awap_grid_05 add column gid serial primary key;")
   dbSendUpdate(delphe,'grant select on awap_grids.awap_grid_05 to public_group')
   dbSendUpdate(delphe,
    'ALTER TABLE awap_grids.awap_grid_05 ALTER COLUMN the_geom SET NOT NULL;
    CREATE INDEX awap_grid_05_index on awap_grids.awap_grid_05 using GIST(the_geom);
    ALTER TABLE awap_grids.awap_grid_05 CLUSTER ON awap_grid_05_index;
    ')
   # realise that contains and within return multiple grid cells, maybe because of polygon?  make point tools
   points_to_geom_query(schema='awap_grids',tablename='awap_grid_05',col_lat='lat',col_long='long')
   dbSendUpdate(delphe,
    "SELECT AddGeometryColumn('awap_grids', 'awap_grid_05', 'the_geom_pt', 4283, 'POINT', 2);
    ALTER TABLE awap_grids.awap_grid_05 ADD CONSTRAINT geometry_valid_check CHECK (isvalid(the_geom_pt));
  
          UPDATE awap_grids.awap_grid_05
          SET the_geom_pt=GeomFromText(
                  'POINT('||
                  long ||
                  ' '||
                  lat ||')'
                  ,4283);
                                  ")
   # dbSendUpdate(delphe,'drop table awap_grids.awap_grid_05_stns')
   dbSendUpdate(delphe,'
    select distinct t1.long, t1.lat, t1.the_geom, t1.the_geom_pt
    into awap_grids.awap_grid_05_stns
    from awap_grids.awap_grid_05 t1,
    weather_bom.combstats t2
    where st_contains(t1.the_geom,t2.the_geom);
    alter table awap_grids.awap_grid_05_stns add column gid serial primary key;
    ALTER TABLE awap_grids.awap_grid_05_stns ALTER COLUMN the_geom SET NOT NULL;
    CREATE INDEX awap_grid_05_stns_index on awap_grids.awap_grid_05_stns using GIST(the_geom);
    ALTER TABLE awap_grids.awap_grid_05_stns CLUSTER ON awap_grid_05_stns_index;
    ')
  
  
  
  
  
   # newnode now do the bulk uploads (via Rconsole, not ess which hates shell)
   setwd(file.path(wd,'data',yy))
   st <- Sys.time()
   for(mm in c(1)){
    # mm <- '3'
    mm <- as.character(mm)
    print(mm)
    setwd(mm)
   # mm <- '1'
   # setwd(mm)
    files <- dir(pattern='.csv')
   f <- files[1]
   load2postgres(gsub('.grid','.csv',f),'awap_grids','awap_grids_indat', pguser='ivan_hanigan',db='delphe',ip='130.56.102.41')
  
  
   for(hh in c('09','15')){
    # hh = '09'
    filesi <- files[grep(paste('vprph',hh,sep=''),files)]
    for(filei in filesi){
  #  filei <- filesi[1]
     print(filei)
     
     shell(paste("type sqlquery.txt \"",filei,"\" | \"i:\\my dropbox\\tools\\pgutils\\psql\" -h 130.56.102.41 -U ivan_hanigan -d delphe",sep=""))
     
     dbSendUpdate(delphe, 
     # cat(
     paste("INSERT INTO awap_grids.vprph_",yy," (lat,long ,yy ,mm ,dd , hh, val)
     SELECT t1.lat, t1.long, year, month, day, '",hh,"', vprph09
     FROM awap_grids.awap_grids_indat t1
     right join awap_grids.awap_grid_05_stns t2
     on t1.long = t2.long and t1.lat = t2.lat 
     ",sep="")
     )
     dbRemoveTable(delphe, 'awap_grids.awap_grids_indat')
  
     # TODO drop all pixels with no stations before insert?
     # TODO vacuum database after each loop?  or every 100?
    }
   }
   setwd(file.path(wd,'data')) 
   }
   en <- Sys.time()
   print(en-st)  
   setwd(file.path(wd))
#+end_src
** check for duplicates
*** TASKS
**** TODO insert BoM response about duplicates in January
**** TODO build a test function to check for this at download
*** code
This will be code
#+name:check4duplicates
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='check4duplicates', clearpage = F, ttype='transformations', nosectionheading = T,
 i = 'check4duplicates',
 o='response by bom',
 append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src
** check for missing days
*** TASKS
**** TODO check4missings
*** code
This will be code
#+name:check4missings
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
newnode(dsc='check4missings', clearpage = F, ttype='transformations', nosectionheading = T,
 i = 'check4missings',
 o='foundMissings',
 append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

* check against a station
Now we can select a timeseries of values for both a pixel and a station and see how well they correspond. 
** TASKS
*** TODO join the station and grid query to one query
   SCHEDULED: <2012-02-15 Wed 14:20>
*** TODO calcute RMSE and R2 for August only
*** TODO change the avg(val) to a IDW based on the cell centres
** code
#+name:checkAstation-newnode
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  
  newnode(dsc='checkAstation-newnode', clearpage = F, ttype='transformations', nosectionheading = T,
   o = c('fig2.jpg','checkAstation'),i='awap_grids.vprph_master',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
#+end_src

#+name:checkAstation
#+begin_src R :session *R* :tangle run/check.r :exports none :eval no
  d<-dbGetQuery(delphe,
   'SELECT  name, year, month, day, hour, "timestamp" ,     t2.lat ,     lon,
         vapour_pressure_in_hpa
    FROM weather_bom.bom_3hourly_data_2010 join weather_bom.combstats t2
    on station_number = stnum
    where station_number = 70014
    and month  =8 and (hour = 9 or hour = 15)
    order by day, hour
   ')
   d
   str(d)
   with(d, plot(as.POSIXct(timestamp), vapour_pressure_in_hpa, type='b',pch=16))
   
   d2 <- dbGetQuery(delphe,
    "SELECT  stnum, yy as year, mm as month, dd as day, hh as hour, 
        to_timestamp(yy || '-' || mm || '-' || dd || ' ' || hh || ':' || 0, 'YYYY-MM-DD HH24:MI') as timestamp2,
        avg(val) as vprph
    FROM awap_grids.vprph_master tab1
        join 
        (       
        select t2.stnum, t1.*
        from awap_grids.awap_grid_05_stns t1,
        (select * from weather_bom.combstats where stnum = 70014) t2
        where st_contains(t1.the_geom,st_centroid(t2.the_geom))
        ) tab2
        on tab1.long = tab2.long and tab1.lat = tab2.lat
        group by stnum, yy, mm, dd, hh
    order by yy, mm , dd, hh
    ")
        d2
        with(d2, lines(as.POSIXct(timestamp2), vprph, type='b',pch=16,col='red'))
   
        d3 <- merge(d,d2, all=T)
        with(d3, plot(vprph, vapour_pressure_in_hpa,xlim=c(3,10),ylim=c(3,10)))
        lines(abline(0,1))
        dev.copy(jpeg,'fig2.jpg')
        dev.off()
  
  # newnode IDW
  dbGetQuery(delphe,'select * from weather_bom.combstats where stnum = 70014')
  d3 <- dbGetQuery(delphe,
   'select *,
    st_distance(
     t1.the_geom, 
     t2.the_geom_pt
    ) as distances        
    from awap_grids.awap_grid_05_stns t2,
    (select * from weather_bom.combstats where stnum = 70014) t1
    where st_distance(
     t1.the_geom, 
     t2.the_geom_pt
     ) <= 0.05
   order by distances desc
   ')
  d3[,c(1:2,5:10,13)]
  d4 <- dbGetQuery(delphe,
   "select stnum, name, table2.yy as year, mm as month, dd as day, hh as hour,
   to_timestamp(yy || '-' || mm || '-' || dd || ' ' || hh, 'YYYY-MM-DD HH24') as timestamp2,
   sum(table2.val * (1/(table1.distances^2))) / sum(1/(table1.distances^2)) as weighted_data 
   from
   (
    select stnum, name, t2.*,
    st_distance(
     t1.the_geom, 
     t2.the_geom_pt
    ) as distances        
    from awap_grids.awap_grid_05_stns t2,
    (select * from weather_bom.combstats where stnum = 70014) t1
    where st_distance(
     t1.the_geom, 
     t2.the_geom_pt
     ) <= 0.05
    ) table1
   join awap_grids.vprph_master as table2
   on table1.long = table2.long and
      table1.lat = table2.lat
   group by table1.stnum,name,table2.yy, mm, dd, hh, to_timestamp(yy || '-' || mm || '-' || dd || ' ' || hh , 'YYYY-MM-DD HH24')
   order by yy, mm, dd, hh
   ")
  str(d4)
  head(d4)
  head(d)
  with(d4, plot(weighted_data, type='b',pch=16))
  d5 <- merge(d,d4)
  with(d5, plot(weighted_data,  vapour_pressure_in_hpa,xlim=c(3,10),ylim=c(3,10)))
  lines(abline(0,1))
  dev.copy(jpeg, res = 150,'fig2.jpg')
  dev.off();dev.off()  
  
#+end_src
The association of the grid and station data for a particular station is shown in \ref{fig:fig2.jpg}
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{fig2.jpg}
\caption{fig2.jpg}
\label{fig:fig2.jpg}
\end{figure}
\clearpage
* DO
** write function to extract timeseries
#+name:function to extract timeseries
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='function to extract timeseries', clearpage = F, ttype='transformations', nosectionheading = T,
 i = 'awap_grids.vprph_master', o = 'function to extract timeseries',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

** test function
#+name:test function
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='test function', clearpage = F, ttype='transformations', nosectionheading = T,
 i='function to extract timeseries', o = 'test function',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

** publish function
#+name:publish function
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no

newnode(dsc='publish function', clearpage = F, ttype='transformations', nosectionheading = T,
 i = 'test function', o = c('to NCEPH PostGIS wiki','to ivanstools','metadata'),append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)
#+end_src

* References
\bibliographystyle{unsrt}
\bibliography{I:/references/library}
* Metadata
** metadata-init
#+name:metadata-init
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='metadata-init', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'metadata-init',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
  
   source('~/My Dropbox/tools/transformations.r')
#+end_src

#+name:metadata-init-src
#+begin_src R :session *shell* :tangle src/metadata-init-src.r :exports none :eval no
  ################################################################
  # name:metadata-init-src
  
  #   delphe <- connectDelphe('130.56.102.41','ivan_hanigan','delphe')
  
  if(!require(RJDBC)) install.packages('RJDBC'); require(RJDBC)
  drv <- JDBC("oracle.jdbc.driver.OracleDriver",
              '/u01/app/oracle/product/11.2.0/xe/jdbc/lib/ojdbc6.jar')
  ch <- dbConnect(drv,"jdbc:oracle:thin:@130.56.102.54:1521","DDIINDEXDB","trojan9!")
  
  idno <- 'AWAP_GRIDS'
  if(!exists('s')){
  s <- dbGetQuery(oracle, paste("select * from stdydscr where idno = '",idno,"'", sep = ''))
  idno <- s$IDNO
  }
  t(s)
  # newnode get tools
  # rm(oracle)
  if(!exists('oracle')) {source(dir('run',pattern = 'tools', full.names=T))}
  
#+end_src

** insert study id
#+name:insert study id
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='insert study id', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'insert study id',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
   
   dir.create('metadata')
   
   write.table(s,'metadata/stdydscr.csv',sep=',',row.names=F)
#+end_src
** get list of files already entered
#+name:get list of files already entered
#+begin_src R :session *R* :tangle run/metadata-transformations.r:exports none :eval no
  
  newnode(dsc='get list of files already entered', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'get list of files already entered',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
  
   # newnode first get the list of files I had previously entered
   fileDscr <- dbGetQuery(oracle,sprintf(
   "SELECT * 
   FROM filedscr 
   where IDno = '%s' 
   order by FILETYPE
   ",idno))
   head(fileDscr)
   fileDscr[,1:4]
   write.csv(fileDscr,file.path('metadata','filedscr.csv'),row.names=F) 
  
  # newnode get filedscr
  # source(dir('run',pattern = 'metadata_metadata', full.names=T))
  
#+end_src

** add a new file
#+name:add a new file
#+begin_src R :session *R* :tangle run/metadata-transformations.r:exports none :eval no
  
  newnode(dsc='add a new file', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'add a new file',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
  
   f <- add_filedscr(fileid = 1, idno = s$IDNO, ask=T)
   f$FILELOCATION <- '-d delphe -s awap_grids' 
   
   
#+end_src
** data
*** include data desc for file1
#+name:include data desc for file1
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='include data desc for file1', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'include data desc for file1',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
   
   # newnode abs data
   t(fileDscr[2,])
   df <-  dbGetQuery(delphe, 'select * from awap_grids.vprph_master limit 1') 
   d <- add_datadscr(data_frame = df, fileid = 3130, ask=T)
   write.table(d,'metadata/datadscr.csv',sep=',',row.names=F)
  
  
   
#+end_src

*** include data desc for file2
#+name:include data desc for file2
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='include data desc for file2', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'include data desc for file2',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
  
   f$PRODDATEDOCFILE <- NA
   f$PRODUCERDOCFILE <- NA
   f$DESTROYED <- 0
   f <- f[,c('FILEID','IDNO','FILENAME','FILETYPE','PROCSTAT','SPECPERMFILE','DATEARCHIVED','DATEDESTROY','FILEDSCR','FILELOCATION','NOTES','REQID','PUBLISHDDI','BACKUPVALID','DATEBACKUPVALID','CHECKED','BACKUPLOCATION','PRODDATEDOCFILE','PRODUCERDOCFILE','DESTROYED')]
  
   # datadscr
   df <- dbGetQuery(delphe, ' select * from awap_grids.awap_grid_05_stns limit 1')
   d <- add_datadscr(data_frame = df, fileid = 1, ask=T) # might not be correct but will update on insert to oracle
   d
   
  
   write.table(f,'metadata/filedscr.csv',sep=',',row.names=F, col.names=F, append=T)
   write.table(d,'metadata/datadscr.csv',sep=',',row.names=F, col.names=F, append=T)
   
#+end_src

** analysis
** document
*** add metadata for the files
#+name:add metadata for the files
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no

newnode(dsc='add metadata for the files', clearpage = F, ttype='transformations', nosectionheading = T,
 o = 'add metadata for the files',append = T,end_doc = F,
 notes='',echoCode = FALSE,
 code=NA)

 # newnode file1 the final document
 #f <- add_filedscr(fileid = 1, idno = s$IDNO, ask=T)
 #f$FILELOCATION <- 'I:/My Dropbox/projects/1.302 Biomass/Biomass Smoke Project/JAWMA_fire_events' 
 
 
#+end_src
** metadata
*** add metadata for files to oracle
#+name:add metadata for files to oracle
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='add metadata for files to oracle', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'add metadata for files to oracle',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
   
   f<-read.table('metadata/filedscr.csv',as.is=T,sep=',',header=T)
   f2 <- as.data.frame(matrix(nrow = 0, ncol=ncol(f)))
   for(i in 1:nrow(f)){
   f2 <- rbind(f2,as.data.frame(t(unlist(ifelse(is.na(f[i,]),'',f[i,])  ))))
   }
   names(f2) <- names(f)
   f2 
   replaceDDI <- F
   if(replaceDDI == T) { dbSendUpdate(oracle, sprintf("delete from filedscr where idno = '%s'",idno))}
   extant <- dbGetQuery(oracle, sprintf("select * from filedscr where idno = '%s'",idno))
   
   if(nrow(extant) == 0){
    dbWriteTable(oracle, 'NUFILES', f2)
    dbSendUpdate(oracle,
    'insert into ivan.filedscr (IDNO, FILENAME, FILETYPE, PROCSTAT, SPECPERMFILE, DATEARCHIVED, DATEDESTROY, FILEDSCR, NOTES, REQID, PUBLISHDDI, BACKUPVALID, DATEBACKUPVALID, CHECKED, BACKUPLOCATION, FILEID, FILELOCATION)
    select IDNO, FILENAME, FILETYPE, PROCSTAT, SPECPERMFILE, to_date(DATEARCHIVED), DATEDESTROY, FILEDSCR, NOTES, REQID, PUBLISHDDI, BACKUPVALID, to_date(DATEBACKUPVALID), CHECKED, BACKUPLOCATION, FILEID, FILELOCATION from nufiles
    ')
    dbSendUpdate(oracle,'
    drop table nufiles
    ')
  
    } else {
   
    for(i in 1:nrow(f2)){
     #i <- 1
     print(f2$FILENAME[i])
     if(length(grep(f2$FILENAME[i], extant$FILENAME)) != 0) {next}
     dbWriteTable(oracle, 'NUFILES', f2[i,])
     dbSendUpdate(oracle,
     'insert into ivan.filedscr (IDNO, FILENAME, FILETYPE, PROCSTAT, SPECPERMFILE, DATEARCHIVED, DATEDESTROY, FILEDSCR, NOTES, REQID, PUBLISHDDI, BACKUPVALID, DATEBACKUPVALID, CHECKED, BACKUPLOCATION, FILEID, FILELOCATION)
     select IDNO, FILENAME, FILETYPE, PROCSTAT, SPECPERMFILE, to_date(DATEARCHIVED), DATEDESTROY, FILEDSCR, NOTES, REQID, PUBLISHDDI, BACKUPVALID, to_date(DATEBACKUPVALID), CHECKED, BACKUPLOCATION, FILEID, FILELOCATION from nufiles
     ')
     dbSendUpdate(oracle,'
     drop table nufiles
     ')
     }
    }
    
#+end_src

*** add metadata for data to oracle
#+name:add metadata for data to oracle
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='add metadata for data to oracle', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'add metadata for data to oracle',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
   
   
   
   # NOW NEED TO IDENTIFY ID NUMBERS
   dbGetQuery(oracle,paste(
    "
    SELECT IDNO, min(FILEID), max(FILEID) FROM FILEDSCR 
    WHERE IDNO = '",idno,"'
    group by idno
    ", sep='')
    )
  
   # FILEIDS ARE 
  # minfileid <- 3122
  # maxfileid <- 3122
  # fileids <- seq(minfileid,maxfileid)
  
   datarows <- read.csv('metadata/datadscr.csv')
   # need to edit this as I made that fileid up above
   names(table(datarows$FILEID))
   datarows[datarows$FILEID == 1,'FILEID']  <- 3137
   fileids <- names(table(datarows$FILEID))
  for(i in 1:length(names(table(datarows$FILEID)))){
    # i <- 1
    rows <- names(table(datarows$FILEID))[i]
    fid<-fileids[i]
    cat(paste('insert into ivan.datadscr (',
    paste(names(read.csv(dir('metadata',full.names=T)[grep('datadscr.csv',dir('metadata',full.names=T))])),sep='',collapse=', '),
    ')
    
    select ',
    gsub('FILEID',fid,paste(names(datarows),sep='',collapse=', ')),
    ' from nudata
    WHERE FILEID = ',rows,'
    ',
    sep='')
    )
    }
  
  
   
   # upload the data table
   nudata <- read.csv('metadata/datadscr.csv')
   nudata
   dbWriteTable(oracle,'NUDATA', nudata)
   
   dbSendUpdate(oracle,
   'insert into ivan.datadscr (LABL, NOTES, SPECPERMVAR, FILEID)
    
    select LABL, NOTES, SPECPERMVAR, 3130 from nudata
    WHERE FILEID = 3130
  
   ')
   dbSendUpdate(oracle,
   'insert into ivan.datadscr (LABL, NOTES, SPECPERMVAR, FILEID)
    
    select LABL, NOTES, SPECPERMVAR, 3137 from nudata
    WHERE FILEID = 1
  
   ')
   dbSendUpdate(oracle,
   'drop table nudata
   ')
   
#+end_src

*** oracle2xml-makeTex
#+name:oracle2xml-makeTex
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='oracle2xml-makeTex', clearpage = F, ttype='transformations', nosectionheading = T,
   o = 'oracle2xml-makeTex',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA)
   
   setwd('~/My Dropbox/projects/0.3 Catalogue/')
   
   # run I:/My Dropbox/projects/0.3 Catalogue/oracle2xml-makeTex.r 
   setwd(dbx)
  
    doc <- dir(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno,'reports'), pattern = '\\.tex')
    file.copy(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno,'reports',doc), file.path('metadata',gsub('_doc','_metadata',doc)), overwrite = T) 
    # edits = find and replace \subsection with \textbf , remove header and end, paste into keynote, keynode output
  
#+end_src
 
*** create catalogue and ddi xmls
#+name:create catalogue and ddi xmls
#+begin_src R :session *R* :tangle run/metadata-transformations.r :exports none :eval no
  
  newnode(dsc='create catalogue and ddi xmls', clearpage = F, ttype='transformations', nosectionheading = T,
  o = 'create catalogue and ddi xmls',append = T,end_doc = F,
  notes='',echoCode = FALSE,
  code=NA)
  
  setwd('I:/My Dropbox/projects/0.3 Catalogue/')
  setwd(dbx)
  
#+end_src

**** at work
**** at home

# need to pipe it thru delphe for JDBC access 
# to make sure delphe is current oracle
# download csv using the sql query tool
# 'select * from filedscr'
# then upload using
# I:\Dropbox\projects\0.3 Catalogue\load_oracle_of_delphe2postgres.r
# NB modify sqlquery.txt with delete from ... after initial create table, saves headache remembering feild types

# then publish to xml with
# I:\Dropbox\projects\0.3 Catalogue\oracle2xml.r

# then upload to
# http://ddiindex-nceph.anu.edu.au/ddiindex/indexer.jsp
# as ivan, ivan123!

# NB check that old version is deleted first

*** edit by browser or code
*** include OTHRSTDYMAT
*** synchronise local metadata
#+name:synchronise local metadata
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no


newnode(dsc='synchronise local metadata', clearpage = F, ttype='metadata_sync',
 dontshow_doc = T, notes='',echoCode = FALSE,doc_code = F,
 code="
 
 s <- dbGetQuery(oracle, paste(\"select * from stdydscr where idno = '\",idno,\"'\", sep = ''))
 matrix(s)
 f <- dbGetQuery(oracle, paste(\"select * from filedscr where idno = '\",idno,\"' order by filetype\", sep = ''))
 f[,1:4]

 d <- dbGetQuery(oracle, paste(\"select * from datadscr where fileid in (\",paste(f$FILEID, collapse = ','),\")\", sep = ''))
 d

 # now overwrite the local copies
 dir('metadata')
 write.csv(s, 'metadata/stdydscr.csv', row.names=F)
 write.csv(f, 'metadata/filedscr.csv', row.names=F)
 write.csv(d, 'metadata/datadscr.csv', row.names=F)


 doclist <- dir(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno), pattern = tolower(idno))
 doclist
 
 for(doc in doclist){
 file.copy(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno,doc), file.path('metadata',doc), overwrite = T)
 }
 
 doc <- dir(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno,'reports'), pattern = 'pdf')
 file.copy(file.path('I:/My Dropbox/projects/0.3 Catalogue/publishddi',idno,'reports',doc), file.path('metadata',gsub('_doc','_metadata',doc)), overwrite = T)
 
 ")
source(dir('run',pattern='metadata_sync', full.names=T) )
##################################################################

#+end_src


*** further edits

# newnode(dsc='further edits', clearpage = F, ttype='metadata',
  # notes=' ',echoCode = FALSE,doc_code = F,
 # code="


 
 # # carefully
 # sqlQuery(oracle, 
 # paste(\"select * from datadscr 
 # where LABL = 'wedge'
 # and fileid in (\",paste(f$FILEID, collapse = ','),\")
 # \", sep = ''), as.is = T)
 
 # sqlQuery(oracle, 
 # paste(\"update datadscr 
 # set NOTES = 'Wedge represents the search radius in kilometres for hotspots around the focal pollution monitor (25  50  75 100 150 200 300 400 500)'
 # where LABL = 'wedge'
 # and fileid in (\",paste(f$FILEID, collapse = ','),\")
 # \", sep = ''), as.is = T)

 # sqlQuery(oracle, 
 # paste(\"select * from datadscr 
 # where LABL = 'idma'
 # and fileid in (\",paste(f$FILEID, collapse = ','),\")
 # \", sep = ''), as.is = T)
 
 # sqlQuery(oracle, 
 # paste(\"update datadscr 
 # set NOTES = ' IDMA (InDex MacArthur) should be more properly called FFDI (Forest Fire Danger Index), and represents the calculated FFDI at the location of the pollution station on that day.'
 # where LABL = 'idma'
 # and fileid in (\",paste(f$FILEID, collapse = ','),\")
 # \", sep = ''), as.is = T)

 
 # ")
  
# ##################################################################

** archive
*** manage access

# newnode(dsc='manage access', clearpage = F, ttype='metadata',
 # notes='',echoCode = FALSE,doc_code = F,
 # code="


 
 # dbSendUpdate(ch,
 # 'grant select on all tables in schema bio_events to bio'
 # )

 
 # ")
 
##################################################################

*** migrate data to final location

newnode(dsc='migrate data to final location',ttype='archive',
 dontshow_doc = T, notes='',echoCode = FALSE,doc_code = F, clearpage = F,
 code="
 
 ")
 
##################################################################
*** archive milestone dataset

newnode(dsc='archive milestone dataset',ttype='archive',
 dontshow_doc = T, notes='',echoCode = FALSE,doc_code = F, clearpage = F,
 code="
 
 # use synchronise it to send from dbxwd to wd (minus metadata) ie "I:/projects/1.302 Biomass/analysis/exposures/event validation" 
 # only a selection of the files
 
 # Mount truecrypt volume to K
 dir('K:')
 outdir <- 'K:/projects/1.302 Biomass/analysis/exposures/event validation'
 dir.create(outdir, recursive=T)
 # # newnode migrate CURRENT_FireEvents.mdb
 t(fileDscr[which(fileDscr$FILENAME == 'CURRENT_FireEvents.mdb'),])
 outfile <- fileDscr[which(fileDscr$FILENAME == 'CURRENT_FireEvents.mdb'),]
 file.copy(file.path(outfile$FILELOCATION,outfile$FILENAME), file.path(outdir,outfile$FILENAME))
 file.copy(file.path(outfile$FILELOCATION,'Mousehook.dll'), file.path(outdir,'Mousehook.dll'))
 # I could just use the GUI?
 ")
 
##################################################################

** the end

newnode(dsc = 'The end', clearpage = F, ttype = 'report', nosectionheading = T,
dontshow = T,
append = T,
document='sweave',
end_doc = T)


 oldwd <- getwd()
 setwd('reports')
 Sweave('bio_validated_bushfire_events_transformations_doc.Rnw')

 



* Archives
** TASKS
*** TODO add to nceph unrestricted and github
** code
This is the achiving node.
#+name:metadata
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  
  newnode(dsc='Archives', clearpage = F, ttype='transformations', nosectionheading = T,
   i='metadata',o = 'Archives',append = T,end_doc = F,
   notes='',echoCode = FALSE,
   code=NA,
   TASK=NA)
#+end_src

** dlAWAP_GRIDS.r

* End
#+name:end
#+begin_src R :session *R* :tangle run/transformations.r :exports none :eval no
  newnode(dsc = 'The end', clearpage = F, ttype = 'transformations', nosectionheading = T,
  dontshow = T,
  append = T,,
  document='sweave',
  end_doc = T)
  # now run 
  #oldwd <- getwd()
  #setwd('reports')
  #Sweave('AWAP_GRIDS_transformations_doc.Rnw')
  #setwd(oldwd)
#+end_src

\begin{comment}
source('~/my dropbox/tools/transformations.r')
source('run/transformations.r')
"i:\My Dropbox\tools\transformationscolour.py"  AWAP_GRIDS_transformations.txt   AWAP_GRIDS_transformations
\end{comment}


