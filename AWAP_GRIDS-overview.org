#+TITLE:AWAP GRIDS 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* TODOlist
** DONE commit the test get awap data after liz
** DONE links to get pgutils
http://download.osgeo.org/postgis/windows/pg92/
postgis-pg92-binaries-2.0.2w64.zip	17-Jan-2013 14:25	21M	 
http://download.osgeo.org/postgis/windows/pg92/postgis-pg92-binaries-2.0.2w64.zip
http://www.enterprisedb.com/products-services-training/pgbindownload

** DONE check solar radiation solarave_2001050820010508.grid
'http://www.bom.gov.au/web03/ncc/www/awap/solar/solarave/daily/grid/0.05/history/nat/1993080619930806.grid.Z'
'http://www.bom.gov.au/web03/ncc/www/awap/solar/solarave/daily/grid/0.05/history/nat/2001050820010508.grid.Z'
# file.remove('data2000-2004/solar/solarave_2001050820010508.grid')
** DONE main awap links must be obvious
# Function to download the Australian Water Availability Grids http://www.bom.gov.au/jsp/awap/
# urls can be like
  # rain                http://www.bom.gov.au/web03/ncc/www/awap/   rainfall/totals/daily/    grid/0.05/history/nat/2010120120101201.grid.Z
	# tmax                http://www.bom.gov.au/web03/ncc/www/awap/   temperature/maxave/daily/ grid/0.05/history/nat/2012020620120206.grid.Z
	# tmin                http://www.bom.gov.au/web03/ncc/www/awap/   temperature/minave/daily/ grid/0.05/history/nat/2012020620120206.grid.Z
	# vapour pressure 9am http://www.bom.gov.au/web03/ncc/www/awap/   vprp/vprph09/daily/       grid/0.05/history/nat/2012020620120206.grid.Z
	# vapour pressure 3pm http://www.bom.gov.au/web03/ncc/www/awap/   vprp/vprph15/daily/       grid/0.05/history/nat/2012020620120206.grid.Z
	# solar               http://www.bom.gov.au/web03/ncc/www/awap/   solar/solarave/daily/     grid/0.05/history/nat/2012020720120207.grid.Z
	# NDVI                http://reg.bom.gov.au/web03/ncc/www/awap/   ndvi/ndviave/month/       grid/history/nat/2012010120120131.grid.Z
* Introduction
NCEPH holds Australian Bureau of Meteorology data for all stations from 1990 to 2010 \cite{NationalClimateCentreoftheBureauofMeteorology:2005}.
The aim of this project is to download the Australian Water Availability Project (AWAP) gridded datasets \cite{Jones2009}.  In particular we want the vapour pressure data from 2010 so that we don't have to buy it again.  We want to compare it with the station data to see if they are close.
* The Codes
** plan
#+begin_src R :session *R* :tangle no :exports none :eval no
  if(!require(devtools)) install.packages("devtools", repos = 'http://cran.csiro.au'); require(devtools)
  if(!require(disentangle)) install_github("disentangle", "ivanhanigan"); require(disentangle)
  
  nodes <- newnode(name='main.r', newgraph = T,
   inputs = 'init')
  
  nodes <- newnode(name='zones',
   inputs='main.r')
  
#+end_src
** init
*** project
#+name: R-init
#+begin_src R  :session *R* :exports none :eval no :tangle no
  # INITIALISE THE PROJECT
  if (!require(ProjectTemplate)) install.packages('ProjectTemplate', repos='http://cran.csiro.au'); require(ProjectTemplate)
  if (!require(makeProject)) install.packages('makeProject', repos='http://cran.csiro.au'); require(makeProject)
  setwd('..')
  dir()
  create.project('AWAP2')
  #copy into curr dir
  ?makeProject
  makeProject(author='ivanhanigan',email='ivan.hanigan@gmail.com',force=TRUE, name = "AWAP_GRIDS")
  
  
  setwd('AWAP_GRIDS')
  load.project()
  
  
  
  
#+end_src
*** create schema
#+name:create_schema
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:create_schema
CREATE SCHEMA awap_grids;
grant ALL on schema awap_grids to gislibrary;
GRANT ALL ON ALL TABLES IN SCHEMA awap_grids TO gislibrary;
grant ALL on all functions in schema awap_grids to gislibrary;
grant ALL on all sequences in schema awap_grids to gislibrary; 
#+end_src
** config
#+name:global.dcf
#+begin_src R :session *R* :tangle config/global.dcf :exports none :eval no
data_loading: off
cache_loading: on
munging: on
logging: off
load_libraries: on
libraries: reshape, plyr, ggplot2, stringr, lubridate, fgui, raster, rgdal, swishdbtools, awaptools
as_factors: on
data_tables: off

#+end_src

** main
*** main
#+name:main
#+begin_src R :session *shell* :tangle main.r :exports none :eval no
  ################################################################
  # Project: AWAP_GRIDS
  # Author: ivanhanigan
  # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
  # This is the main file for the project
  # It should do very little except call the other files
  
  ####################
  ### Set the working directory
  if(exists('workdir')){
    workdir <- workdir
  } else {
    workdir <- "~/data/AWAP_GRIDS"
  }
  setwd(workdir)
  
  ####################
  # Functions for the project
  
  if (!require(ProjectTemplate)) install.packages('ProjectTemplate', repos='http://cran.csiro.au'); require(ProjectTemplate)
  load.project()
  
  ####################
  # user definitions, or setup interactively
  destination_server <- "tern5.qern.qcif.edu.au" 
  source_server <- "115.146.92.162"
  fresh <- FALSE
  startdate <- '1980-01-01'
  enddate <-  '1980-02-01' #Sys.Date()-2
  checkDates <- TRUE
  interactively <- FALSE
  variablenames <- 'maxave,minave,totals,vprph09,vprph15' #,solarave
  aggregation_factor <- 3
  if(length(grep('linux',sessionInfo()[[1]]$os)) == 1)
  {
    os <- 'linux'
  } else {
    os <- 'windows'
  }
  #os <- 'linux' # only linux and windoze supported
  pgisutils <- "/usr/pgsql-9.1/bin/"
  #"\"C:\\pgutils\\postgis-pg92-binaries-2.0.2w64\\bin\\"
  pgutils <- "\"C:\\pgutils\\pgsql\\bin\\"
  
  ####################
  # run the project (alternately do this from Kepler)
  source(file.path(workdir, "src/scoping.r"))
  if(fresh == TRUE)
  {
    source(file.path(workdir, "src/load.r"))  
  } else {
    source(file.path(workdir, "src/load_mirrored_grids.r"))  
  }
  
  # source("src/load.r")
  # source("src/clean.r")
  # source("src/do.r")
  
#+end_src
*** deprecated main
#+name:main-newnode
#+begin_src R :session *R* :tangle no :exports none :eval no
  # Project: AWAP_GRIDS
  # Author: ivanhanigan
  # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
  # This is the main file for the project
  # It should do very little except call the other files
  
  ### Set the working directory
  setwd("/home/ivan/data/AWAP_GRIDS")
  
  
  ### Set any global variables here
  if(exists('startdate')){
    startdate <- startdate
  } else {
    startdate <- '2000-01-01'
  }
  if(exists('enddate')){
    enddate <- enddate
  } else {
    enddate <- '2000-01-02'
  }
  
  ####################
  ## if (!require(ProjectTemplate)) install.packages('ProjectTemplate', repos='http://cran.csiro.au'); require(ProjectTemplate)
  ## load.project()
  ## #require(fgui)
  if(!require(fgui)) install.packages("fgui", repos='http://cran.csiro.au'); require(fgui)
  if(!require(swishdbtools)) print('Please download the swishdbtools package and install it.')
  # for instance
  # install.packages("~/tools/swishdbtools_1.0_R_x86_64-pc-linux-gnu.tar.gz", repos = NULL, type = "source");
  require(swishdbtools)
  
  ####################
  getscope <- function (
    sdate = startdate,
    edate = enddate,
    variablenames) {
    scope <- list(
      startdate <- sdate,
      enddate <- edate,
      variablenames <- variablenames
    )
    return(scope)
  }
  scope <- guiv(getscope, argList = list(variablenames = c('totals','maxave','minave','vprph09','vprph15','solarave')))
  # print(scope)
  p <- getPassword()
  
  ####################
  
  # source("src/load.r")
  # source("src/clean.r")
  # source("src/do.r")
  
  
  ### Run the code
  ## source("code/load.R")
  ## source("code/clean.R")
  ## source("code/func.R")
  ## source("code/do.R")
  
#+end_src

** scoping  
#+name:scoping
#+begin_src R :session *shell* :tangle src/scoping.r :exports none :eval no
  ###########################################################################
  # newnode: scoping
    require(awaptools)
    variableslist <- variableslist()  
    require(fgui)
    #require(ProjectTemplate)
    #load.project()
    # # user definitions, or setup interactively
    # startdate <- '1995-01-01'
    # enddate <-  '1997-01-01'
    # interactively <- FALSE
    # variablenames <- 'maxave'
    # aggregation_factor <- 3
    # this will aggregate the 5 km pixels into 15 km averages, for storage
    if (exists('startdate')){
      startdate <- as.Date(startdate)
    } else {
      startdate <- '2013-01-08'
    }
    if (exists('enddate')){
      enddate <- as.Date(enddate)
    } else {
      enddate <-  '2013-01-20'
    }
    if (exists('interactively')){
      interactively <- interactively
    } else {
      interactively <- FALSE
    }
    # if (variablenames == 'all'){
    # variablenames <-  c('totals','maxave','minave','vprph09','vprph15','solarave'))
    # }
    if (exists('variablenames')){
      variablenames <- variablenames
      variablenames <- strsplit(variablenames, ',')
    } else {
      variablenames <- 'maxave,minave,totals'
      variablenames <- strsplit(variablenames, ',')
    }
    # if these all exist don't run the scope gui?
    #if(!exists('username') & !exists('spatialzones') & !exists('outdir')){
    # or set
  
    if(interactively == TRUE){
      getscope <- function (
        sdate = startdate,
        edate = enddate,
        variablenames) {
        scope <- list(
          startdate <- sdate,
          enddate <- edate,
          variablenames <- variablenames
        )
        return(scope)
      }
      scope <- guiv(getscope, argList = list(variablenames = c('totals','maxave','minave','vprph09','vprph15','solarave')))
  
    } else {
        scope <- list(
          startdate <- startdate,
          enddate <- enddate,
          variablenames <- variablenames
        )
    }
    print(scope)
  
#+end_src
*** deprecated scoping
#+name:scope
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:scope
# This workflow will deliver weather data from the EWEDB to a local directory.
# Ivan Hanigan 2012-12-14

# README:
#   Running this workflow will cause a GUI box to appear for your password.
# Sometimes this GUI box is behind other windows.
# 
# Either change the inputs above, or set interactively to TRUE.
# In interactively mode a GUI box will open where you can change the values, 
# or leave blank to accept the defaults.
# 
# NB dates need quotation marks if using the GUI box.
# 
# TODO:
#   There are missing days in  solarave, vprph09, vprph15.
# Try downloading again to see if fixed now.
# Add the population weighted averaging approach.

if(!require(fgui)) install.packages("fgui", repos='http://cran.csiro.au'); require(fgui)
if(!require(swishdbtools)) print('Please download the swishdbtools package and install it.')
# for instance 
# install.packages("~/tools/swishdbtools_1.0_R_x86_64-pc-linux-gnu.tar.gz", repos = NULL, type = "source");
require(swishdbtools)


# # user definitions, or setup interactively
# username <- 'gislibrary'
# spatialzones <- 'SD'
# outdir <- '~/'
# startdate <- '1995-01-01'
# enddate <-  '1997-01-01'
# interactively <- TRUE 
# 
if (exists('username')) {
  u <- username
} else {
  u <- 'gislibrary'
}
if (exists('spatialzones')) {
  s <- spatialzones
} else {
  s <- 'SD'
}
if (exists('outdir')) {
  o <- outdir
} else {
  o <- '~/'
}
if (exists('startdate')){
  startdate <- as.Date(startdate) 
} else {
  startdate <- '1995-01-01'
}
if (exists('enddate')){    
  enddate <- as.Date(enddate)
} else {
  enddate <-  '1997-01-01'
}
if (exists('interactively')){    
  interactively <- interactively
} else {
  interactively <- TRUE
}
# if these all exist don't run the scope gui?
#if(!exists('username') & !exists('spatialzones') & !exists('outdir')){
# or set 

if(interactively == TRUE){
  scope <- function(usernameOrBlank=u, 
                    spatialzonesOrBlank = s, 
                    outdirOrBlank=o,
                    startdateOrBlank=startdate,
                    enddateOrBlank=enddate){
    L <- list(
      u <- usernameOrBlank,
      s <- spatialzonesOrBlank,
      o <- outdirOrBlank,
      startdate <- startdateOrBlank,
      enddate <- enddateOrBlank
    )
    return(L)
  }
  Listed <- guiv(scope)
  Listed
  u <- Listed[1]
  s <- Listed[2]
  o <- Listed[[3]][1]
  startdate <- as.Date(Listed[[4]][1])
  enddate <- as.Date(Listed[[5]][1])
}
# don't let password get hardcoded
p <- getPassword()

# ch <- connect2postgres(h = '115.146.84.135', 
#                        d =  'ewedb', 
#                        u = u, 
#                        p = p)


# dat <- dbGetQuery(ch,
#                  "SELECT date, year, sla_code, minave, maxave, solarave, vprph09,vprph15
#                  FROM weather_sla.weather_sla
#                  where sla_code = 105051100 order by date
# ")
# with(dat, plot(date, maxave, type = 'l'))

#+end_src




** COMMENT DEPRECATED IN FAVOUR OF PACKAGE func
*** core libs
#+begin_src R  :session *R* :exports none :eval no :tangle no
  # Project: AWAP_GRIDS
  # Author: ivanhanigan
  # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
  # Functions for the project
  if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
  if(!require(swishdbtools)){
  if(length(grep('linux',sessionInfo()[[1]]$os)) == 1)
  {
    os <- 'linux'
  
  print('Downloading the swishdbtools package and install it.')
   download.file('http://swish-climate-impact-assessment.github.com/tools/swishdbtools/swishdbtools_1.1_R_x86_64-pc-linux-gnu.tar.gz', '~/swishdbtools_1.1_R_x86_64-pc-linux-gnu.tar.gz', mode = 'wb')
  # for instance
  install.packages("~/swishdbtools_1.1_R_x86_64-pc-linux-gnu.tar.gz", repos = NULL, type = "source");
  
  } else {
      os <- 'windows'
  
  print('Downloading the swishdbtools package and install it.')
   download.file('http://swish-climate-impact-assessment.github.com/tools/swishdbtools/swishdbtools_1.1.zip', '~/swishdbtools_1.1.zip', mode = 'wb')
  # for instance
  install.packages("~/swishdbtools_1.1.zip", repos = NULL);
  
  }
  }
  require(swishdbtools)
  if(!require(raster)) install.packages('raster', repos='http://cran.csiro.au');require(raster)
  if(!require(fgui)) install.packages('fgui', repos='http://cran.csiro.au');require(fgui)
  if(!require(rgdal)) install.packages('rgdal', repos='http://cran.csiro.au');require(rgdal)
  
  ####
  # MAKE SURE YOU HAVE THE CORE LIBS
  if (!require(lubridate)) install.packages('lubridate', repos='http://cran.csiro.au'); require(lubridate)
  if (!require(reshape)) install.packages('reshape', repos='http://cran.csiro.au'); require(reshape)
  if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
  if (!require(ggplot2)) install.packages('ggplot2', repos='http://cran.csiro.au'); require(ggplot2)
  
#+end_src
*** DatesUnavaliable
#+name:DatesUnavailable
#+begin_src R :session *shell* :tangle no :exports none :eval no
###########################################################################
# newnode: DatesUnavailable

# get the list of dates between the start and end dates that is not found in the database 
DatesUnavaliable <- function (dataBaseConnection, variableName, startDate, endDate) 
{
  ch <- dataBaseConnection
  measure_i <- variableName
  start_at <- startDate
  end_at <- endDate
  
  datelist_full <- as.data.frame(seq(as.Date(start_at),
                                     as.Date(end_at), 1))
  names(datelist_full) <- 'date'
  
  
  tbls <- pgListTables(conn=ch, schema='awap_grids', pattern = measure_i)
  #     pattern=paste(measure_i,"_", gsub("-","",sdate), sep=""))
  pattern_x <- paste(measure_i,"_",sep="")
  tbls$date <- paste(
    substr(gsub(pattern_x,"",tbls[,1]),1,4),
    substr(gsub(pattern_x,"",tbls[,1]),5,6),
    substr(gsub(pattern_x,"",tbls[,1]),7,8),
    sep="-")
  tbls$date <- as.Date(tbls$date)
  datelist <-  which(datelist_full$date %in% tbls$date)
  
  
  if(length(datelist) == 0)
  {
    datelist <- datelist_full[,]
  } else {
    datelist <- datelist_full[-datelist,]
  }
  
  
}


#+end_src

*** Get Data 
#+begin_src R :session *R* :tangle no :exports none :eval no
# newnode get_data
# authors: Joseph Guillaume
# downloads from http://www.bom.gov.au/jsp/awap/
get_data<-function(variable,measure,timestep,startdate,enddate){
  url="http://www.bom.gov.au/web03/ncc/www/awap/{variable}/{measure}/{timestep}/grid/0.05/history/nat/{startdate}{enddate}.grid.Z"
  url=gsub("{variable}",variable,url,fixed=TRUE)
  url=gsub("{measure}",measure,url,fixed=TRUE)
  url=gsub("{timestep}",timestep,url,fixed=TRUE)
  url=gsub("{startdate}",startdate,url,fixed=TRUE)
  url=gsub("{enddate}",enddate,url,fixed=TRUE)

  try(download.file(url,sprintf("%s_%s%s.grid.Z",measure,startdate,enddate),mode="wb"))
  }
#+end_src
*** Get Data Range
#+begin_src R :session *R* :tangle no :exports none :eval no
# newnode get_data_range
# authors: Joseph Guillaume and Francis Markham
# downloads from http://www.bom.gov.au/jsp/awap/
  
get_data_range<-function(variable,measure,timestep,startdate,enddate){
  if (timestep == "daily"){
    thisdate<-startdate
    while (thisdate<=enddate){
      get_data(variable,measure,timestep,format(as.POSIXct(thisdate),"%Y%m%d"),format(as.POSIXct(thisdate),"%Y%m%d"))
      thisdate<-thisdate+as.double(as.difftime(1,units="days"),units="secs")
    }
  } else if (timestep == "month"){
    # Make sure that we go from begin of the month
    startdate <- as.POSIXlt(startdate)
    startdate$mday <- 1
    # Find the first and last day of each month overlapping our range
    data.period.start <- seq(as.Date(startdate), as.Date(enddate), by = 'month')
    data.period.end <- as.Date(sapply(data.period.start, FUN=function(x){as.character(seq(x, x + 40, by = 'month')[2] - 1)}))
    # Download them
    for (i in 1:length(data.period.start)){
      get_data(variable,measure,timestep,format(as.POSIXct(data.period.start[i]),"%Y%m%d"),format(as.POSIXct(data.period.end[i]),"%Y%m%d"))
    }
   
} else {
    stop("Unsupported timestep, only 'daily' and 'month' are currently supported")
  }
}
#+end_src

*** variableslist
#+name:variableslist
#+begin_src R :session *R* :tangle no :exports none :eval no
  ###########################################################################
  # newnode: variableslist
  variableslist<-"variable,measure,timestep
  rainfall,totals,daily
  temperature,maxave,daily
  temperature,minave,daily
  vprp,vprph09,daily
  vprp,vprph15,daily
  solar,solarave,daily
  ndvi,ndviave,month
  "
  variableslist <- read.csv(textConnection(variableslist))
    
#+end_src

*** ProcessFunctions
#+name:ProcessFunctions.R
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:ProcessFunctions.R
  
  RunProcess = function(executable, arguments)
  {
    command = paste(sep="", "\"", executable,  "\" ", arguments);
    
    print (command)
    
    exitCode = system(command, intern = FALSE, ignore.stdout = FALSE, ignore.stderr = FALSE, wait = TRUE, input = NULL
                      , show.output.on.console = TRUE
                      #, minimized = FALSE
                      , invisible = FALSE
    );
    if(exitCode != 0)
    {
      stop("Process returned error");
    }
    return (exitCode)
  }
  
  
  RunViaBat = function(executableFileName, arguments)
  {
    command = paste(sep="", "\"", executableFileName,  "\" ", arguments);
    sink("C:\\Users\\u5265691\\Desktop\\ThingToRun.bat")
    cat(command)
    sink()
    
    exitCode = system("C:\\Users\\u5265691\\Desktop\\ThingToRun.bat")
    if(exitCode != 0)
    {
      stop("Process returned error");
    }
    return (exitCode)
  }
  
#+end_src

*** ZipFunctions
#+name:ZipFunctions.R
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:ZipFunctions.R
  uncompress_linux <- function(filename)
    {
      print(filename)
      system(sprintf('uncompress %s',filename))
    }
  
  # tries to find 7 zip exe
  ExecutableFileName7Zip <- function()
  {
    executableName <- "C:\\Program Files\\7-Zip\\7z.exe"
  
    if(file.exists(executableName))
    {
      return (executableName)
    }
  
    #other executable file names and ideas go here ...
    stop("failed to find 7zip")
  }
  
  # simple function to extract 7zip file
  # need to have 7zip installed
  Decompress7Zip <- function(zipFileName, outputDirectory, delete)
  {
    executableName <- ExecutableFileName7Zip()
  
  #   fileName = GetFileName(zipFileName)
  #   fileName = PathCombine(outputDirectory, fileName)
  
  
  #   if(file.exists(fileName))
  #   {
  #     unlink(zipFileName);
  #   }
  
    arguments <- paste(sep="",
                      "e ",
                      "\"", zipFileName, "\" ",
                      "\"-o", outputDirectory, "\" ",
      "")
  
    print( arguments)
  
    RunProcess(executableName, arguments)
  
    if(delete)
    {
      unlink(zipFileName);
    }
  }
  
  #test
  # Decompress7Zip("D:\\Development\\Awap Work\\2013010820130108.grid.Z", "D:\\Development\\Awap Work\\", TRUE)
  
#+end_src

*** raster_aggregate
#+name:raster_aggregate
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:raster_aggregate
  raster_aggregate <- function(filename, aggregationfactor, delete = TRUE)
  {
    r <- raster(filename)
    r <- aggregate(r, fact = aggregationfactor, fun = mean)
    writeRaster(r, gsub('.grid','',fname), format="GTiff",
  overwrite = TRUE)
    if(delete)
      {
        file.remove(filename)
      }
  }
  
#+end_src

*** COMMENT load2postgres_raster
#+name:load2postgres_raster
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:load2postgres_raster
  load2postgres_raster <- function(filename, remove = TRUE)
  {
    outname <- gsub('.tif',"", filename)
    outname <- substr(outname, 1, nchar(outname) - 8)
    if(os == 'linux')
    {
     system(
    #        cat(
            paste(pgisutils,"raster2pgsql -s 4283 -I -C -M ",filename," -F awap_grids.",outname," > ",outname,".sql", sep="")
            )
  
     system(
    #        cat(
            paste("psql -h 115.146.84.135 -U gislibrary -d ewedb -f ",outname,".sql",
              sep = ""))
    } else {
      sink('raster2sql.bat')
      cat(paste(pgisutils,"raster2pgsql\" -s 4283 -I -C -M ",filename," -F awap_grids.",outname," > ",outname,".sql\n",sep=""))
  
      cat(
      paste(pgutils,"psql\" -h 115.146.84.135 -U gislibrary -d ewedb -f ",outname,".sql",
      sep = "")
        )
      sink()
      system('raster2sql.bat')
      file.remove('raster2sql.bat')
    }
  
    if(remove)
      {
        file.remove(filename)
        file.remove(paste(outname, '.sql', sep =""))
      }
  }
  
#+end_src

*** COMMENT deprecated pgListTables, moved to swishdbtools
#+name:pgListTables
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:pgListTables
  pgListTables <- function(conn, schema, pattern = NA)
  {
    tables <- dbGetQuery(conn, 'select   c.relname, nspname
                         FROM pg_catalog.pg_class c
                         LEFT JOIN pg_catalog.pg_namespace n
                         ON n.oid = c.relnamespace
                         where c.relkind IN (\'r\',\'\') ')
    tables <- tables[grep(schema,tables$nspname),]
    if(!is.na(pattern)) tables <- tables[grep(pattern, tables$relname),]
    tables <- tables[order(tables$relname),]
    return(tables)
  }
#+end_src
*** COMMENT pgListTables
#+name:pgListTables
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:pgListTables
pgListTables <- function(conn, schema, pattern = NA)
{
  tables <- dbGetQuery(conn, "select   c.relname, nspname
                       FROM pg_catalog.pg_class c
                       LEFT JOIN pg_catalog.pg_namespace n
                       ON n.oid = c.relnamespace
                       where c.relkind IN ('r','','v') ")
  tables <- tables[grep(schema,tables$nspname),]
  if(!is.na(pattern)) tables <- tables[grep(pattern, tables$relname),]
  tables <- tables[order(tables$relname),]
  return(tables)
}
#+end_src

*** pgListTables-test dates
#+name:pgListTables-test
#+begin_src R :session *R* :tangle tests/test-pgListTables.r :exports none :eval no
  ################################################################
  # name:pgListTables-test
  require(ProjectTemplate)
  load.project()
  
  require(swishdbtools)
  p <- getPassword(remote=T)
  ch <- connect2postgres(h = '130.102.48.116', db = 'ewedb', user=
                         'gislibrary', p=p)
  measure_i <- 'vprph15'
  tbls <- pgListTables(conn=ch, schema='awap_grids')#, table=measure_i, match=F)
  tbls$date <- paste(substr(gsub(paste(measure_i,"_",sep=""),"",tbls[,1]),1,4),
          substr(gsub(paste(measure_i,"_",sep=""),"",tbls[,1]),5,6),
          substr(gsub(paste(measure_i,"_",sep=""),"",tbls[,1]),7,8),
          sep="-")
  tbls$date <- as.Date(tbls$date)
  head(tbls)
  tbls <- tbls[tbls$date > as.Date('1912-01-01'),]
  plot(tbls$date, rep(1,nrow(tbls)), type = 'h')
  tbls[tbls$date < as.Date('1999-01-01'),]
  tbls[tbls$date >= as.Date('2006-07-01') & tbls$date < as.Date('2007-01-01'),]
  tbls[tbls$date >= as.Date('2004-01-01') & tbls$date < as.Date('2005-01-01'),]
  
#+end_src
*** sqlquery_oracle
#+name:sqlquery
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:aggregate_postgres
  sqlquery <- function(channel, dimensions, operation,
                       variable, variablename=NA, into, append = FALSE,
                       tablename, where, group_by_dimensions=NA,
                       having=NA,
                       grant = NA, force = FALSE,
                       print = FALSE)
  {
  
    exists <- try(dbGetQuery(channel,
                             paste("select * from",into,"limit 1")))
    if(!force & length(exists) > 0 & append == FALSE)
                             stop("Table exists. Force Drop or Insert Into?")
    if(force & length(exists) > 0) dbGetQuery(channel,
                             paste("drop table ",into))
    if(length(exists) > 0 & append == TRUE)
      {
        sqlquery <- paste("INSERT INTO ",into," (",
                             paste(names(exists), collapse=',', sep='') ,")\n",
                          "select ", dimensions,
                          sep = ""
                          )
      } else {
        sqlquery <- paste("select ", dimensions, sep = "")
      }
    if(!is.na(operation))
    {
    sqlquery <- paste(sqlquery, ", ", operation, "(",variable,") as ",
      ifelse(is.na(variablename), variable,
      variablename), '\n', sep = "")
    }
    if(append == FALSE){
      sqlquery <- paste(sqlquery, "into ", into ,"\n", sep = "")
    }
    sqlquery <- paste(sqlquery, "from ", tablename ,"\n", sep = "")
    if(!is.na(where))
    {
    sqlquery <- paste(sqlquery, "where ", where, "\n", sep = "")
    }
    if(group_by_dimensions == TRUE)
    {
    sqlquery <- paste(sqlquery, "group by ",dimensions, "\n", sep = "")
    }
  #  cat(sqlquery)
  
  
  
    ## sqlquery <-  paste("select ", dimensions,
    ##                ", ",operation,"(",variables,") as ",variables,
    ##                operation, "
    ##                into ", into ,"
    ##                from ",tablename," t1
    ##                group by ",dimensions,
    ##                sep="")
    if(print) {
      cat(sqlquery)
    } else {
      dbSendQuery(channel, sqlquery)
    }
  
  }
#+end_src
*** sqlquery_postgres
#+name:sqlquery
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:aggregate_postgres
    
  sqlquery_postgres <- function(channel, dimensions, operation,
                       variable, variablename=NA, into_schema = 'public',
                       into_table, append = FALSE,
                       from_schema = 'public', from_table, where=NA,
                       group_by_dimensions=NA,
                       having=NA,
                       grant = NA, force = FALSE,
                       print = FALSE)
  {
    # assume ch exists
    exists <- pgListTables(channel, into_schema, into_table)
    if(!force & nrow(exists) > 0 & append == FALSE)
      {
        stop("Table exists. Force Drop or Insert Into?")
      }
    
    if(force & nrow(exists) > 0)
      {
        dbGetQuery(channel, paste("drop table ",into_schema,".",into_table,sep=""))
      }
    
    if(!force & nrow(exists) >0)
      {
        existing_table <- dbGetQuery(channel,
                                     paste('select * from ',
                                           into_schema,'.',
                                           into_table,' limit 1',sep=''
                                           )
                                     )
      }
    
    if(nrow(exists) > 0 & append == TRUE)
      {
        sqlquery <- paste("INSERT INTO ",into_schema,".",into_table," (",
                             paste(names(existing_table), collapse=',', sep='') ,")\n",
                          "select ", dimensions,
                          sep = ""
                          )
      } else {
        sqlquery <- paste("select ", dimensions, "", sep = "")
      }
    
    if(!is.na(operation))
      {
        sqlquery <- paste(sqlquery, ", ", operation, "(",variable,") as ",
          ifelse(is.na(variablename), variable,
          variablename), '\n', sep = "")
      } else {
        sqlquery <- paste(sqlquery, ", ",variable," as ",
                          ifelse(is.na(variablename),variable,variablename),
                          "\n", sep="")
      }
    
    # this is when append is true but the table doesnt exist yet
    if(nrow(exists) == 0 & append == TRUE)
      {
        sqlquery <- paste(sqlquery, "into ",
                          into_schema,".",into_table,"\n", sep = ""
                          )
      }
    
    # otherwise append is false and the table just needs to be created
    if(append == FALSE)
      {
        sqlquery <- paste(sqlquery, "into ",
                          into_schema,".",into_table,"\n", sep = ""
                          )
      }
    
    sqlquery <- paste(sqlquery, "from ", from_schema,".",from_table ,"\n", sep = "")
    
    if(!is.na(where))
      {
        sqlquery <- paste(sqlquery, "where ", where, "\n", sep = "")
      }
    
    if(group_by_dimensions == TRUE)
      {
        sqlquery <- paste(sqlquery, "group by ",
                          dimensions, "\n",
                          sep = ""
                          )
      }
  #  cat(sqlquery)
    
    
    
    ## sqlquery <-  paste("select ", dimensions,
    ##                ", ",operation,"(",variables,") as ",variables,
    ##                operation, "
    ##                into ", into ,"
    ##                from ",tablename," t1
    ##                group by ",dimensions,
    ##                sep="")
    if(print) {
      cat(sqlquery)
    } else {
      dbSendQuery(channel, sqlquery)
    }
    
  }
    
#+end_src
*** sqlquery-test
#+name:sqlquery-test
#+begin_src R :session *R* :tangle tests/test-sqlquery.r :exports none :eval no
  ################################################################
  # name:sqlquery-test
  require(ProjectTemplate)
  load.project()
  
  require(swishdbtools)
  p <- getPassword(remote=T)
  ch <- connect2postgres(hostip='115.146.84.135', db='ewedb', user='gislibrary', p=p)
  sqlquery_postgres(
      channel = ch,
      append = TRUE,
      force = FALSE,
      print = FALSE,
      dimensions = 'stnum, date',
      variable = 'gv',
      variablename = NA,
      into_schema = 'public',
      into_table = 'awapmaxave_qc2',
      from_schema = 'public',
      from_table = 'awapmaxave_qc',
      operation = NA,
      where = "date = '2013-01-02' and stnum = 70351",
      group_by_dimensions = FALSE,
      having = NA,
      grant = 'public_group'
      )
  
  dbGetQuery(ch, 'select * from awapmaxave_qc2 limit 10')
  # for dev work
  
  ##     channel = ch
  ##     dimensions = 'stnum, date'
  ##     variable = 'gv'
  ##     variablename = NA
  ##     into_schema = 'public'
  ##     into_table = 'awapmaxave_qc2'
  ##     append = TRUE
  ##     grant = 'public_group'
  ##     print = TRUE
  ##     from_schema = 'public'
  ##     from_table = 'awapmaxave_qc'
  ##     operation = NA
  ##     force = FALSE
  ##     where = "date = '2007-01-01'"
  ##     group_by_dimensions = FALSE
  ##     having = NA
  
#+end_src
*** test2
#+name:sqlquery_postgres-test2
#+begin_src R :session *R* :tangle tests/test-sqlquery_postgres2.r :exports none :eval no
################################################################
# name:sqlquery_postgres-test2



  
  
    require(ProjectTemplate)
    load.project()
  
    require(swishdbtools)
    p <- getPassword(remote=T)
    ch <- connect2postgres(hostip='115.146.84.135', db='ewedb', user='gislibrary', p=p)
  
    variable_j <- "maxave"
    date_i <- '2012-01-01'
  #  debug(sqlquery)
    sqlquery(channel = ch,
      dimensions = paste("stnum, cast('",date_i,"' as date) as date",sep=""),
      variable = 'rt.rast, pt.the_geom',
      variablename = 'gv',
      into = 'awapmaxave_qc',
      append = FALSE,
      grant = 'public_group',
      print = FALSE,
      tablename = paste('awap_grids.',variable_j,'_',gsub('-','',date_i),' rt,\n weather_bom.combstats pt',sep=''),
      operation = "ST_Value",
      force = TRUE,
      where = "ST_Intersects(rast, the_geom)",
      group_by_dimensions = FALSE,
      having = NA)
  #  undebug(sqlquery)
  for(date_i in seq(as.Date('2012-01-21'), as.Date('2013-01-20'), 1))
    {
     date_i <- as.Date(date_i, origin = '1970-01-01')
     date_i <- as.character(date_i)
     print(date_i)
  
  #  debug(sqlquery)
    sqlquery(channel = ch,
      dimensions = paste("stnum, cast('",date_i,"' as date) as date",sep=""),
      variable = 'rt.rast, pt.the_geom',
      variablename = 'gv',
      into = 'awapmaxave_qc',
      append = TRUE,
      grant = 'public_group',
      print = FALSE,
      tablename = paste('awap_grids.',variable_j,'_',gsub('-','',date_i),' rt,\n weather_bom.combstats pt',sep=''),
      operation = "ST_Value",
      force = FALSE,
      where = "ST_Intersects(rast, the_geom)",
      group_by_dimensions = FALSE,
      having = NA)
    }
  
#+end_src

** load
*** COMMENT clean-slate-code
#+name:clean-slate
#+begin_src R :session *R* :tangle src/clean-slate.r :exports none :eval no
  ################################################################
  # name:clean-slate
  require(ProjectTemplate)
  load.project()
  pwd  <- getPassword(remote = T)
  ch <- connect2postgres("tern5.qern.qcif.edu.au", "ewedb", "gislibrary", p = pwd)
  grids2remove  <- pgListTables(ch, "awap_grids")
  head(grids2remove)
  # check
  dbGetQuery(ch, sprintf("select * from awap_grids.%s", grids2remove[20,1]))
  for(grid_i in grids2remove[-1,1])
    {
  #    grid_i <- grids2remove[1,1]    
      print(grid_i)    
      dbSendQuery(ch,
                  sprintf("drop table awap_grids.%s; ", grid_i)
                  )      
    }
  
#+end_src

*** load if day not available
#+name:load
#+begin_src R :session *shell* :tangle src/load.r :exports none :eval no
  ################################################################
  # name:load
  ################################################################
  # name:load
  # Project: AWAP_GRIDS
  # Author: ivanhanigan
  # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
  # This file loads all the libraries and data files needed
  # Don't do any cleanup here
  
  ### Load any needed libraries
  #load(LibraryName)
  setwd(workdir)
  require(ProjectTemplate)
  load.project()
  ## ch <- connect2postgres(h = '115.146.84.135', db = 'ewedb',
  ##                        user = 'gislibrary')
  ##
  ch <- connect2postgres2("ewedb")
  print(paste('root directory:', workdir))
  setwd('data')
  
  start_at <- scope[[1]][1]
  print(start_at)
  end_at <- scope[[2]][1]
  print(end_at)
  
  vars <- scope[[3]]
  #  print(vars)
  
  #  started <- Sys.time()
  
  for(i in 1:length(vars[[1]])){
  #    i = 1
    measure_i <- vars[[1]][i]
    variable <- variableslist[which(variableslist$measure == measure_i),]
    vname <- as.character(variable[,1])
    if(checkDates == TRUE)
      {
        datelist <- DatesUnavailable(ch, measure_i, start_at, end_at)
      } else {
        datelist <- seq(as.Date(start_at), as.Date(end_at), 1)
      }
    for(date_i in datelist)
    {
    # date_i <- datelist[1]
      date_i <- as.Date(date_i, origin = '1970-01-01')
      date_i <- as.character(date_i)
    #  print(date_i)
  
      sdate <- date_i
      edate <- date_i
    #}
      get_data_range(variable=as.character(variable[,1]),
                     measure=as.character(variable[,2]),
                     timestep=as.character(variable[,3]),
                     startdate=as.POSIXct(sdate),
                     enddate=as.POSIXct(edate))
  
      fname <- sprintf("%s_%s%s.grid.Z",measure_i,gsub("-","",sdate),gsub("-","",edate))
  
      if(file.info(fname)$size == 0)
        {
          file.remove(fname)
          next
        }
  
      if(os == 'linux')
        {
          uncompress_linux(filename = fname)
        } else {
          Decompress7Zip(zipFileName= fname, outputDirectory=getwd(), TRUE)
        }
      # hack to see if this fixes random breaking
      if(!file.exists(gsub('.Z$','',fname)))
      {
        Sys.sleep(time=10)
        uncompress_linux(filename = fname)
      }
      raster_aggregate(filename = gsub('.Z$','',fname),
        aggregationfactor = aggregation_factor, delete = TRUE)
      outname <- gsub('.tif',"", fname)
      outname <- substr(outname, 1, nchar(outname) - (7 + 8))
      load2postgres_raster(
                           ipaddress = destination_server,
                           u = "gislibrary", d = 'ewedb',
                           pgisutils = pgisutils, srid = 4283,
                           filename = gsub(".grid.Z", ".tif", fname),
                           out_schema="awap_grids",
                           out_table=outname, remove = T
                           )
  
    }
  
  }
  
  setwd(workdir)
  
#+end_src


*** load mirrored GRIDS
#+name:load_mirrored_grids
#+begin_src R :session *R* :tangle src/load_mirrored_grids.r :exports none :eval no
################################################################
# name:load_mirrored_grids

# Project: AWAP_GRIDS
# Author: ivanhanigan
# Maintainer: Who to complain to <ivan.hanigan@gmail.com>

# This file loads all the libraries and data files needed
# Don't do any cleanup here

### Load any needed libraries
#load(LibraryName)
setwd(workdir)
require(ProjectTemplate)
load.project()
## ch <- connect2postgres(h = '115.146.84.135', db = 'ewedb',
##                        user = 'gislibrary')
##
ch <- connect2postgres2("ewedb")
print(paste('root directory:', workdir))
setwd('data')

start_at <- scope[[1]][1]
print(start_at)
end_at <- scope[[2]][1]
print(end_at)

vars <- scope[[3]]
#  print(vars)

#  started <- Sys.time()

for(i in 1:length(vars[[1]])){
#    i = 1
  measure_i <- vars[[1]][i]
  variable <- variableslist[which(variableslist$measure == measure_i),]
  vname <- as.character(variable[,1])
  if(checkDates == TRUE)
    {
      datelist <- DatesUnavailable(ch, measure_i, start_at, end_at)
    } else {
      datelist <- seq(as.Date(start_at), as.Date(end_at), 1)
    }
  for(date_i in datelist)
  {
  # date_i <- datelist[1]
    date_i <- as.Date(date_i, origin = '1970-01-01')
    date_i <- as.character(date_i)
  #  print(date_i)

    sdate <- date_i
    edate <- date_i
  #}
#     get_data_range(variable=as.character(variable[,1]),
#                    measure=as.character(variable[,2]),
#                    timestep=as.character(variable[,3]),
#                    startdate=as.POSIXct(sdate),
#                    enddate=as.POSIXct(edate))

    fname <- sprintf("%s_%s%s.grid.Z",measure_i,gsub("-","",sdate),gsub("-","",edate))
#
#     if(file.info(fname)$size == 0)
#       {
#         file.remove(fname)
#         next
#       }

#     if(os == 'linux')
#       {
#         uncompress_linux(filename = fname)
#       } else {
#         Decompress7Zip(zipFileName= fname, outputDirectory=getwd(), TRUE)
#       }
#     # hack to see if this fixes random breaking
#     if(!file.exists(gsub('.Z$','',fname)))
#     {
#       Sys.sleep(time=10)
#       uncompress_linux(filename = fname)
#     }
#     raster_aggregate(filename = gsub('.Z$','',fname),
#       aggregationfactor = aggregation_factor, delete = TRUE)
     outname <- gsub('.grid.Z',"", fname)
     outname <- substr(outname, 1, nchar(outname) - (8))

    p <- get_passwordTable()
    p <- p[which(p$V3 == "ewedb"), "V5"]
    r <- readGDAL2(source_server, 'gislibrary', 'ewedb',
                   schema = 'awap_grids', table = outname, p = p)
#    image(r)
    writeGDAL(r, gsub(".grid.Z", ".tif", fname), drivername="GTiff")

    load2postgres_raster(
                         ipaddress = destination_server,
                         u = "gislibrary", d = 'ewedb',
                         pgisutils = pgisutils, srid = 4283,
                         filename = gsub(".grid.Z", ".tif", fname),
                         out_schema="awap_grids",
                         out_table=outname, remove = T
                         )
  closeAllConnections()
  }

}

setwd(workdir)

#+end_src

*** COMMENT TODO load as function
#+name:load
#+begin_src R :session *R* :tangle no :exports none :eval no
    ################################################################
    # name:load
    ################################################################
    # name:load
    # Project: AWAP_GRIDS
    # Author: ivanhanigan
    # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
    # This file loads all the libraries and data files needed
    # Don't do any cleanup here
  
    ### Load any needed libraries
    #load(LibraryName)
    setwd(workdir)
    require(ProjectTemplate)
    load.project()
    p <- getPassword(remote=T)
    ch <- connect2postgres(h = '115.146.84.135', db = 'ewedb',
                           user = 'gislibrary',
                           p=p)
    print(paste('root directory:', workdir))
    setwd('data')
  
    start_at <- scope[[1]][1]
    print(start_at)
    end_at <- scope[[2]][1]
    print(end_at)
  
    vars <- scope[[3]]
    #  print(vars)
  
    #  started <- Sys.time()
    datelist_full <- as.data.frame(seq(as.Date(start_at),
      as.Date(end_at), 1))
    names(datelist_full) <- 'date'
    for(i in 1:length(vars[[1]])){
    #    i = 1
      measure_i <- vars[[1]][i]
      variable <- variableslist[which(variableslist$measure == measure_i),]
      vname <- as.character(variable[,1])
  
     tbls <- pgListTables(conn=ch, schema='awap_grids', pattern = measure_i)
  #     pattern=paste(measure_i,"_", gsub("-","",sdate), sep=""))
     pattern_x <- paste(measure_i,"_",sep="")
     tbls$date <- paste(
                    substr(gsub(pattern_x,"",tbls[,1]),1,4),
                    substr(gsub(pattern_x,"",tbls[,1]),5,6),
                    substr(gsub(pattern_x,"",tbls[,1]),7,8),
                    sep="-")
     tbls$date <- as.Date(tbls$date)
     datelist <-  which(datelist_full$date %in% tbls$date)
  
      if(length(datelist) == 0)
        {
          datelist <- datelist_full[,]
        } else {
          datelist <- datelist_full[-datelist,]
        }
  
  
      for(date_i in datelist)
      {
        date_i <- as.Date(date_i, origin = '1970-01-01')
        date_i <- as.character(date_i)
      #  print(date_i)
  
        sdate <- date_i
        edate <- date_i
      #}
        get_data_range(variable=as.character(variable[,1]),
                       measure=as.character(variable[,2]),
                       timestep=as.character(variable[,3]),
                       startdate=as.POSIXct(sdate),
                       enddate=as.POSIXct(edate))
  
        fname <- sprintf("%s_%s%s.grid.Z",measure_i,gsub("-","",sdate),gsub("-","",edate))
  
        if(file.info(fname)$size == 0)
          {
            file.remove(fname)
            next
          }
  
        if(os == 'linux')
          {
            uncompress_linux(filename = fname)
          } else {
            Decompress7Zip(zipFileName= fname, outputDirectory=getwd(), TRUE)
          }
  
        raster_aggregate(filename = gsub('.Z$','',fname),
          aggregationfactor = aggregation_factor, delete = TRUE)
        outname <- gsub('.tif',"", fname)
        outname <- substr(outname, 1, nchar(outname) - 8)
        load2postgres_raster(filename = gsub(".grid.Z", ".tif", fname),
          out_schema="awap_grids",
          out_table=outname)
  
      }
  
    }
  
    setwd(workdir)
  
#+end_src

*** TODO run multiple sessions
#+name:setupCLsession
#+begin_src sh :session *shell* :tangle src/setupCLsession.txt :exports none :eval no
################################################################
# name:setupCLsession
  R
  setwd('~/data/AWAP_GRIDS/')
  startdate <- '1993-01-18'
  enddate <- '1993-03-18'
  source('main.r')
#+end_src

*** check database size
#+name:check_dbsize
#+begin_src R :session *R* :tangle src/check_dbsize.r :exports none :eval no
  ################################################################
  # name:check_dbsize
   require(ProjectTemplate)
    load.project()
  
    require(swishdbtools)
    p <- getPassword(remote=T)
    ch <- connect2postgres(h = '115.146.84.135', db = 'ewedb', user=
                           'gislibrary', p = p)
    sql_subset(ch, x = 'dbsize', limit = -1, eval = TRUE)
  
#+end_src

*** COMMENT deprecated load loop
#+name:load
#+begin_src R :session *R* :tangle no :exports none :eval no
    ################################################################
    # name:load
    # Project: AWAP_GRIDS
    # Author: ivanhanigan
    # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  
    # This file loads all the libraries and data files needed
    # Don't do any cleanup here
  
    ### Load any needed libraries
    #load(LibraryName)
    require(ProjectTemplate)
    load.project()
  
    setwd('data')
    rootdir <- getwd()
    start_at <- scope[[1]][1]
    print(start_at)
    end_at <- scope[[2]][1]
    print(end_at)
    for(date_i in seq(as.Date(start_at), as.Date(end_at), 1))
    {
      date_i <- as.Date(date_i, origin = '1970-01-01')
      date_i <- as.character(date_i)
      print(date_i)
    
      sdate <- date_i
      edate <- date_i
      vars <- scope[[3]]
      print(vars)
     
    #  started <- Sys.time()
      for(i in 1:length(vars[[1]])){
  #     i <- 1
    #  variable <- variableslist[which(variableslist$measure == vars[[1]][i]),]
      variable <- variableslist[which(variableslist$measure == vars[[1]][i]),]
      vname <- as.character(variable[,1])
      #try(dir.create(vname))
      #setwd(vname)
      # TODO recognise if day not available to download
      get_data_range(variable=as.character(variable[,1]),measure =as.character(variable[,2]),timestep=as.character(variable[,3]),
                      startdate=as.POSIXct(sdate),
                      enddate=as.POSIXct(edate))
  
      files <- dir(pattern='.grid.Z$')
      if(os == 'linux'){
      for (f in files) {
        # f <- files[1]
        print(f)
        system(sprintf('uncompress %s',f))
      }
      } else {
       for (f in files) {
       if(!require(uncompress)) "find the old uncompress package off cran";
       require(uncompress)
       #f <- files[1]
       print(f)
       handle <- file(f, "rb")
       data <- readBin(handle, "raw", 99999999)
       close(handle)
       uncomp_data <- uncompress(data)
       handle <- file(gsub('.Z','',f), "wb")
       writeBin(uncomp_data, handle)
       close(handle)
       # clean up
       file.remove(f)
       }
      }
      files <- dir(pattern=".grid$")
      for(fname in files){
        # fname <- files[1]
        r <- raster(fname)
    #    writeGDAL(r, gsub('.grid','test1.TIF',fname), drivername="GTiff")
        #r <- raster(r)
        r <- aggregate(r, fact = aggregation_factor, fun = mean)
        writeRaster(r, gsub('.grid','.TIF',fname), format="GTiff",
      overwrite = TRUE)
        file.remove(fname)
      }
      files <- dir(pattern=".tif$")
      for(fname in files){
  #    fname <- files[1]
        outname <- gsub('.tif',"", fname)
        outname <- substr(outname, 1, nchar(outname) - 8)
        if(os == 'linux'){
  
         system(
  #         cat(
             paste(pgisutils,"raster2pgsql -s 4283 -I -C -M ",fname," -F awap_grids.",outname," > ",outname,".sql", sep="")
             )
         system(
           #cat(
           paste("psql -h 115.146.84.135 -U gislibrary -d ewedb -f ",outname,".sql",
                 sep = ""))
       } else {
         sink('raster2sql.bat')
         cat(paste(pgisutils,"raster2pgsql\" -s 4283 -I -C -M ",fname," -F awap_grids.",outname," > ",outname,".sql\n",sep=""))
  
         cat(
         paste(pgutils,"psql\" -h 115.146.84.135 -U gislibrary -d ewedb -f ",outname,".sql", sep = ""))
         sink()
         system('raster2sql.bat')
         file.remove('raster2sql.bat')
       }
      }
      files <- dir()
      # cleanup
      for(fname in files){
        file.remove(fname)
      }
      #setwd('..')
      }
     }
     setwd('..')
  
#+end_src
*** deprecated code
#+name:deprecated code
#+begin_src R :session *shell* :tangle no :exports none :eval no
###########################################################################
# newnode: deprecated code


      #}
  
      ## finished <- Sys.time()
      ## finished - started
      ## system('df -h')
      ## # newnode uncompress
      ## # test with one
      ## started <- Sys.time()
      ## for(i in 1:6){
      ## # i <- 1
      ## variable <- as.character(vars[i,1])
      ## print(variable)
      ## setwd(variable)
      ## files <- dir(pattern='.grid.Z')
      ## # files
      ## for (f in files) {
      ## # f <- files[1]
  
      ## # print(f)
      ## system(sprintf('uncompress %s',f))
      ## # grid2csv(gsub('.Z','',f))
      ## }
      ## setwd(rootdir)
      ## }
      ## finished <- Sys.time()
      ## finished - started
      ## system('df -h')
  
    #  files
    #  alreadyGot <- dir(file.path(workdir,paste('data',year,'-', year2, sep=''), vname), pattern='.grid')
    #  alreadyGot[1:10]
    #  gsub('.Z','',files) %in% alreadyGot
  
#+end_src

*** deprecated load

# don't let password get hardcoded
#p <- getPassword()
  
# ch <- connect2postgres(h = '115.146.84.135',
#                        d =  'ewedb',
#                        u = u,
#                        p = p)
  
  
# dat <- dbGetQuery(ch,
#                  "SELECT date, year, sla_code, minave, maxave, solarave, vprph09,vprph15
#                  FROM weather_sla.weather_sla
#                  where sla_code = 105051100 order by date
# ")
# with(dat, plot(date, maxave, type = 'l'))
  
** test upload to postgres

*** raster2pgsql
http://postgis.refractions.net/docs/using_raster.xml.html#RT_Raster_Loader
ie
raster2pgsql -s 4236 -I -C -M *.tif -F -t 100x100 public.demelevation > elev.sql
psql -d gisdb -f elev.sql
*** SQL extraction
#+name:sql-test
#+begin_src sql :tangle no :exports none :eval no
  
  -- TODO look at diff with ascii grid and geotiff
  -- http://blogs.esri.com/esri/arcgis/2010/12/21/rasters-get-speed-save-space/
  
  -- start with poa
  select poa_code, st_x(the_geom), st_y(the_geom)
  from abs_poa.actpoa01;
  
  select * from awap_grids.tmax2013010820130108 limit 1;
  -- try from postgis tute
  -- http://gis.stackexchange.com/questions/19856/intersecting-a-raster-with-a-polygon-using-postgis-artefact-error/19858#19858
  -- and http://www.mentby.com/Group/postgis-users/extract-a-set-of-wkt-raster-values-from-a-point-geometry-table.html
  CREATE TABLE caribou_srtm_inter AS
   SELECT poa_code, 
          (gv).geom AS the_geom, 
          (gv).val
   FROM (SELECT poa_code, 
                ST_Intersection(rast, the_geom) AS gv
         FROM awap_grids.tmax2013010820130108,
              abs_poa.actpoa01
         WHERE ST_Intersects(rast, the_geom)
        ) foo;
  
   CREATE TABLE result01 AS
   SELECT poa_code, 
          avg(val) AS tmax
   FROM caribou_srtm_inter
   GROUP BY poa_code
   ORDER BY poa_code;
  
   select t1.*,t2.tmax 
   into result02
   from abs_poa.actpoa01 t1
   join
   result01 t2
   on t1.poa_code = t2.poa_code
  
   alter table result02 add column gid2 serial primary key;
  
  -- worked but slow
   -- try NSW
   
  CREATE TABLE caribou_srtm_inter2 AS
   SELECT stnum, 
          (gv).geom AS the_geom, 
          (gv).val
   FROM (SELECT stnum, 
                ST_Intersection(rast, the_geom) AS gv
         FROM awap_grids.tmax2013010820130108,
              weather_bom.combstats
         WHERE ST_Intersects(rast, the_geom)
        ) foo;
  
  select * from caribou_srtm_inter2 limit 1;
  
   select t1.*,t2.tmax 
   into caribou_srtm_inter3
   from weather_bom.combstats t1
   join
   caribou_srtm_inter2 t2
   on t1.stnum = t2.stnum
  
   alter table caribou_srtm_inter3 add column gid2 serial primary key;
  
   -- try2 stations
  
  SELECT stnum,  (gv).val
  into try2
  FROM (
  SELECT pt.stnum, ST_Intersection(rt.rast, pt.the_geom) as gv
  FROM awap_grids.tmax2013010820130108 rt,
              weather_bom.combstats pt
  WHERE ST_Intersects(rast, the_geom)            
  ) foo
   
  --try3
  -- based on http://gis.stackexchange.com/questions/14960/postgis-raster-value-of-a-lat-lon-point
  --drop table try3;
  SELECT pt.stnum, ST_Value(rt.rast, pt.the_geom) as gv
  into try3
  FROM awap_grids.tmax2013010820130108 rt,
              (select * from weather_bom.combstats) pt
  WHERE ST_Intersects(rast, the_geom); 
  select * from try3;
  
  --drop table try3_1;
   select t1.*,t2.gv as tmax 
   into try3_1
   from weather_bom.combstats t1
   join
   try3 t2
   on t1.stnum = t2.stnum;
  
   alter table try3_1 add column gid2 serial primary key;
  
  -- with aggregated pixels
  --drop table try4;
  SELECT pt.stnum, ST_Value(rt.rast, pt.the_geom) as gv
  into try4
  FROM awap_grids.maxave_2013010820130108 rt,
              (select * from weather_bom.combstats) pt
  WHERE ST_Intersects(rast, the_geom); 
  select * from try4;
         
         --drop table try4_1;
          select t1.*,t2.gv as tmax 
   into try4_1
   from weather_bom.combstats t1
   join
   try4 t2
   on t1.stnum = t2.stnum;
  
   alter table try4_1 add column gid2 serial primary key;
  
  -- with bulk upload
  select * from awap_grids.maxave limit 1;
  --drop table try5;
  SELECT pt.stnum, rt.filename, ST_Value(rt.rast, pt.the_geom) as gv
  into try5
  FROM awap_grids.maxave rt,
              (select * from weather_bom.combstats) pt
  WHERE ST_Intersects(rast, the_geom); 
  select * from try5 where stnum = 91004;
  
#+end_src

** test geotiff
save storage space as geotiff
#+name:load
#+begin_src R :session *R* :tangle src/qc-geotiff.r :exports none :eval no
  ################################################################
  # name:test geotiff
  
    rootdir <- paste(getwd(),'/',variableslist[v,1],sep='')
    #  dir(rootdir)[1]
    cfiles <- dir(rootdir)
    cfiles <- cfiles[grep(as.character(variableslist[v,2]), cfiles)]
    fname <- cfiles[[i]]
  
    r <- readGDAL(file.path(rootdir,fname))
    outfile <- gsub('.grid', '.TIF', fname)
    writeGDAL(r, file.path(rootdir, outfile), drivername="GTiff")
    r <- readGDAL(file.path(rootdir,outfile))
  
#+end_src
** test readGDAL
#+name:test-readGDAL
#+begin_src R :session *shell* :tangle tests/test-readGDAL2.r :exports none :eval no
    ################################################################
    # name:test-readGDAL
    require(raster)
    readGDAL2 <- function(hostip=NA,user=NA,db=NA, schema= NA, table=NA, p = NA) {
     if (!require(rgdal)) install.packages('rgdal', repos='http://cran.csiro.au'); require(rgdal)
     if(is.na(p)){
     pwd=readline('enter password (ctrl-L will clear the console after): ')
     } else {
     pwd <- p
     }
     r <- readGDAL(sprintf('PG:host=%s
                             user=%s
                             dbname=%s
                             password=%s
                             table=%s
                             schema=%s
                             port=5432',hostip,user,db,pwd, table, schema)
                            # layer=layer
                   )
     return(r)
    }
    
    # bah
    require(swishdbtools)
    p <- getPassword(remote=F)
  #dbSendQuery(ch, "drop table awap_grids.maxave_20130101")
  r <- readGDAL2('tern5.qern.qcif.edu.au', 'gislibrary', 'ewedb',
                 schema = 'awap_grids', table = 'maxave_19881005', p = p
  )
  image(r)
  writeGDAL(r, '~/test1.TIF',drivername="GTiff")
  ch <- connect2postgres("tern5.qern.qcif.edu.au","ewedb", user="gislibrary", p)
  tbls <- pgListTables(ch, "awap_grids")
  nrow(tbls)
  nrow(tbls)/60000
  
  
    r <- readGDAL(sprintf("PG:host=115.146.84.135 port=5432 dbname='ewedb' user='gislibrary' password='%s' schema='awap_grids' table=maxave_20130108", p))
    
    r2 <- raster(r)
    r3 <- aggregate(r2, fact=2, fun = mean)
    
    writeRaster(r3, 'data/test2.TIF',format="GTiff")
    
                                            #writeGDAL(r3, "PG:host=115.146.84.135 port=5432 dbname='ewedb' user='gislibrary' password='' schema='awap_grids' table=tmax20130108201301082")
  # gdalinfo  "PG:host=115.146.84.135 port=5432 dbname='ewedb' user='gislibrary' password='' schema='awap_grids' table=tmax2013010820130108"
  
#+end_src

** test uncompress
#+name:test-uncompress
#+begin_src R :session *R* :tangle src/test-uncompress.r :exports none :eval no
################################################################
# name:test-uncompress
#http://cran.r-project.org/src/contrib/Archive/uncompress/uncompress_1.34.tar.gz
install.packages("C:/Users/Ivan/Downloads/uncompress_1.34.tar.gz", repos = NULL, type = "source")
require(uncompress)
?uncompress


files <- dir(pattern='.grid.Z')
strt=Sys.time()
for (f in files) {
   f <- files[1]
  print(f)
  handle <- file(f, "rb")
  data <- readBin(handle, "raw", 99999999)
  close(handle)
  uncomp_data <- uncompress(data)
  handle <- file(gsub('.Z','',f), "wb")
  writeBin(uncomp_data, handle)
  close(handle)
  
  # clean up
  #file.remove(f)
}

endd=Sys.time()
print(endd-strt)

sink('test.bat')
cat("\"C:\\pgutils\\postgis-pg92-binaries-2.0.2w64\\bin\\raster2pgsql\" -s 4283 -I -C -M *.grid -F awap_grids.maxave_aggby3 > maxave_aggby3.sql")
sink()
system('test.bat')
#+end_src


** unresponsive psql on some raster2pgsql.sql files
#+name:restarts
#+begin_src sh :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:restarts
  ssh ivan_hanigan@130.56.102.53
  
  cd data/AWAP_GRIDS/data
  ls
  rm *
  
  R
  setwd('~/data/AWAP_GRIDS/')
  startdate <- '1993-01-18'
  enddate <- '1993-03-18'
  source('main.r')
  
#+end_src

** move from rawdata (or 5 year chunks) to one year Directories
#+name:file-rename-to-annual
#+begin_src R :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:file-rename-to-annual
  require(ProjectTemplate)
  load.project()
  
  files <- dir('RawData', full.names = T, recursive = TRUE)
  files[1:20]
  for(v in vars[[1]]){
  #  v <- vars[[1]][2]
  vfiles <- files[grep(v, files)]
  for(fname in vfiles){
  #  fname <- vfiles[1]
    year <- substr(strsplit(fname,'_')[[1]][2],1,4)
    variablename <- strsplit(strsplit(fname,'_')[[1]][1],'/')[[1]][2]
    try(dir.create(file.path('data',variablename, year), recursive =
                   TRUE))
    outfile <- file.path('data',variablename, year, strsplit(fname,'/')[[1]][3])
    file.rename(fname, outfile)
  }
  }
  
#+end_src

** clean

*** COMMENT check-duplicates-src
#+name:check-duplicates
#+begin_src R :session *R* :tangle diagnostics/check_duplicates.r :exports none :eval no
  ################################################################
  # name:check-duplicates
  # in 23oct2007, and from 08jan2009 to  17apr2010, vprph09 and vprph15
  # are the same.
  require(ProjectTemplate)
  load.project()
  require(devtools)
  install_github("awaptools", "swish-climate-impact-assessment")
  ch <- connect2postgres2("ewedb")
  pwd <- get_passwordTable()
  pwd <- pwd[which(pwd$V3 == 'ewedb'),5]
  datesList <- seq(as.Date("2010-01-02"), as.Date("2010-01-05"), 1)
  date_j <- datesList[1]
  print(date_j)
  
  r <- readGDAL2("115.146.84.135", "gislibrary", "ewedb", "awap_grids",
                 "maxave_20130305", pwd)
  image(r)
  #rm(sus_dates)
  system.time(
  sus_dates <- check_duplicates(ch, dates = datesList, measures = c("vprph09", "vprph15"), measure_name = "vprph")
    )
  
  system.time(
  sus_dates <- check_duplicates(ch, dates = datesList, measures = c("maxave", "minave"), measure_name = "temp")
    )
  #unlist(sus_dates)
  
#+end_src
*** COMMENT check-duplicates-report-code
#+name:check-duplicates-report
#+begin_src R :session *R* :tangle diagnostics/check_duplicates_report.r :exports none :eval no
  ################################################################
  # name:check-duplicates-report
  measure_name <- "vprph"
  sus_dates <- read.table(paste("~/data/AWAP_GRIDS/sus_dates_",measure_name,".csv",sep = ""), quote="\"")
  sus_dates$date <- paste(substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),1,4),
    substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),5,6),
    substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),7,8),
    sep="-")
  sus_dates$date <- as.Date(sus_dates$date)
  head(sus_dates)
  
  full_dates <- as.data.frame(c(as.Date('2007-10-23'), seq(as.Date('2009-01-08'), as.Date('2010-04-17'),1)))
  names(full_dates) <- 'date'
  sus_dates2 <- merge(full_dates, sus_dates, all.x=TRUE)
  sus_dates2[which(is.na(sus_dates2$V1)),]
  head(sus_dates2)
  
#+end_src
*** COMMENT remove_duplicates-code
#+name:remove_duplicates
#+begin_src R :session *R* :tangle diagnostics/remove_duplicates.r :exports none :eval no
  ################################################################
  # name:remove_duplicates
  require(ProjectTemplate)
  load.project()
  ch <- connect2postgres2("ewedb")
  
  
  source("diagnostics/check_duplicates_report.r")
  sus_dates2
  for(date_i in as.character(sus_dates2$date[-c(1:3)]))
    {
  #    date_i <- as.character(sus_dates2$date[1])
      print(date_i)
      date_j <- gsub("-","", date_i)
      for(measure_i in c("vprph09", "vprph15"))
        {
  #        measure_i <- "vprph09"
          print(measure_i)
          dbSendQuery(ch,
  #        cat(
                      sprintf("drop table awap_grids.%s_%s; ", measure_i, date_j)
                      )
        }
  
    }
  
  
  # now run the kepler file
  # some were missed?
  sus_dates <- pgListTables(ch, "awap_grids", "vprph09")
  measure_i <- "vprph09"
  sus_dates$date <- paste(substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),1,4),
      substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),5,6),
      substr(gsub(paste(measure_i,"_",sep=""),"",sus_dates[,1]),7,8),
      sep="-")
    sus_dates$date <- as.Date(sus_dates$date)
    head(sus_dates)
  
  
    full_dates <- as.data.frame(c(as.Date('2007-10-23'), seq(as.Date('2009-01-08'), as.Date('2010-04-17'),1)))
    names(full_dates) <- 'date'
    sus_dates2 <- merge(full_dates, sus_dates, all.x=TRUE)
    sus_dates2[which(is.na(sus_dates2$relname)),]
    head(sus_dates2)
    tail(sus_dates2)
    subset(sus_dates2, date == as.Date("2009-02-12"))
    pwd <- getPassword()
    r <- readGDAL2("115.146.84.135", "gislibrary", "ewedb", "awap_grids",
                   "vprph09_20090212", pwd)
    image(r)
    #rm(sus_dates)
  
#+end_src


*** COMMENT deprecated-check-duplicates-sql-code
#+name:deprecated-check-duplicates-sql
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:deprecated-check-duplicates-sql



    
  # or on db
  measures = c("vprph09","vprph15")
  #measures <- c("maxave","minave", "solarave","totals",
  #suspicious_dates <- list()
  
  dbSendQuery(ch, "drop table sus_dates")
  system.time(
  for(j in 1:length(datesList))
      {
  #      j = 1
        #date_j <- dates[2]
        date_j <- datesList[j]
        date_i <- gsub("-","",date_j)
        print(date_i)
  #      rasters <- list()
  
  ## for(i in 1:length(measures))
  ##       {
          i = 1
          measure <- measures[i]
          print(measure)
          rastername <- paste(measure, "_", date_i, sep ="")
          tableExists <- pgListTables(ch, schema="awap_grids", pattern=rastername)
          if(nrow(tableExists) > 0)
          {
          i = 2
          measure <- measures[i]
          print(measure)
          rastername2 <- paste(measure, "_", date_i, sep ="")
  if(date_j == datesList[1])
    {
  dbSendQuery(ch,
  #          cat(
            paste("
            select cast('",as.character(date_j),"' as date) as
  sus_dates, (foo.rastval2).min, (foo.rastval2).max,
  (foo.rastval2).mean
            into sus_dates
            from
            (
            select t1.*, t2.*, st_summarystats(ST_MapAlgebraExpr(t1.rast, t2.rast,'[rast1.val] / [rast2.val]', '2BUI')) as rastval2
            from awap_grids.",rastername," t1,
            awap_grids.",rastername2," t2
            where st_intersects(t1.rast, t2.rast)
            ) foo
            ", sep = "")
            )
  } else {
  dbSendQuery(ch,
  #          cat(
            paste("insert into sus_dates (sus_dates, min, max, mean)
            select cast('",as.character(date_j),"' as date) as
  sus_dates, (foo.rastval2).min, (foo.rastval2).max,
  (foo.rastval2).mean
  
            from
            (
            select t1.*, t2.*, st_summarystats(ST_MapAlgebraExpr(t1.rast, t2.rast,'[rast1.val] / [rast2.val]', '2BUI')) as rastval2
            from awap_grids.",rastername," t1,
            awap_grids.",rastername2," t2
            where st_intersects(t1.rast, t2.rast)
            ) foo
            ", sep = "")
            )
  }
  }
  }
  )
  sus_dates2 <- sql_subset(ch, 'sus_dates', subset = "mean = 1", eval = T)
  unlist(sus_dates)
  sus_dates2
  dir()
  


  ## sql <- sql_subset(ch, paste("awap_grids.",rastername,sep=""), limit = 1, eval = F, check = F)
  ## cat(sql)
  ## compare <- dbGetQuery(ch,
  ## #          cat(
  ##           paste("
  ##           select cast('",as.character(date_j),"' as date) as sus_dates, (foo.rastval2).min, (foo.rastval2).max,  (foo.rastval2).mean
  ##           from
  ##           (
  ##           select t1.*, t2.*, st_summarystats(ST_MapAlgebraExpr(t1.rast, t2.rast,'[rast1.val] / [rast2.val]', '2BUI')) as rastval2
  ##           from awap_grids.",rastername," t1,
  ##           awap_grids.",rastername2," t2
  ##           where st_intersects(t1.rast, t2.rast)
  ##           ) foo
  ##           ", sep = "")
  ##           )
  ## compare
  ## "
  ## select t1.*, t2.*
  ## from awap_grids.vprph09_20100401 t1,
  ## awap_grids.vprph09_20100401 t2
  ## where st_intersects(t1.rast, t2.rast)
  ## ")
  ## dbSendQuery(ch,
  ## #          cat(
  ##           paste("
  ##           select cast('",as.character(date_j),"' as date) as sus_dates, (foo.rastval2).min, (foo.rastval2).max,  (foo.rastval2).mean
  ##           from
  ##           (
  ##           select t1.*, t2.*, st_summarystats(ST_MapAlgebraExpr(t1.rast, t2.rast,'[rast1.val] / [rast2.val]', '2BUI')) as rastval2
  ##           from awap_grids.",rastername," t1,
  ##           awap_grids.",rastername2," t2
  ##           where st_intersects(t1.rast, t2.rast)
  ##           ) foo
  ##           ", sep = "")
  ##           )
  
#+end_src

*** clean-check-against-stations
#+name:checkAstation
#+begin_src R :session *shell* :tangle src/check-against-stations.r :exports none :eval no
  # based on
  # ~/projects/swish-climate-impact-assessment.github.com/tools/ExtractAWAPdata4locations
  
  # this script runs the ExtractAWAPGRIDS functions for sample locations
  # depends on swishdbtools package from http:/swish-climate-impact-assessment.github.com
  # eg
  workingdir <- "~/data/AWAP_GRIDS/data"
  setwd("~/data/AWAP_GRIDS")
  # eg
  percentSample <- 0.1
  #fileName <-  "zones.xlsx"
  # eg
  outputFileName <- "locations.shp"
  # eg
  outputDataFile <- "check-against-stations.csv"
  # eg
  StartDate <- "2010-01-01" 
  # eg
  EndDate <- "2010-01-01" 
    
  ################################################################
  # name: Get-selected-stations
  # want to get a set of stations that observed any of our awap variables
  require(swishdbtools)
  p  <- getPassword(remote = T)
  ch <- connect2postgres("tern5.qern.qcif.edu.au", "ewedb", "gislibrary", p = p)
  tbls  <- pgListTables(ch, "weather_bom")
  tbls
  # vprph
  sql  <- sql_subset(ch, "weather_bom.bom_3hourly_data_1990_2010_master",
                     select = "distinct station_number",
                     subset = "quality_of_vapour_pressure = 'Y'",
                     eval = T
                     )
  head(sql)  
  nrow(sql)
  # 953
  # temp
  sql2  <- sql_subset(ch, "weather_bom.bom_3hourly_data_1990_2010_master",
                     select = "distinct station_number",
                     subset = "quality_of_air_temperature = 'Y'",
                     eval = T
                     )
  head(sql2)  
  nrow(sql2)
  # 980
  # rain
  sql3  <- sql_subset(ch, "weather_bom.bom_3hourly_data_1990_2010_master",
                     select = "distinct station_number",
                     subset = "quality_of_precipitation = 'Y'",
                     eval = T
                     )
  head(sql3)  
  nrow(sql3)  
  # 948
  stations  <- merge(sql, sql2)
  nrow(stations)
  # 953
  stations  <- merge(stations, sql3)
  nrow(stations)
  # 943
  write.csv(stations, file.path(workingdir, "selected-stations.csv"), row.names = F)
  
  ################################################################
  # name: GeoCode-selected-stations
  require(swishdbtools)
  ch <- connect2postgres2("ewedb")
  stations  <- sql_subset(ch, "weather_bom.combstats", eval = T)
  nrow(stations)
  # 8139
  # only on mainland
  stations <- subset(stations, lat > -50 & lon < 160)
  # only those with observations of all vars
  selectedStations  <- read_file(file.path(workingdir, "selected-stations.csv"))
  head(stations)
  head(selectedStations)
  stations  <- merge(stations, selectedStations, by.x = "stnum", by.y = "station_number")
  nrow(stations)
  # 939
  sampled  <- sample(stations$stnum, percentSample * nrow(stations))
  length(sampled)
  # 93
  locations  <- stations[which(stations$stnum %in% sampled),]
  names(locations) <- gsub("lon", "long", names(locations))
  names(locations) <- gsub("stnum", "address", names(locations))
  # not gid
  locations <- locations[,-c(which(names(locations) == "gid"))]
  nrow(locations)
  
  
  epsg <- make_EPSG()
  df <- SpatialPointsDataFrame(cbind(locations$long,locations$lat),locations,                             
                               proj4string=CRS(epsg$prj4[epsg$code %in% "4283"])
                               )
  setwd(workingdir)
  if(file.exists(outputFileName))
  {
    for(ext in c(".shp", ".shx", ".dbf", ".prj"))
    {
      file.remove(gsub(".shp",ext,outputFileName))
    }
  }
  writeOGR(df,outputFileName,gsub(".shp","",outputFileName),"ESRI Shapefile")
  tempTableName <- outputFileName
  
  ################################################################
  # name: send2postgis
  require(swishdbtools)
  ch <- connect2postgres2("ewedb")
  locations <- read_file(file.path(workingdir,tempTableName))
  locations <- locations@data
  
  if(!require(oz)) install.packages("oz"); require(oz)
  require(maps)
  require(fields)
  png("reports/selected-stations.png")
  with(stations, plot(lon, lat, pch = 16, xlim =c(112,155), cex = .5))
  with(locations, points(long, lat, pch = 19, col = 'red'))
  oz(add = T)
  map.scale(ratio=F)
  dev.off()
  
  r <- readGDAL2("115.146.92.162","gislibrary","ewedb","awap_grids","maxave_20130118",p=p)
  
  png("reports/grid-nsw.png", width = 500, height = 400)
  
  zs <- c(15,48)
  par(oma=c( 0,0,0,4)) # margin of 4 spaces width at right hand side
  oz(sections=4, xlim=c(140,155), ylim = c(-38,-28))
  image(r, add = T,  zlim=zs, col=tim.colors())
  oz(add=T)
  map.scale(ratio=F)
  box()
  title(main="maximum temperature (C) 2013-01-18")
  par(oma=c( 0,0,0,1))# reset margin to be much smaller.
  image.plot( legend.only=TRUE, zlim=zs) 
  
  dev.off()
  
  
  tempTableName <- swish_temptable()
  dbWriteTable(ch, tempTableName$table, locations, row.names = F)
  tested <- sql_subset(ch, tempTableName$fullname, eval = T)
  #tested
  tempTableName <- tempTableName$fullname
  tempTableName
  
  # points2geom
  sch <- strsplit(tempTableName, "\\.")[[1]][1]
  tbl <- strsplit(tempTableName, "\\.")[[1]][2]
  sql <- points2geom(
    schema=sch,
    tablename=tbl,
    col_lat= "lat",col_long="long", srid="4283"
  )
  # cat(sql)
  dbSendQuery(ch, sql)
  tbl
  
  ################################################################
  # name: R_raster_extract_by_day
  require(swishdbtools)
  require(awaptools)
  if(!require(reshape))  install.packages("reshape", repos="http://cran.csiro.au/"); require(reshape);
  tempTableName_locations <- tbl
  startdate <- StartDate
  enddate <- EndDate
  ch<-connect2postgres2("ewedb")
  tempTableName <- swish_temptable("ewedb")
  
  raster_extract_by_day(ch, startdate, enddate,
                        schemaName = tempTableName$schema,
                        tableName = tempTableName$table,
                        pointsLayer = tempTableName_locations,
                        measures = c("maxave", "minave", "totals", "vprph09", "vprph15")
  )
  
  output_data <- reformat_awap_data(
    tableName = tempTableName$fullname
  )
  
  outputDataFile <- file.path(workingdir, outputDataFile)
  write.csv(output_data,outputDataFile, row.names = FALSE)
  outputFileName <- outputDataFile
  outputFileName
  
  ################################################################
  # name: get the observed data for these
  require(swishdbtools)
  require(reshape)
  p <- getPassword(remote = T)
  ch <- connect2postgres("tern5.qern.qcif.edu.au", "ewedb", "gislibrary", p = p)
  # all the stations are in
  # selectedStations  <- read_file(file.path(workingdir, "selected-stations.csv"))
  check_against_stations <- read.csv("~/data/AWAP_GRIDS/data/check-against-stations.csv")
  check_against_stations$date <- as.Date(check_against_stations$date)
  head(check_against_stations)
  stnum_ids  <- sample(1:93, 4)
  locations[stnum_ids,]
  stnums <- locations[stnum_ids,1]
  stnames <- locations[stnum_ids,2]
  png("reports/sampled-timeseries-from-grid.png", width = 800, height = 500)
  par(mfrow = c(2,2))
  for(j in 1:4){
    with(subset(check_against_stations, address == stnums[j]), plot(date, maxave, type = "l"))
    title(main = paste("maxt, ", stnames[j], "(", format(locations$long[j], digits = 4), ", ", format(locations$lat[j], digits = 4), ")"), cex = .6)
  }
  dev.off()
  
  selectedStations <- names(table(check_against_stations$address))
  head(selectedStations)
  length(selectedStations)
  for(hour in c(15))
  {
  #hour <- 9
  d <- dbGetQuery(ch,
  # cat(
   paste("SELECT  station_number as address, name, cast(year || '-' || month || '-' ||  day as date) as date, hour, \"timestamp\" ,     t2.lat ,     lon,
         vapour_pressure_in_hpa
    FROM weather_bom.bom_3hourly_data_1990_2010_master join weather_bom.combstats t2
    on station_number = stnum
    where station_number in ('",
         paste(selectedStations, sep = "", collapse = c("','")),
         "')
    and hour = ", hour ,"
    and quality_of_vapour_pressure = 'Y'
    order by day, hour
   ", sep = "")
  )
                  
  #head(d)
  #str(d)
  ## with(d,
  ##      plot(
  ##        as.POSIXct(timestamp), vapour_pressure_in_hpa,type='b',pch=16
  ##        )
  ##      )
  
  ##  # get mean absolute difference with the grid vs stations
  #str(check_against_stations)
  df <- merge(check_against_stations, d)
  #head(df)
  
  # plots 
    if(hour == 9){
    fit <- lm(df$vprph09 ~ df$vapour_pressure_in_hpa)
    summary(fit)
    # Multiple R-squared: 0.969,
    png("reports/vprph09.png")
    plot(df$vapour_pressure_in_hpa, df$vprph09)
    #abline(0,1, col = 'blue')
    abline(fit, col = 'red')
    legend("topright", legend = paste("R2 is ", format(summary(fit)$adj.r.squared, digits = 4)))
    dev.off()
    } else {
    fit <- lm(df$vprph15 ~ df$vapour_pressure_in_hpa)
    #summary(fit)
    png("reports/vprph15.png")
    plot(df$vapour_pressure_in_hpa, df$vprph15)
    #abline(0,1, col = 'blue')
    abline(fit, col = 'red')
    legend("topright", legend = paste("R2 is ", format(summary(fit)$adj.r.squared, digits = 4)))
    dev.off()  
    }
  }
  # great stuff. now temps and rain
  names(sql_subset(ch, "weather_bom.bom_daily_data_1990_2010", limit = 1, eval = T))
  # [1] "station_number"                                                 
  # [2] "year"                                                           
  # [3] "month"                                                          
  # [4] "day"  
  # [5] "global_solar_exposure_at_location_derived_from_satellite_data_i"
  # [6] "quality_of_global_solar_exposure_value"                         
  # [7] "precipitation_in_the_24_hours_before_9am_local_time_in_mm"      
  # [8] "quality_of_precipitation_value"                                 
  # [9] "number_of_days_of_rain_within_the_days_of_accumulation"         
  # [10] "accumulated_number_of_days_over_which_the_precipitation_was_mea"
  # [11] "maximum_temperature_in_24_hours_after_9am_local_time_in_degrees"
  # [12] "quality_of_maximum_temperature_in_24_hours_after_9am_local_time"
  # [13] "days_of_accumulation_of_maximum_temperature"                    
  # [14] "minimum_temperature_in_24_hours_before_9am_local_time_in_degree"
  # [15] "quality_of_minimum_temperature_in_24_hours_before_9am_local_tim"
  d <- dbGetQuery(ch,
                  # cat(
                  paste("SELECT  station_number as address, name, cast(year || '-' || month || '-' ||  day as date) as date, t2.lat ,     lon,
         maximum_temperature_in_24_hours_after_9am_local_time_in_degrees,
                        quality_of_maximum_temperature_in_24_hours_after_9am_local_time,
                        minimum_temperature_in_24_hours_before_9am_local_time_in_degree,
                        quality_of_minimum_temperature_in_24_hours_before_9am_local_tim,
                        precipitation_in_the_24_hours_before_9am_local_time_in_mm,
                        quality_of_precipitation_value
                        FROM weather_bom.bom_daily_data_1990_2010 join weather_bom.combstats t2
                        on station_number = stnum
                        where station_number in ('",
         paste(selectedStations, sep = "", collapse = c("','")),
                        "') 
                        ", sep = "")
  )
  str(d)
  
  str(check_against_stations)
  df <- merge(check_against_stations, d)
  head(df)
    
  # plots 
  fit <- lm(df$maxave ~ df$maximum_temperature_in_24_hours_after_9am_local_time_in_degrees)
  summary(fit)
  png("reports/maxave.png")
  plot(df$maximum_temperature_in_24_hours_after_9am_local_time_in_degrees, df$maxave)
  #abline(0,1, col = 'blue')
  abline(fit, col = 'red')
  legend("topright", legend = paste("R2 is ", format(summary(fit)$adj.r.squared, digits = 4)))
  dev.off()
  
  fit <- lm(df$minave ~ df$minimum_temperature_in_24_hours_before_9am_local_time_in_degree)
  #summary(fit)
  png("reports/minave.png")
  plot(df$minimum_temperature_in_24_hours_before_9am_local_time_in_degree, df$minave)
  #abline(0,1, col = 'blue')
  abline(fit, col = 'red')
  legend("topright", legend = paste("R2 is ", format(summary(fit)$adj.r.squared, digits = 4)))
  dev.off()  
  
  fit <- lm(df$totals ~ df$precipitation_in_the_24_hours_before_9am_local_time_in_mm)
  #summary(fit)
  png("reports/totals.png")
  plot(df$precipitation_in_the_24_hours_before_9am_local_time_in_mm, df$totals)
  #abline(0,1, col = 'blue')
  abline(fit, col = 'red')
  legend("topright", legend = paste("R2 is ", format(summary(fit)$adj.r.squared, digits = 4)))
  dev.off()  
  
  
  ################################################################
  # name: tidy up
  require(swishdbtools)
  ch<-connect2postgres2("ewedb")
  sch <- swish_temptable("ewedb")
  sch <- sch$schema
  tbls <- pgListTables(ch, sch, table="foo", match = FALSE)
  tbls
  for(tab in tbls[,1])
  {
    dbSendQuery(ch, 
                sprintf("drop table %s.%s", sch, tab)
    )
  }
    
  
  
  
  
  
  
#+end_src
*** COMMENT deprecated checkAstation
#+name:clean
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:clean
  # Project: AWAP_GRIDS
  # Author: ivanhanigan
  # Maintainer: Who to complain to <ivan.hanigan@gmail.com>
  require(ProjectTemplate)
  load.project()
  
  ch <- connect2postgres(h = '115.146.84.135', db = 'ewedb', user= 'gislibrary')
  start_at <- '2012-01-01'
  end_at <- '2013-01-20'
  datelist_full <- as.data.frame(seq(as.Date(start_at),
    as.Date(end_at), 1))
  names(datelist_full) <- 'date'
  
  measure_i <- 'maxave'
  tbls <- pgListTables(conn=ch, schema='awap_grids', table = measure_i, match = F)
  
  pattern_x <- paste(measure_i,"_",sep="")
  tbls$date <- paste(
                 substr(gsub(pattern_x,"",tbls[,1]),1,4),
                 substr(gsub(pattern_x,"",tbls[,1]),5,6),
                 substr(gsub(pattern_x,"",tbls[,1]),7,8),
                 sep="-")
  tbls$date <- as.Date(tbls$date)
  datelist <- which(datelist_full$date %in% tbls$date)
  
  if(length(datelist) == 0)
  {
    datelist <- datelist_full[,]
  } else {
    datelist <- datelist_full[datelist,]
  }
  
  tbl_exists <- pgListTables(conn=ch, schema='awap_grids', table =
                             paste(measure_i,"_join_stations",
                                   sep = ""), match = T
                             )
  tbl_exists
  for(date_i in datelist)
  {
  #  date_i <- datelist[2]
    date_i <- as.Date(date_i, origin = '1970-01-01')
    date_i <- as.character(date_i)
  #  print(date_i)
  
    date_name <- gsub('-','',date_i)
  
    if(which(date_i == datelist) == 1 & nrow(tbl_exists))
    {
    dbSendQuery(ch,
    #  cat(
      paste("drop table awap_grids.",measure_i,"_join_stations",
            sep = "")
      )
    }
  
    if(which(date_i == datelist) == 1)
    {
    dbSendQuery(ch,
    #  cat(
      paste("SELECT pt.stnum, cast('",date_i,"' as date) as date,
        ST_Value(rt.rast, pt.the_geom) as ",measure_i,"
      into awap_grids.",measure_i,"_join_stations
      FROM awap_grids.",measure_i,"_",date_name," rt,
           weather_bom.combstats pt
      WHERE ST_Intersects(rast, the_geom)
      ", sep ="")
      )
    } else {
    dbSendQuery(ch,
    #  cat(
      paste("insert into awap_grids.",measure_i,"_join_stations
      SELECT pt.stnum, cast('",date_i,"' as date) as date,
        ST_Value(rt.rast, pt.the_geom) as ",measure_i,"
      FROM awap_grids.",measure_i,"_",date_name," rt,
           weather_bom.combstats pt
      WHERE ST_Intersects(rast, the_geom)
      ", sep ="")
      )
    }
  }
  
  qc <- dbGetQuery(ch,
                   "select *
                   from awap_grids.maxave_join_stations
                   where stnum = 70351
                   order by date
                   ")
  sql_subset(ch, x='maxave_join_stations', subset="stnum = 70351",
                schema="awap_grids", limit=10, eval=F)
  qc <- sql_subset_into(ch, x='maxave_join_stations', subset="stnum = 70351",
    schema="awap_grids", into_schema = 'awap_grids', into_table = 'maxave_join_stations2', limit=-1, eval=T)
  str(qc)
  qc <- dbGetQuery(ch, "select * from awap_grids.maxave_join_stations2")
  qc <- arrange(qc,by=qc$date)
  with(qc, plot(date, maxave, type = 'l'))
  tail(qc)
  
  qc2 <- EHIs(analyte = qc,
                   exposurename = 'maxave',
                   datename = 'date',
                   referencePeriodStart = as.Date('1980-1-1'),
                   referencePeriodEnd = as.Date('2000-12-31'),
                   nlags = 32)
  head(qc2)
  hist(subset(qc2, EHF >= 1)[,'EHF'])
  threshold <- quantile(subset(qc2, EHF >= 1)[,'EHF'], probs=0.9)
  
  with(qc, plot(date, maxave, type = 'l'))
  with(subset(qc2, EHF > threshold), points(date, maxave, col = 'red', pch = 16))
#+end_src

*** COMMENT deprecated-clean
#+name:deprecated-clean
#+begin_src R :session *R* :tangle no :exports none :eval no
################################################################
# name:deprecated-clean




    # enter password at console
    shp <- dbGetQuery(ch, 'select stnum, lat, lon from weather_bom.combstats')
  #  shp <- dbGetQuery(ch, 'select sla_code, st_x(st_centroid(the_geom)) as lon, st_y(st_centroid(the_geom)) as lat from abs_sla.aussla01')
    nrow(shp)
    if (!require(rgdal)) install.packages('rgdal'); require(rgdal)
    epsg <- make_EPSG()
  
    ## Treat data frame as spatial points
    shp <- SpatialPointsDataFrame(cbind(shp$lon,shp$lat),shp,
                                  proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
    str(shp)
    head(shp@data)
    ## #writeOGR(shp, 'test.shp', 'test', driver='ESRI Shapefile')
  
  
    #################################
    # start getting CCD temperatures
    #setwd(rootdir)
  #  started <- Sys.time()
  #  for(v in 4:6){
     v = 1
    rootdir <- paste(getwd(),'/',variableslist[v,1],sep='')
  #  dir(rootdir)[1]
    cfiles <- dir(rootdir)
    cfiles <- cfiles[grep(as.character(variableslist[v,2]), cfiles)]
  
  #    for (i in seq_len(length(cfiles))) {# solar failed at this day 494:length(cfiles)){
      #   i <- 1
        #i <- grep('20000827',cfiles)
        fname <- cfiles[[i]]
        variablename <- strsplit(fname, '_')[[1]][1]
        timevar <- gsub('.TIF', '', strsplit(fname, '_')[[1]][2])
        timevar <- substr(timevar, 1,8)
        year <- substr(timevar, 1,4)
        month <- substr(timevar, 5,6)
        day <- substr(timevar, 7,8)
        timevar <- as.Date(paste(year, month, day, sep = '-'))
        r <- raster(file.path(rootdir,fname))
        e <- extract(r, shp, df=T)
        str(e) ## print for debugging
        image(r)
        plot(shp, add = T)
  
#+end_src

** do
#+name:do
#+begin_src R :session *R* :tangle src/do.r :exports none :eval no
################################################################
# name:do
# The actual work

#+end_src
*** COMMENT calculate-for-grid-code
#+name:calculate-for-grid
#+begin_src R :session *R* :tangle src/calculate-for-grid.r :exports none :eval no
  ################################################################
  # name:calculate-for-grid
  
  # based on
  # ~/projects/swish-climate-impact-assessment.github.com/tools/ExtractAWAPdata4locations
  
  # this script runs the ExtractAWAPGRIDS functions for sample locations
  # depends on swishdbtools package from http:/swish-climate-impact-assessment.github.com
  # eg
  workingdir <- "~/data/AWAP_GRIDS/data" 
  # eg
  outputFileName <- "locations.shp"
  # eg
  outputDataFile <- "calculate-for-grid.csv"
  # eg
  StartDate <- "2013-01-10" 
  # eg
  EndDate <- "2013-01-20" 
    
  ################################################################
  # name: Get-test-grid-locations
  require(rgdal)
  res=0.15
  xs=seq(112,155,res)
  ys=seq(-45,-9,res)
  d=expand.grid(xs,ys)
  head(d)
  
  
  epsg <- make_EPSG()
  df <- SpatialPointsDataFrame(cbind(d$Var1,d$Var2),d,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  plot(df, pch = 16)
  setwd(workingdir)
  if(file.exists(outputFileName))
  {
    for(ext in c(".shp", ".shx", ".dbf", ".prj"))
    {
      file.remove(gsub(".shp",ext,outputFileName))
    }
  }
  writeOGR(df,outputFileName,gsub(".shp","",outputFileName),"ESRI Shapefile")
  tempTableName <- outputFileName
  
  ################################################################
  # name: send2postgis
  require(swishdbtools)
  ch <- connect2postgres2("ewedb")
  locations <- read_file(file.path(workingdir,tempTableName))
  locations <- locations@data
  tempTableName <- swish_temptable()
  names(locations) <- c("long", "lat")
  dbWriteTable(ch, tempTableName$table, locations, row.names = F)
  tested <- sql_subset(ch, tempTableName$fullname, eval = T)
  #head(tested)
  tempTableName <- tempTableName$fullname
  tempTableName
  
  # points2geom
  sch <- strsplit(tempTableName, "\\.")[[1]][1]
  tbl <- strsplit(tempTableName, "\\.")[[1]][2]
  sql <- points2geom(
    schema=sch,
    tablename=tbl,
    col_lat= "lat",col_long="long", srid="4283"
  )
   cat(sql)
  dbSendQuery(ch, sql)
  tbl
  
  ################################################################
  # name: R_raster_extract_by_day
  require(swishdbtools)
  require(awaptools)
  if(!require(reshape))  install.packages("reshape", repos="http://cran.csiro.au/"); require(reshape);
  tempTableName_locations <- tbl
  startdate <- StartDate
  enddate <- EndDate
  ch<-connect2postgres2("ewedb")
  tempTableName <- swish_temptable("ewedb")
  # address doesn't exist
  dbSendQuery(ch, sprintf("alter table %s.%s add column address serial",tempTableName$schema, tempTableName_locations))
  raster_extract_by_day(ch, startdate, enddate,
                        schemaName = tempTableName$schema,
                        tableName = tempTableName$table,
                        pointsLayer = tempTableName_locations,
                        measures = c("maxave", "minave") 
  )
  # test_out <- sql_subset(ch, tempTableName$fullname, eval = T)
  # test_out_loc <- sql_subset(ch, paste("gislibrary.", tempTableName_locations, sep =""), eval = T)
  # head(test_out)
  # head(test_out_loc)
  # test_out <- merge(test_out_loc, test_out)
  # head(test_out)
  # points(test_out$long, test_out$lat, col = test_out$value)
  
  
  # "minave", "totals", "vprph09", "vprph15")
  output_data <- reformat_awap_data(
    tableName = tempTableName$fullname
  )
  
  outputDataFile <- file.path(workingdir, outputDataFile)
  write.csv(output_data,outputDataFile, row.names = FALSE)
  outputFileName <- outputDataFile
  outputFileName
  
  
  ################################################################
  # name: intersect pixels with coast
  pgListTables(ch, "boundaries_electorates")
  sql_subset(ch, 
             "boundaries_electorates.electorates2009"
             , limit = 1, eval = T)
  loc_tab <- paste(sep = "",tempTableName$schema,".",tempTableName_locations)
  sql <- sql_subset(ch, 
             loc_tab
             , limit = 1, eval = F)
  cat(sql)
  dbSendQuery(ch,
             paste("
  select long, lat, t1.gid, address, elect_div 
  into tempfoobar
  from ", loc_tab, " t1,
  boundaries_electorates.electorates2009 t2
  where st_contains(t2.the_geom, t1.the_geom)
                   ", sep = "")
             )
  sql_subset(ch, paste(sep = ".", tempTableName$schema, "tempfoobar"), limit = 1, eval = T)
  dbSendQuery(ch,
              paste("
  select t1.*, case when t2.elect_div is not null then 1 else 0 end as landmask 
  into tempfoobar1
  from ", loc_tab, " t1
  left join
  john_snow.tempfoobar t2
  on t1.address = t2.address
  ", sep = "")
              )
  df <- sql_subset(ch, paste(sep = ".", tempTableName$schema, "tempfoobar1"), limit = -1, eval = T)
  with(df, plot(long, lat, col = landmask+1, pch = 16))
  head(df)
  write.csv(df, "landmask.csv", row.names = F)
  ################################################################
  # name: visualise a pixel
  head(output_data)
  which(test_out_loc$long == 150 & test_out_loc$lat == -30)
  subset(output_data, address == 65031)
  with(subset(output_data, address == 65031), plot(date, maxave, type= "l"))
  
  # TODO figure out how to turn these into a raster?
  head(test_out_loc)
  head(output_data)
  
  ################################################################
  # name: tidy up
  require(swishdbtools)
  ch<-connect2postgres2("ewedb")
  sch <- swish_temptable("ewedb")
  sch <- sch$schema
  tbls <- pgListTables(ch, sch, table="foo", match = FALSE)
  tbls
  for(tab in tbls[,1])
  {
    dbSendQuery(ch, 
                sprintf("drop table %s.%s", sch, tab)
    )
  }
    
  
#+end_src



** TODO zones
#+name:zones
#+begin_src R :session *R* :tangle src/zones.r :exports none :eval no
################################################################
# name:zones

#+end_src

* Tests
* DRO awap grids
** COMMENT 2013-10-26-extract-weather-data-from-awap-grids
#+name:extract-weather-data-from-awap-grids-header
#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-26-extract-weather-data-from-awap-grids.md :exports none :eval no :padline no
---
name: extract-weather-data-from-awap-grids
layout: post
title: Daintree Rainforest Observatory Climate Data from AWAP-GRIDS
date: 2013-10-26
categories:
- awap grids
- extreme weather events
---

#+end_src
** Introduction and Methods

This is a work in progress.  It is a stub of an article I want to put together which shows how to use several online data repositories together as a showcase of the [Scientific Workflow and Integration Software for Health (SWISH) Climate Impact Assessments](https://github.com/swish-climate-impact-assessment) project.

*** Authors
- Ivan Hanigan and [Markus Nolf](http://www.thinkoholic.com)

*** Background 

- Markus Nolf offers this use case of the [SWISH EWEDB](http://swish-climate-impact-assessment.github.io/)
- Markus is pulling together his  Daintree Rainforest Observatory (DRO) data into a manuscript for publication, and was looking for climate data from 2012 as well as long-term. 
- More specifically, the annual precipitation and mean annual temperature for both 2012 and the 30-year mean.
- The Australian Bureau of Meteorology has a nice rainfall dataset available at http://www.bom.gov.au/climate/data/ ("Cape Trib Store" weather station), but it seems like the temperature records are patchy.
- So it is advised to use the data the DRO collects its self
- You need to apply through the [ASN SuperSite data portal](http://www.tern-supersites.net.au/knb/) for access to the daily data for the DRO.
- Note the use of the DRO met data will need to be properly cited as it is harder to keep
an AWS station running in the tropics for years than it is to collect most other data. 
The citation information is provided when you make a request to access the data.
- The long term mean used by most DRO researchers is from the BOM station as we only have a short record from the station itself.  The offset is around 1000mm.
- However what we want is mean annual temperatures but the BOM website seems to focus more on mean minimum and maximum temperatures.

*** Material and Methods 

**** Baseline Climate Data 2012, Far North Queensland Rainforest Supersite, Cape Tribulation Node

- We can use the data portal too see [the data file in question](http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html)
- Application for access is via email

**** Extract mean annual temperatures at the BOM website

- SWISH uses BoM data a fair bit and aims to streamline access to BoM data for extreme weather event analysis (which require long term average climatology to provide the baseline that extremes are measured against).
- WRT to temperature most daily averages from BoM are calculated as average of maximum_temperature_in_24_hours_after_9am_local_time_in_degrees and minimum_temperature_in_24_hours_before_9am_local_time_in_degree (only couple of hundred AWS provide hourly data to get the proper mean of 24 obs).
- The Bureau of Meteorology has generated a range of gridded meteorological datasets for Australia as a contribution to the Australian Water Availability Project (AWAP). These include daily max and min temperature which you could use to generate daily averages, then calculate your long term averages from those?  
- http://www.bom.gov.au/jsp/awap/
- Documentation is at http://www.bom.gov.au/amm/docs/2009/jones.pdf

**** A workflow to download and process the public BoM weather grids.

- This workflow uses the open source R software with some of our custom written packages:

** R-depends
#+begin_src R :session *R* :tangle no :exports reports :eval no
  # depends
  install.packages(c('raster', 'rgdal', 'plyr', 'RODBC', 'RCurl', 'XML', 'ggmap', 'maptools', 'spdep'))
  
#+end_src

** R-code-for-extraction
#+name:r-code
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:r-code
  
  
  
  # aim daily weather for any point location from online BoM weather grids
  
  # depends on some github packages
  require(awaptools)
  #http://swish-climate-impact-assessment.github.io/tools/awaptools/awaptools-downloads.html
  require(swishdbtools)
  #http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html
  require(gisviz)
  # http://ivanhanigan.github.io/gisviz/
  
  # and this from CRAN
  if(!require(raster)) install.packages('raster'); require(raster)
  
  # get weather data, beware that each grid is a couple of megabytes
  vars <- c("maxave","minave","totals","vprph09","vprph15") #,"solarave") 
  # solar only available after 1990
  for(measure in vars)
  {
    #measure <- vars[1]
    get_awap_data(start = '1960-01-01',end = '1960-01-02', measure)
  }
  
  # get location
  locn <- geocode("daintree rainforest")
  # this uses google maps API, better check this
  # lon       lat
  # 1 145.4185 -16.17003
  ## Treat data frame as spatial points
  epsg <- make_EPSG()
  shp <- SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  # now loop over grids and extract met data
  cfiles <-  dir(pattern="grid$")
  
  for (i in seq_len(length(cfiles))) {
    #i <- 1 ## for stepping thru
    gridname <- cfiles[[i]]
    r <- raster(gridname)
    #image(r) # plot to look at
    e <- extract(r, shp, df=T)
    #str(e) ## print for debugging
    e1 <- shp
    e1@data$values <- e[,2]
    e1@data$gridname <- gridname
    # write to to target file
    write.table(e1@data,"output.csv",
      col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE)
  }
  
  # further work is required to format the column with the gridname to get out the date and weather paramaters.
#+end_src
** Results
**** Results 
  
- Markus reports:
- "The R-script worked great once i had set a working directory that did not include spaces. (It may have been a different problem that got solved by changing the wd, but the important thing is it's running now.)"
- Markus downloaded 70+ GB of gridded weather data from the BoM website to his local computer
- Also note there is another set of gridded data available from the BOM, which contains pre-computed longterm mean temps, [ready to be extracted with the script](http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&period=#maps)
- "Using this file, I only needed to get the 2012 temp grids for a comparison of 2012 vs. 30-year data. I'm going to run the extraction of 1961-1990 data, just to be sure."
- "When we finished analysis of the long-term temperature from daily means found:
- While the official, pre-computed long-term mean (i.e. 30-year grid file, analysed with your script) was 22.29 C for the DRO coordinates (145.4494, -16.1041), the new value from daily means (i.e. daily minave and maxave averaged) is 24.91 C.
- We're not sure what causes this discrepancy, but thought we'd note that there is one.
- For the manuscript, we ended up using the means obtained via BOM's method* to compare 1961-1990 values to 2012, both computed with the above script.
- (* average of daily min/max temperature for each year, then averaged across the entire 30 year period)

**** Dataset discrepancy

- Following up on the interesting a difference between the two BoM datasets. 
- One thing that might cause this might be if you are calculating the average of the annual averages ie sum(annavs)/30 or the average of all the daily averages as sum(dailyavs)/(30 * 365 or 366)?  the variance will differ by these methods.
- looks like the 30 year dataset is the former:
- "Average annual temperatures (maximum, minimum or mean) are calculated by adding daily temperature values each year, dividing by the number of days in that year to get an average for that particular year. The average values for each year in a specified period (1961 to 1990) are added together and the final value is calculated by dividing by the number of years in the period (30 years in this case)."
[metadata](http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&period=#maps)
- Markus followed the BOM calculation method, and just compared it with two other approaches.
- average of all 21914 values
- average of yearly sum(min and max values per year)/(ndays*2)
- average of yearly sum(daily average)/ndays)
- where ndays = number of days per year.
- Differences between these methods show only in the 6th decimal place, far from 2.62 degrees.


** R-code-for-comparison
#+name:r-code
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # This is Markus' comparison script 
  # also see the formatted table csv, as well as the SWISH script's raw output csv
  
  setwd("E:\\markus\\ph.d\\aus-daintree\\data-analysis\\climate")
  climate <- read.csv("minmaxave-30year-daily.csv", sep=",", dec=".")
  
  climate$year <- substr(climate$file,1,4)
  climate$dailymean <- (climate$maxave+climate$minave)/2
  head(climate)
  
  
  #total average across all days and values
  annmean <- mean(c(climate$maxave,climate$minave))
  annmean
  
  
  #daily means averaged by year, then total average
  annmean1 <- c(1,2)
  for(i in 1:30) {
          annmean1[i] <- mean(climate[climate$year==(i+1960),]$dailymean)
          #print(annmean1[i])
  }
  annmean1
  mean(annmean1)
  
  
  #mean of all values per year, then total average
  annmean2 <- c(1,2)
  for(i in 1:30) {
          tmpdata <- climate[climate$year==(i+1960),]
          annmean2[i] <- (sum(tmpdata$maxave) + sum(tmpdata$minave))/(length(tmpdata$maxave)+length(tmpdata$minave))
          #print(annmean2[i])
  }
  annmean2; mean(annmean2)
  
  
  #differences
  annmean - mean(annmean1)
  annmean - mean(annmean2)
  mean(annmean1) - mean(annmean2)
  
  
#+end_src

** Discussion 
  
- Principal findings: Very convenient automated extraction of location-based time series data for the precise period that is requested.

- Weaknesses (whole method, not your script): very long download time for daily grids (~11.000 grids = huge dataset, took several days in my case). Yearly grids would be beneficial (and I believe most others are also looking mainly for data on a yearly (or larger) scale).


**** Conclusion

- Take home message: Seems like a perfect case of "double-check the data using one and the same method".
  

** COMMENT md-formatted-to-html

#+begin_src markdown :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-10-26-extract-weather-data-from-awap-grids.md :exports none :eval no :padline no
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">AWAP GRIDS </h1>


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Introduction and Methods</a>
<ul>
<li><a href="#sec-1-1">1.1 Authors</a></li>
<li><a href="#sec-1-2">1.2 Background</a></li>
<li><a href="#sec-1-3">1.3 Material and Methods</a></li>
</ul>
</li>
<li><a href="#sec-2">2 R-code-for-extraction</a></li>
<li><a href="#sec-3">3 Results</a></li>
<li><a href="#sec-4">4 R-code-for-comparison</a></li>
<li><a href="#sec-5">5 Discussion</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> Introduction and Methods</h3>
<div class="outline-text-3" id="text-1">


<p>
This is a work in progress.  It is a stub of an article I want to put together which shows how to use several online data repositories together as a showcase of the [Scientific Workflow and Integration Software for Health (SWISH) Climate Impact Assessments](<a href="https://github.com/swish-climate-impact-assessment">https://github.com/swish-climate-impact-assessment</a>) project.
</p>

</div>

<div id="outline-container-1-1" class="outline-4">
<h4 id="sec-1-1"><span class="section-number-4">1.1</span> Authors</h4>
<div class="outline-text-4" id="text-1-1">

<ul>
<li>Ivan Hanigan and [Markus Nolf](<a href="http://www.thinkoholic.com">http://www.thinkoholic.com</a>)
</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-4">
<h4 id="sec-1-2"><span class="section-number-4">1.2</span> Background</h4>
<div class="outline-text-4" id="text-1-2">


<ul>
<li>Markus Nolf offers this use case of the [SWISH EWEDB](<a href="http://swish-climate-impact-assessment.github.io/">http://swish-climate-impact-assessment.github.io/</a>)
</li>
<li>Markus is pulling together his  Daintree Rainforest Observatory (DRO) data into a manuscript for publication, and was looking for climate data from 2012 as well as long-term. 
</li>
<li>More specifically, the annual precipitation and mean annual temperature for both 2012 and the 30-year mean.
</li>
<li>The Australian Bureau of Meteorology has a nice rainfall dataset available at <a href="http://www.bom.gov.au/climate/data/">http://www.bom.gov.au/climate/data/</a> ("Cape Trib Store" weather station), but it seems like the temperature records are patchy.
</li>
<li>So it is advised to use the data the DRO collects its self
</li>
<li>You need to apply through the [ASN SuperSite data portal](<a href="http://www.tern-supersites.net.au/knb/">http://www.tern-supersites.net.au/knb/</a>) for access to the daily data for the DRO.
</li>
<li>Note the use of the DRO met data will need to be properly cited as it is harder to keep
</li>
</ul>

<p>an AWS station running in the tropics for years than it is to collect most other data. 
The citation information is provided when you make a request to access the data.
</p><ul>
<li>The long term mean used by most DRO researchers is from the BOM station as we only have a short record from the station itself.  The offset is around 1000mm.
</li>
<li>However what we want is mean annual temperatures but the BOM website seems to focus more on mean minimum and maximum temperatures.
</li>
</ul>


</div>

</div>

<div id="outline-container-1-3" class="outline-4">
<h4 id="sec-1-3"><span class="section-number-4">1.3</span> Material and Methods</h4>
<div class="outline-text-4" id="text-1-3">


<ul>
<li id="sec-1-3-1">Baseline Climate Data 2012, Far North Queensland Rainforest Supersite, Cape Tribulation Node<br/>

<ul>
<li>We can use the data portal too see [the data file in question](<a href="http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html">http://www.tern-supersites.net.au/knb/metacat/lloyd.238.13/html</a>)
</li>
<li>Application for access is via email
</li>
</ul>


</li>
</ul>
<ul>
<li id="sec-1-3-2">Extract mean annual temperatures at the BOM website<br/>

<ul>
<li>SWISH uses BoM data a fair bit and aims to streamline access to BoM data for extreme weather event analysis (which require long term average climatology to provide the baseline that extremes are measured against).
</li>
<li>WRT to temperature most daily averages from BoM are calculated as average of maximum<sub>temperature</sub><sub>in</sub><sub>24</sub><sub>hours</sub><sub>after</sub><sub>9am</sub><sub>local</sub><sub>time</sub><sub>in</sub><sub>degrees</sub> and minimum<sub>temperature</sub><sub>in</sub><sub>24</sub><sub>hours</sub><sub>before</sub><sub>9am</sub><sub>local</sub><sub>time</sub><sub>in</sub><sub>degree</sub> (only couple of hundred AWS provide hourly data to get the proper mean of 24 obs).
</li>
<li>The Bureau of Meteorology has generated a range of gridded meteorological datasets for Australia as a contribution to the Australian Water Availability Project (AWAP). These include daily max and min temperature which you could use to generate daily averages, then calculate your long term averages from those?  
</li>
<li><a href="http://www.bom.gov.au/jsp/awap/">http://www.bom.gov.au/jsp/awap/</a>
</li>
<li>Documentation is at <a href="http://www.bom.gov.au/amm/docs/2009/jones.pdf">http://www.bom.gov.au/amm/docs/2009/jones.pdf</a>
</li>
</ul>


</li>
</ul>
<ul>
<li id="sec-1-3-3">A workflow to download and process the public BoM weather grids.<br/>
<div id="outline-container-1" class="outline-3">
<h3 id="sec-1"><span class="section-number-3">1</span> R-depends</h3>
<div class="outline-text-3" id="text-1">




<pre class="src src-R"><span style="color: #7f9f7f;"># </span><span style="color: #7f9f7f;">depends</span>
install.packages(c(<span style="color: #cc9393;">'raster'</span>, <span style="color: #cc9393;">'rgdal'</span>, <span style="color: #cc9393;">'plyr'</span>, <span style="color: #cc9393;">'RODBC'</span>, <span style="color: #cc9393;">'RCurl'</span>, <span style="color: #cc9393;">'XML'</span>, <span style="color: #cc9393;">'ggmap'</span>, <span style="color: #cc9393;">'maptools'</span>, <span style="color: #cc9393;">'spdep'</span>))

</pre>

</div>
</div>

<ul>
<li>This workflow uses the open source R software with some of our custom written packages:
</li>
</ul>



</li>
</ul>
</div>
</div>

</div>

<div id="outline-container-2" class="outline-3">
<h3 id="sec-2"><span class="section-number-3">2</span> R-code-for-extraction</h3>
<div class="outline-text-3" id="text-2">




<pre class="src src-R"><span style="color: #93a1a1;">################################################################</span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">name:r-code</span>



<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">aim daily weather for any point location from online BoM weather grids</span>

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">depends on some github packages</span>
<span style="color: #268bd2; font-weight: bold;">require</span>(awaptools)
<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">http://swish-climate-impact-assessment.github.io/tools/awaptools/awaptools-downloads.html</span>
<span style="color: #268bd2; font-weight: bold;">require</span>(swishdbtools)
<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html</span>
<span style="color: #268bd2; font-weight: bold;">require</span>(gisviz)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">http://ivanhanigan.github.io/gisviz/</span>

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">and this from CRAN</span>
<span style="color: #859900; font-weight: bold;">if</span>(!<span style="color: #268bd2; font-weight: bold;">require</span>(raster)) install.packages(<span style="color: #2aa198;">'raster'</span>); <span style="color: #268bd2; font-weight: bold;">require</span>(raster)

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">get weather data, beware that each grid is a couple of megabytes</span>
vars <span style="color: #268bd2; font-weight: bold;">&lt;-</span> c(<span style="color: #2aa198;">"maxave"</span>,<span style="color: #2aa198;">"minave"</span>,<span style="color: #2aa198;">"totals"</span>,<span style="color: #2aa198;">"vprph09"</span>,<span style="color: #2aa198;">"vprph15"</span>) <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">,"solarave") </span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">solar only available after 1990</span>
<span style="color: #859900; font-weight: bold;">for</span>(measure <span style="color: #859900; font-weight: bold;">in</span> vars)
{
  <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">measure &lt;- vars[1]</span>
  get_awap_data(start = <span style="color: #2aa198;">'1960-01-01'</span>,end = <span style="color: #2aa198;">'1960-01-02'</span>, measure)
}

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">get location</span>
locn <span style="color: #268bd2; font-weight: bold;">&lt;-</span> geocode(<span style="color: #2aa198;">"daintree rainforest"</span>)
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">this uses google maps API, better check this</span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">lon       lat</span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">1 145.4185 -16.17003</span>
<span style="color: #93a1a1;">## </span><span style="color: #93a1a1;">Treat data frame as spatial points</span>
epsg <span style="color: #268bd2; font-weight: bold;">&lt;-</span> make_EPSG()
shp <span style="color: #268bd2; font-weight: bold;">&lt;-</span> SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
                              proj4string=CRS(epsg$prj4[epsg$code %<span style="color: #859900; font-weight: bold;">in</span>% <span style="color: #2aa198;">'4283'</span>]))
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">now loop over grids and extract met data</span>
cfiles <span style="color: #268bd2; font-weight: bold;">&lt;-</span>  dir(pattern=<span style="color: #2aa198;">"grid$"</span>)

<span style="color: #859900; font-weight: bold;">for</span> (i <span style="color: #859900; font-weight: bold;">in</span> seq_len(length(cfiles))) {
  <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">i &lt;- 1 ## for stepping thru</span>
  gridname <span style="color: #268bd2; font-weight: bold;">&lt;-</span> cfiles[[i]]
  r <span style="color: #268bd2; font-weight: bold;">&lt;-</span> raster(gridname)
  <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">image(r) # plot to look at</span>
  e <span style="color: #268bd2; font-weight: bold;">&lt;-</span> extract(r, shp, df=T)
  <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">str(e) ## print for debugging</span>
  e1 <span style="color: #268bd2; font-weight: bold;">&lt;-</span> shp
  e1@data$values <span style="color: #268bd2; font-weight: bold;">&lt;-</span> e[,2]
  e1@data$gridname <span style="color: #268bd2; font-weight: bold;">&lt;-</span> gridname
  <span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">write to to target file</span>
  write.table(e1@data,<span style="color: #2aa198;">"output.csv"</span>,
    col.names = i == 1, append = i&gt;1 , sep = <span style="color: #2aa198;">","</span>, row.names = <span style="color: #b58900;">FALSE</span>)
}

<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">further work is required to format the column with the gridname to get out the date and weather paramaters.</span>
</pre>

</div>

</div>

<div id="outline-container-3" class="outline-3">
<h3 id="sec-3"><span class="section-number-3">3</span> Results</h3>
<div class="outline-text-3" id="text-3">

<ul>
<li id="sec-3-1">Results<br/>

<ul>
<li>Markus reports:
</li>
<li>"The R-script worked great once i had set a working directory that did not include spaces. (It may have been a different problem that got solved by changing the wd, but the important thing is it's running now.)"
</li>
<li>Markus downloaded 70+ GB of gridded weather data from the BoM website to his local computer
</li>
<li>Also note there is another set of gridded data available from the BOM, which contains pre-computed longterm mean temps, [ready to be extracted with the script](<a href="http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;period=#maps">http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;period=#maps</a>)
</li>
<li>"Using this file, I only needed to get the 2012 temp grids for a comparison of 2012 vs. 30-year data. I'm going to run the extraction of 1961-1990 data, just to be sure."
</li>
<li>"When we finished analysis of the long-term temperature from daily means found:
</li>
<li>While the official, pre-computed long-term mean (i.e. 30-year grid file, analysed with your script) was 22.29 C for the DRO coordinates (145.4494, -16.1041), the new value from daily means (i.e. daily minave and maxave averaged) is 24.91 C.
</li>
<li>We're not sure what causes this discrepancy, but thought we'd note that there is one.
</li>
<li>For the manuscript, we ended up using the means obtained via BOM's method* to compare 1961-1990 values to 2012, both computed with the above script.
</li>
<li>(* average of daily min/max temperature for each year, then averaged across the entire 30 year period)
</li>
</ul>


</li>
</ul>
<ul>
<li id="sec-3-2">Dataset discrepancy<br/>

<ul>
<li>Following up on the interesting a difference between the two BoM datasets. 
</li>
<li>One thing that might cause this might be if you are calculating the average of the annual averages ie sum(annavs)/30 or the average of all the daily averages as sum(dailyavs)/(30 * 365 or 366)?  the variance will differ by these methods.
</li>
<li>looks like the 30 year dataset is the former:
</li>
<li>"Average annual temperatures (maximum, minimum or mean) are calculated by adding daily temperature values each year, dividing by the number of days in that year to get an average for that particular year. The average values for each year in a specified period (1961 to 1990) are added together and the final value is calculated by dividing by the number of years in the period (30 years in this case)."
</li>
</ul>

<p>[metadata](<a href="http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;period=#maps">http://reg.bom.gov.au/jsp/ncc/climate_averages/temperature/index.jsp?maptype=6&amp;period=#maps</a>)
</p><ul>
<li>Markus followed the BOM calculation method, and just compared it with two other approaches.
</li>
<li>average of all 21914 values
</li>
<li>average of yearly sum(min and max values per year)/(ndays*2)
</li>
<li>average of yearly sum(daily average)/ndays)
</li>
<li>where ndays = number of days per year.
</li>
<li>Differences between these methods show only in the 6th decimal place, far from 2.62 degrees.
</li>
</ul>



</li>
</ul>
</div>

</div>

<div id="outline-container-4" class="outline-3">
<h3 id="sec-4"><span class="section-number-3">4</span> R-code-for-comparison</h3>
<div class="outline-text-3" id="text-4">




<pre class="src src-R"><span style="color: #93a1a1;">################################################################</span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">This is Markus' comparison script </span>
<span style="color: #93a1a1;"># </span><span style="color: #93a1a1;">also see the formatted table csv, as well as the SWISH script's raw output csv</span>

setwd(<span style="color: #2aa198;">"E:\\markus\\ph.d\\aus-daintree\\data-analysis\\climate"</span>)
climate <span style="color: #268bd2; font-weight: bold;">&lt;-</span> read.csv(<span style="color: #2aa198;">"minmaxave-30year-daily.csv"</span>, sep=<span style="color: #2aa198;">","</span>, dec=<span style="color: #2aa198;">"."</span>)

climate$year <span style="color: #268bd2; font-weight: bold;">&lt;-</span> substr(climate$file,1,4)
climate$dailymean <span style="color: #268bd2; font-weight: bold;">&lt;-</span> (climate$maxave+climate$minave)/2
head(climate)


<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">total average across all days and values</span>
annmean <span style="color: #268bd2; font-weight: bold;">&lt;-</span> mean(c(climate$maxave,climate$minave))
annmean


<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">daily means averaged by year, then total average</span>
annmean1 <span style="color: #268bd2; font-weight: bold;">&lt;-</span> c(1,2)
<span style="color: #859900; font-weight: bold;">for</span>(i <span style="color: #859900; font-weight: bold;">in</span> 1:30) {
        annmean1[i] <span style="color: #268bd2; font-weight: bold;">&lt;-</span> mean(climate[climate$year==(i+1960),]$dailymean)
        <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">print(annmean1[i])</span>
}
annmean1
mean(annmean1)


<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">mean of all values per year, then total average</span>
annmean2 <span style="color: #268bd2; font-weight: bold;">&lt;-</span> c(1,2)
<span style="color: #859900; font-weight: bold;">for</span>(i <span style="color: #859900; font-weight: bold;">in</span> 1:30) {
        tmpdata <span style="color: #268bd2; font-weight: bold;">&lt;-</span> climate[climate$year==(i+1960),]
        annmean2[i] <span style="color: #268bd2; font-weight: bold;">&lt;-</span> (sum(tmpdata$maxave) + sum(tmpdata$minave))/(length(tmpdata$maxave)+length(tmpdata$minave))
        <span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">print(annmean2[i])</span>
}
annmean2; mean(annmean2)


<span style="color: #93a1a1;">#</span><span style="color: #93a1a1;">differences</span>
annmean - mean(annmean1)
annmean - mean(annmean2)
mean(annmean1) - mean(annmean2)


</pre>


</div>

</div>

<div id="outline-container-5" class="outline-3">
<h3 id="sec-5"><span class="section-number-3">5</span> Discussion</h3>
<div class="outline-text-3" id="text-5">


<ul>
<li>Principal findings: Very convenient automated extraction of location-based time series data for the precise period that is requested.
</li>
<li>Weaknesses (whole method, not your script): very long download time for daily grids (~11.000 grids = huge dataset, took several days in my case). Yearly grids would be beneficial (and I believe most others are also looking mainly for data on a yearly (or larger) scale).
</li>
</ul>


<ul>
<li id="sec-5-1">Conclusion<br/>

<ul>
<li>Take home message: Seems like a perfect case of "double-check the data using one and the same method".
</li>
</ul>


</li>
</ul>
</div>
</div>
</div>

</body>
</html>

#+end_src
* Seasonal Rainfall
** 2014-02-28-long-term-climatology-contextual-data-for-ecological-research
*** COMMENT dev-code
#+name:dev
#+begin_src R :session *R* :tangle src/dev.r :exports none :eval no
  #### name:dev####
        setwd("inst/extdata")
        # download a couple hundred megs
        load_monthly(start_date="1970-01-01", end_date="2014-01-31")
        # expand to over a gig
        cfiles <- dir()
        for(f in cfiles){
          unzip_monthly(f)
        }
        # now we can try to select the seasonal bits
    # this comes from Lu Porfirio
    grids  <- dir()
    # grids[1:12]
    summer.mth <- c('11','12','01','02','03')
    mths <- substr(grids,20,21) %in% summer.mth
    test <- ifelse(mths == "TRUE", grids, NA)
    grids.brick <- brick(stack(grids))
    awap.stack<-stackApply(grids.brick, indices = match(test,grids), fun = "mean")
    # the output has too many layers...
    ## this is because we are not considering that summers go across 2-yr
  
#+end_src
*** prod
#+name:long-term-climatology-contextual-data-for-ecological-research-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2014-02-28-long-term-climatology-contextual-data-for-ecological-research.md :exports none :eval no :padline no
  ---
  name: 2014-02-28-long-term-climatology-contextual-data-for-ecological-research
  layout: post
  title: long-term-climatology-contextual-data-for-ecological-research
  date: 2014-02-28
  categories:
  - extreme weather events
  - Drought
  ---
  
  - Studies of extreme weather events such as drought require long term climate data 
  - these are available at continental scale derived from
  - observations from a network of weather stations that are interpolated to a surface
  - I have been working on techniques with R and online resources (the Australian Water Availabilty Project AWAP) to make working with these long term climatology datasets easier.
  - The package is in development at [https://github.com/swish-climate-impact-assessment/awaptools](https://github.com/swish-climate-impact-assessment/awaptools)
  
  
  ## Case Study
     
  - aim is need to look at seasonal rainfall means.
  - first thing is to download the data (I'm also working on a Rstudio server to host these data, as a Virtual Lab).
  - data = multiple years of monthly rainfall data in a raster grid format. 
  - aim = combine rainfall in a seasonal basis in one grid
  - (i.e. M-J-J-A-S-O 1900, 1901 etc.) calculate mean of each cell. 
  - assumption1 = filenames have year, month embedded so they will be sorted in order when listed 
  - assumption2 = all months are available, from 1:12 for all years in
  - study period
  - notes: 
  - this requires the files are listed in the right order by name, and all months are present. might be better to use grep on the file name and strsplit/substr to extract the month identifier more precisely? 
  
  
  ### Results
  
  ![alttext](/images/season_hot.png)
    
  ### I'm looking for collaboration on this!
  
  - I've been in contact with:
  - [https://github.com/RationShop/rain_r/wiki/The-Rain-Project](https://github.com/RationShop/rain_r/wiki/The-Rain-Project)
  - I apologise to windoze users but feel a bit like [this guy](http://librestats.com/2014/02/24/how-to-make-a-bad-password-with-r/) when recomending how to install:
  
  #### quote:
      probably the easiest way to do this is to use
      Hadley's devtools package.  Assuming you have devtools and my package's
      dependencies.  If you're using Linux or the BSD's, this should just
      work.  Welcome to the good life, player.  I think this will work out
      of the box on a Mac.  I have no idea if this will work on Windows; how
      you strange people get anything done amazes me.  At the least,
      Im guessing you have to install Rtools first.  
      You could also just source all the R scripts like 
      some kind of barbarian
  
  <p></p>
  
  #### R Code:
      # depends
      require(swishdbtools)
      if(!require(raster)) install.packages("raster", dependencies = T); require(raster)
      if(!require(rgdal)) install.packages("rgdal", dependencies = T); require(rgdal)
  
      # on linux can install direct, on windoze you configure Rtools
      require(devtools)
      install_github("awaptools", "swish-climate-impact-assessment")
      require(awaptools)
  
      homedir <- "~/data/AWAP_GRIDS/data"
      outdir <- "~/data/AWAP_GRIDS/data-seasonal-vignette"
       
      # first make sure there are no left over files from previous runs
      #oldfiles <- list.files(pattern = '.tif', full.names=T) 
      #for(oldfile in oldfiles)
      #{
      #  print(oldfile)
      #  file.remove(oldfile)
      #}
      ################################################
      setwd(homedir)
       
      # local customisations
      workdir  <- homedir
      setwd(workdir)
      # don't change this
      years <- c(1900:2014)
      lengthYears <- length(years)
      # change this
      startdate <- "2013-01-01"
      enddate <- "2014-01-31"
      # do
      load_monthly(start_date = startdate, end_date = enddate)
       
      # do
      filelist <- dir(pattern = "grid.Z$")
      for(fname in filelist)
      {
        #fname <- filelist[1]
        unzip_monthly(fname, aggregation_factor = 1)
        fin <- gsub(".grid.Z", ".grid", fname)
        fout <- gsub(".grid.Z", ".tif", fname)
        r <- raster(fin)
        writeRaster(r, fout, format="GTiff",  overwrite = TRUE)
        file.remove(fin)
      }
       
      cfiles <- list.files(pattern = '.tif', full.names=T) 
      # loop thru
      # NEED TO SET THE FILESOFSEASEON_I counter EACH TIME YOU start
       
       
      for(season in c("hot", "cool"))
      {
        # season <- "hot" # for labelling
        if(season == "cool")
        {
          filesOfSeason_i <- c(5,6,7,8,9,10)  
          endat <- lengthYears
        } else {
          filesOfSeason_i <- c(11,12,13,14,15,16) 
          endat <- lengthYears - 1
        }
        
        for (year in 1:endat){ 
          ## setup for checking month 
          # year  <- 1 #endat
          
          
          ## checking
          print(cat("####################\n\n"))
          print(cfiles[filesOfSeason_i])
          
          b <- brick(stack(cfiles[filesOfSeason_i])) 
          ## calculate mean 
          m <- mean(b) 
          ## checking 
          # image(m) 
          writeRaster(m, file.path(outdir,sprintf("season_%s_%s.tif", season, year)), drivername="GTiff")
          filesOfSeason_i <- filesOfSeason_i + 12
        } 
      }
       
      ##### now we will overall average
      setwd(outdir)
      for(season in c("cool", "hot"))
      {
        cfiles <- list.files(pattern = season, full.names=T)   
        print(cfiles)
        b <- brick(stack(cfiles)) 
        ## calculate mean 
        m <- mean(b) 
        ## checking 
        # image(m) 
        writeRaster(m, file.path(outdir,sprintf("season_%s.tif", season)), drivername="GTiff")
      }
       
      # qc
      cool <- raster("season_cool.tif")
      hot <- raster("season_hot.tif")
      par(mfrow = c(2,1))
      image(cool)
      image(hot)
       
      # just summer rainfall
      png("season_hot.png")
      image(hot)
      dev.off()
   
#+end_src

* monthly
*** COMMENT AWAP_GRIDS-monthly.r
#+name:AWAP_GRIDS-monthly.r
#+begin_src R :session *R* :tangle AWAP_GRIDS-monthly.r :exports none :eval no
require(devtools)
install_github("swish-climate-impact-assessment/awaptools")
require(awaptools)
#install_github("swishdbtools", "swish-climate-impact-assessment")
require(swishdbtools)
# local customisations
workdir <- "data"
setwd(workdir)
dir()
startdate <- "2014-01-01"
enddate <- "2015-10-30"
# do
load_monthly(start_date = startdate, end_date = enddate)

# do
filelist <- dir(pattern = "grid.Z$")
for(fname in filelist)
{
  #fname = filelist[1]
  #fname
  unzip_monthly(fname, aggregation_factor = 1)
  fname <- dir(pattern = "grid$")
  r <- readGDAL(fname)
  outfile <- gsub('.grid', '.tif', fname)
  writeGDAL(r, outfile, drivername="GTiff")
  file.remove(fname)
}
setwd("..")

#+end_src

* Conclusions
* versions
* COMMENT DEPRECATED SEE HUTCH PROJ GH PAGES  Monthly Rainfall
*** COMMENT test-monthly-code
  #### name:test-monthly####
  
  R version 3.1.3 (2015-03-09) -- "Smooth Sidewalk"
  Copyright (C) 2015 The R Foundation for Statistical Computing
  Platform: x86_64-pc-linux-gnu (64-bit)
  
  R is free software and comes with ABSOLUTELY NO WARRANTY.
  You are welcome to redistribute it under certain conditions.
  Type 'license()' or 'licence()' for distribution details.
  
    Natural language support but running in an English locale
  
  R is a collaborative project with many contributors.
  Type 'contributors()' for more information and
  'citation()' on how to cite R or R packages in publications.
  
  Type 'demo()' for some demos, 'help()' for on-line help, or
  'help.start()' for an HTML browser interface to help.
  Type 'q()' to quit R.
  
  > options(STERM='iESS', str.dendrogram.last="'", editor='emacsclient', show.error.locations=TRUE)
  > > 
  Error: unexpected '>' in ">"
  > require(awaptools)
  Loading required package: awaptools
  Loading required package: raster
  Loading required package: sp
  > require(swishdbtools)
  Loading required package: swishdbtools
  Loading required package: foreign
  Loading required package: rgdal
  rgdal: version: 0.9-1, (SVN revision 518)
  Geospatial Data Abstraction Library extensions to R successfully loaded
  Loaded GDAL runtime: GDAL 1.10.0, released 2013/04/24
  Path to GDAL shared files: /usr/share/gdal
  Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
  Path to PROJ.4 shared files: (autodetected)
  Loading required package: plyr
  Loading required package: RODBC
  > require(gisviz)
  Loading required package: gisviz
  Loading required package: RCurl
  Loading required package: bitops
  Loading required package: XML
  Loading required package: ggmap
  Loading required package: ggplot2
  Loading required package: maps
  Loading required package: maptools
  Checking rgeos availability: TRUE
  Loading required package: RColorBrewer
  Loading required package: spdep
  Loading required package: Matrix
  > if(!require(raster)) install.packages('raster'); require(raster)
  > locn <- geocode("daintree rainforest")
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=daintree+rainforest&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  > epsg <- make_EPSG()
  shp <- SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  > 
  + > dir()
   [1] "1.R"                             "AWAP_GRIDS-monthly-summary.r"   
   [3] "extract-point-from-awap-grids.R" "test-monthly.R"                 
   [5] "test-pgListTables.r"             "test-raster-outdb.R"            
   [7] "test-raster-works.R"             "test-readGDAL2.r"               
   [9] "test-sqlquery_postgres2.r"       "test-sqlquery.r"                
  > setwd("../data-seasonal-vignette/")
  > cfiles <-  dir(pattern="grid$")
  > cfiles
    [1] "totals_1970010119700131.grid" "totals_1970020119700228.grid"
    [3] "totals_1970030119700331.grid" "totals_1970040119700430.grid"
    [5] "totals_1970050119700531.grid" "totals_1970060119700630.grid"
    [7] "totals_1970070119700731.grid" "totals_1970080119700831.grid"
    [9] "totals_1970090119700930.grid" "totals_1970100119701031.grid"
   [11] "totals_1970110119701130.grid" "totals_1970120119701231.grid"
   [13] "totals_1971010119710131.grid" "totals_1971020119710228.grid"
   [15] "totals_1971030119710331.grid" "totals_1971040119710430.grid"
   [17] "totals_1971050119710531.grid" "totals_1971060119710630.grid"
   [19] "totals_1971070119710731.grid" "totals_1971080119710831.grid"
   [21] "totals_1971090119710930.grid" "totals_1971100119711031.grid"
   [23] "totals_1971110119711130.grid" "totals_1971120119711231.grid"
   [25] "totals_1972010119720131.grid" "totals_1972020119720229.grid"
   [27] "totals_1972030119720331.grid" "totals_1972040119720430.grid"
   [29] "totals_1972050119720531.grid" "totals_1972060119720630.grid"
   [31] "totals_1972070119720731.grid" "totals_1972080119720831.grid"
   [33] "totals_1972090119720930.grid" "totals_1972100119721031.grid"
   [35] "totals_1972110119721130.grid" "totals_1972120119721231.grid"
   [37] "totals_1973010119730131.grid" "totals_1973020119730228.grid"
   [39] "totals_1973030119730331.grid" "totals_1973040119730430.grid"
   [41] "totals_1973050119730531.grid" "totals_1973060119730630.grid"
   [43] "totals_1973070119730731.grid" "totals_1973080119730831.grid"
   [45] "totals_1973090119730930.grid" "totals_1973100119731031.grid"
   [47] "totals_1973110119731130.grid" "totals_1973120119731231.grid"
   [49] "totals_1974010119740131.grid" "totals_1974020119740228.grid"
   [51] "totals_1974030119740331.grid" "totals_1974040119740430.grid"
   [53] "totals_1974050119740531.grid" "totals_1974060119740630.grid"
   [55] "totals_1974070119740731.grid" "totals_1974080119740831.grid"
   [57] "totals_1974090119740930.grid" "totals_1974100119741031.grid"
   [59] "totals_1974110119741130.grid" "totals_1974120119741231.grid"
   [61] "totals_1975010119750131.grid" "totals_1975020119750228.grid"
   [63] "totals_1975030119750331.grid" "totals_1975040119750430.grid"
   [65] "totals_1975050119750531.grid" "totals_1975060119750630.grid"
   [67] "totals_1975070119750731.grid" "totals_1975080119750831.grid"
   [69] "totals_1975090119750930.grid" "totals_1975100119751031.grid"
   [71] "totals_1975110119751130.grid" "totals_1975120119751231.grid"
   [73] "totals_1976010119760131.grid" "totals_1976020119760229.grid"
   [75] "totals_1976030119760331.grid" "totals_1976040119760430.grid"
   [77] "totals_1976050119760531.grid" "totals_1976060119760630.grid"
   [79] "totals_1976070119760731.grid" "totals_1976080119760831.grid"
   [81] "totals_1976090119760930.grid" "totals_1976100119761031.grid"
   [83] "totals_1976110119761130.grid" "totals_1976120119761231.grid"
   [85] "totals_1977010119770131.grid" "totals_1977020119770228.grid"
   [87] "totals_1977030119770331.grid" "totals_1977040119770430.grid"
   [89] "totals_1977050119770531.grid" "totals_1977060119770630.grid"
   [91] "totals_1977070119770731.grid" "totals_1977080119770831.grid"
   [93] "totals_1977090119770930.grid" "totals_1977100119771031.grid"
   [95] "totals_1977110119771130.grid" "totals_1977120119771231.grid"
   [97] "totals_1978010119780131.grid" "totals_1978020119780228.grid"
   [99] "totals_1978030119780331.grid" "totals_1978040119780430.grid"
  [101] "totals_1978050119780531.grid" "totals_1978060119780630.grid"
  [103] "totals_1978070119780731.grid" "totals_1978080119780831.grid"
  [105] "totals_1978090119780930.grid" "totals_1978100119781031.grid"
  [107] "totals_1978110119781130.grid" "totals_1978120119781231.grid"
  [109] "totals_1979010119790131.grid" "totals_1979020119790228.grid"
  [111] "totals_1979030119790331.grid" "totals_1979040119790430.grid"
  [113] "totals_1979050119790531.grid" "totals_1979060119790630.grid"
  [115] "totals_1979070119790731.grid" "totals_1979080119790831.grid"
  [117] "totals_1979090119790930.grid" "totals_1979100119791031.grid"
  [119] "totals_1979110119791130.grid" "totals_1979120119791231.grid"
  [121] "totals_1980010119800131.grid" "totals_1980020119800229.grid"
  [123] "totals_1980030119800331.grid" "totals_1980040119800430.grid"
  [125] "totals_1980050119800531.grid" "totals_1980060119800630.grid"
  [127] "totals_1980070119800731.grid" "totals_1980080119800831.grid"
  [129] "totals_1980090119800930.grid" "totals_1980100119801031.grid"
  [131] "totals_1980110119801130.grid" "totals_1980120119801231.grid"
  [133] "totals_1981010119810131.grid" "totals_1981020119810228.grid"
  [135] "totals_1981030119810331.grid" "totals_1981040119810430.grid"
  [137] "totals_1981050119810531.grid" "totals_1981060119810630.grid"
  [139] "totals_1981070119810731.grid" "totals_1981080119810831.grid"
  [141] "totals_1981090119810930.grid" "totals_1981100119811031.grid"
  [143] "totals_1981110119811130.grid" "totals_1981120119811231.grid"
  [145] "totals_1982010119820131.grid" "totals_1982020119820228.grid"
  [147] "totals_1982030119820331.grid" "totals_1982040119820430.grid"
  [149] "totals_1982050119820531.grid" "totals_1982060119820630.grid"
  [151] "totals_1982070119820731.grid" "totals_1982080119820831.grid"
  [153] "totals_1982090119820930.grid" "totals_1982100119821031.grid"
  [155] "totals_1982110119821130.grid" "totals_1982120119821231.grid"
  [157] "totals_1983010119830131.grid" "totals_1983020119830228.grid"
  [159] "totals_1983030119830331.grid" "totals_1983040119830430.grid"
  [161] "totals_1983050119830531.grid" "totals_1983060119830630.grid"
  [163] "totals_1983070119830731.grid" "totals_1983080119830831.grid"
  [165] "totals_1983090119830930.grid" "totals_1983100119831031.grid"
  [167] "totals_1983110119831130.grid" "totals_1983120119831231.grid"
  [169] "totals_1984010119840131.grid" "totals_1984020119840229.grid"
  [171] "totals_1984030119840331.grid" "totals_1984040119840430.grid"
  [173] "totals_1984050119840531.grid" "totals_1984060119840630.grid"
  [175] "totals_1984070119840731.grid" "totals_1984080119840831.grid"
  [177] "totals_1984090119840930.grid" "totals_1984100119841031.grid"
  [179] "totals_1984110119841130.grid" "totals_1984120119841231.grid"
  [181] "totals_1985010119850131.grid" "totals_1985020119850228.grid"
  [183] "totals_1985030119850331.grid" "totals_1985040119850430.grid"
  [185] "totals_1985050119850531.grid" "totals_1985060119850630.grid"
  [187] "totals_1985070119850731.grid" "totals_1985080119850831.grid"
  [189] "totals_1985090119850930.grid" "totals_1985100119851031.grid"
  [191] "totals_1985110119851130.grid" "totals_1985120119851231.grid"
  [193] "totals_1986010119860131.grid" "totals_1986020119860228.grid"
  [195] "totals_1986030119860331.grid" "totals_1986040119860430.grid"
  [197] "totals_1986050119860531.grid" "totals_1986060119860630.grid"
  [199] "totals_1986070119860731.grid" "totals_1986080119860831.grid"
  [201] "totals_1986090119860930.grid" "totals_1986100119861031.grid"
  [203] "totals_1986110119861130.grid" "totals_1986120119861231.grid"
  [205] "totals_1987010119870131.grid" "totals_1987020119870228.grid"
  [207] "totals_1987030119870331.grid" "totals_1987040119870430.grid"
  [209] "totals_1987050119870531.grid" "totals_1987060119870630.grid"
  [211] "totals_1987070119870731.grid" "totals_1987080119870831.grid"
  [213] "totals_1987090119870930.grid" "totals_1987100119871031.grid"
  [215] "totals_1987110119871130.grid" "totals_1987120119871231.grid"
  [217] "totals_1988010119880131.grid" "totals_1988020119880229.grid"
  [219] "totals_1988030119880331.grid" "totals_1988040119880430.grid"
  [221] "totals_1988050119880531.grid" "totals_1988060119880630.grid"
  [223] "totals_1988070119880731.grid" "totals_1988080119880831.grid"
  [225] "totals_1988090119880930.grid" "totals_1988100119881031.grid"
  [227] "totals_1988110119881130.grid" "totals_1988120119881231.grid"
  [229] "totals_1989010119890131.grid" "totals_1989020119890228.grid"
  [231] "totals_1989030119890331.grid" "totals_1989040119890430.grid"
  [233] "totals_1989050119890531.grid" "totals_1989060119890630.grid"
  [235] "totals_1989070119890731.grid" "totals_1989080119890831.grid"
  [237] "totals_1989090119890930.grid" "totals_1989100119891031.grid"
  [239] "totals_1989110119891130.grid" "totals_1989120119891231.grid"
  [241] "totals_1990010119900131.grid" "totals_1990020119900228.grid"
  [243] "totals_1990030119900331.grid" "totals_1990040119900430.grid"
  [245] "totals_1990050119900531.grid" "totals_1990060119900630.grid"
  [247] "totals_1990070119900731.grid" "totals_1990080119900831.grid"
  [249] "totals_1990090119900930.grid" "totals_1990100119901031.grid"
  [251] "totals_1990110119901130.grid" "totals_1990120119901231.grid"
  [253] "totals_1991010119910131.grid" "totals_1991020119910228.grid"
  [255] "totals_1991030119910331.grid" "totals_1991040119910430.grid"
  [257] "totals_1991050119910531.grid" "totals_1991060119910630.grid"
  [259] "totals_1991070119910731.grid" "totals_1991080119910831.grid"
  [261] "totals_1991090119910930.grid" "totals_1991100119911031.grid"
  [263] "totals_1991110119911130.grid" "totals_1991120119911231.grid"
  [265] "totals_1992010119920131.grid" "totals_1992020119920229.grid"
  [267] "totals_1992030119920331.grid" "totals_1992040119920430.grid"
  [269] "totals_1992050119920531.grid" "totals_1992060119920630.grid"
  [271] "totals_1992070119920731.grid" "totals_1992080119920831.grid"
  [273] "totals_1992090119920930.grid" "totals_1992100119921031.grid"
  [275] "totals_1992110119921130.grid" "totals_1992120119921231.grid"
  [277] "totals_1993010119930131.grid" "totals_1993020119930228.grid"
  [279] "totals_1993030119930331.grid" "totals_1993040119930430.grid"
  [281] "totals_1993050119930531.grid" "totals_1993060119930630.grid"
  [283] "totals_1993070119930731.grid" "totals_1993080119930831.grid"
  [285] "totals_1993090119930930.grid" "totals_1993100119931031.grid"
  [287] "totals_1993110119931130.grid" "totals_1993120119931231.grid"
  [289] "totals_1994010119940131.grid" "totals_1994020119940228.grid"
  [291] "totals_1994030119940331.grid" "totals_1994040119940430.grid"
  [293] "totals_1994050119940531.grid" "totals_1994060119940630.grid"
  [295] "totals_1994070119940731.grid" "totals_1994080119940831.grid"
  [297] "totals_1994090119940930.grid" "totals_1994100119941031.grid"
  [299] "totals_1994110119941130.grid" "totals_1994120119941231.grid"
  [301] "totals_1995010119950131.grid" "totals_1995020119950228.grid"
  [303] "totals_1995030119950331.grid" "totals_1995040119950430.grid"
  [305] "totals_1995050119950531.grid" "totals_1995060119950630.grid"
  [307] "totals_1995070119950731.grid" "totals_1995080119950831.grid"
  [309] "totals_1995090119950930.grid" "totals_1995100119951031.grid"
  [311] "totals_1995110119951130.grid" "totals_1995120119951231.grid"
  [313] "totals_1996010119960131.grid" "totals_1996020119960229.grid"
  [315] "totals_1996030119960331.grid" "totals_1996040119960430.grid"
  [317] "totals_1996050119960531.grid" "totals_1996060119960630.grid"
  [319] "totals_1996070119960731.grid" "totals_1996080119960831.grid"
  [321] "totals_1996090119960930.grid" "totals_1996100119961031.grid"
  [323] "totals_1996110119961130.grid" "totals_1996120119961231.grid"
  [325] "totals_1997010119970131.grid" "totals_1997020119970228.grid"
  [327] "totals_1997030119970331.grid" "totals_1997040119970430.grid"
  [329] "totals_1997050119970531.grid" "totals_1997060119970630.grid"
  [331] "totals_1997070119970731.grid" "totals_1997080119970831.grid"
  [333] "totals_1997090119970930.grid" "totals_1997100119971031.grid"
  [335] "totals_1997110119971130.grid" "totals_1997120119971231.grid"
  [337] "totals_1998010119980131.grid" "totals_1998020119980228.grid"
  [339] "totals_1998030119980331.grid" "totals_1998040119980430.grid"
  [341] "totals_1998050119980531.grid" "totals_1998060119980630.grid"
  [343] "totals_1998070119980731.grid" "totals_1998080119980831.grid"
  [345] "totals_1998090119980930.grid" "totals_1998100119981031.grid"
  [347] "totals_1998110119981130.grid" "totals_1998120119981231.grid"
  [349] "totals_1999010119990131.grid" "totals_1999020119990228.grid"
  [351] "totals_1999030119990331.grid" "totals_1999040119990430.grid"
  [353] "totals_1999050119990531.grid" "totals_1999060119990630.grid"
  [355] "totals_1999070119990731.grid" "totals_1999080119990831.grid"
  [357] "totals_1999090119990930.grid" "totals_1999100119991031.grid"
  [359] "totals_1999110119991130.grid" "totals_1999120119991231.grid"
  [361] "totals_2000010120000131.grid" "totals_2000020120000229.grid"
  [363] "totals_2000030120000331.grid" "totals_2000040120000430.grid"
  [365] "totals_2000050120000531.grid" "totals_2000060120000630.grid"
  [367] "totals_2000070120000731.grid" "totals_2000080120000831.grid"
  [369] "totals_2000090120000930.grid" "totals_2000100120001031.grid"
  [371] "totals_2000110120001130.grid" "totals_2000120120001231.grid"
  [373] "totals_2001010120010131.grid" "totals_2001020120010228.grid"
  [375] "totals_2001030120010331.grid" "totals_2001040120010430.grid"
  [377] "totals_2001050120010531.grid" "totals_2001060120010630.grid"
  [379] "totals_2001070120010731.grid" "totals_2001080120010831.grid"
  [381] "totals_2001090120010930.grid" "totals_2001100120011031.grid"
  [383] "totals_2001110120011130.grid" "totals_2001120120011231.grid"
  [385] "totals_2002010120020131.grid" "totals_2002020120020228.grid"
  [387] "totals_2002030120020331.grid" "totals_2002040120020430.grid"
  [389] "totals_2002050120020531.grid" "totals_2002060120020630.grid"
  [391] "totals_2002070120020731.grid" "totals_2002080120020831.grid"
  [393] "totals_2002090120020930.grid" "totals_2002100120021031.grid"
  [395] "totals_2002110120021130.grid" "totals_2002120120021231.grid"
  [397] "totals_2003010120030131.grid" "totals_2003020120030228.grid"
  [399] "totals_2003030120030331.grid" "totals_2003040120030430.grid"
  [401] "totals_2003050120030531.grid" "totals_2003060120030630.grid"
  [403] "totals_2003070120030731.grid" "totals_2003080120030831.grid"
  [405] "totals_2003090120030930.grid" "totals_2003100120031031.grid"
  [407] "totals_2003110120031130.grid" "totals_2003120120031231.grid"
  [409] "totals_2004010120040131.grid" "totals_2004020120040229.grid"
  [411] "totals_2004030120040331.grid" "totals_2004040120040430.grid"
  [413] "totals_2004050120040531.grid" "totals_2004060120040630.grid"
  [415] "totals_2004070120040731.grid" "totals_2004080120040831.grid"
  [417] "totals_2004090120040930.grid" "totals_2004100120041031.grid"
  [419] "totals_2004110120041130.grid" "totals_2004120120041231.grid"
  [421] "totals_2005010120050131.grid" "totals_2005020120050228.grid"
  [423] "totals_2005030120050331.grid" "totals_2005040120050430.grid"
  [425] "totals_2005050120050531.grid" "totals_2005060120050630.grid"
  [427] "totals_2005070120050731.grid" "totals_2005080120050831.grid"
  [429] "totals_2005090120050930.grid" "totals_2005100120051031.grid"
  [431] "totals_2005110120051130.grid" "totals_2005120120051231.grid"
  [433] "totals_2006010120060131.grid" "totals_2006020120060228.grid"
  [435] "totals_2006030120060331.grid" "totals_2006040120060430.grid"
  [437] "totals_2006050120060531.grid" "totals_2006060120060630.grid"
  [439] "totals_2006070120060731.grid" "totals_2006080120060831.grid"
  [441] "totals_2006090120060930.grid" "totals_2006100120061031.grid"
  [443] "totals_2006110120061130.grid" "totals_2006120120061231.grid"
  [445] "totals_2007010120070131.grid" "totals_2007020120070228.grid"
  [447] "totals_2007030120070331.grid" "totals_2007040120070430.grid"
  [449] "totals_2007050120070531.grid" "totals_2007060120070630.grid"
  [451] "totals_2007070120070731.grid" "totals_2007080120070831.grid"
  [453] "totals_2007090120070930.grid" "totals_2007100120071031.grid"
  [455] "totals_2007110120071130.grid" "totals_2007120120071231.grid"
  [457] "totals_2008010120080131.grid" "totals_2008020120080229.grid"
  [459] "totals_2008030120080331.grid" "totals_2008040120080430.grid"
  [461] "totals_2008050120080531.grid" "totals_2008060120080630.grid"
  [463] "totals_2008070120080731.grid" "totals_2008080120080831.grid"
  [465] "totals_2008090120080930.grid" "totals_2008100120081031.grid"
  [467] "totals_2008110120081130.grid" "totals_2008120120081231.grid"
  [469] "totals_2009010120090131.grid" "totals_2009020120090228.grid"
  [471] "totals_2009030120090331.grid" "totals_2009040120090430.grid"
  [473] "totals_2009050120090531.grid" "totals_2009060120090630.grid"
  [475] "totals_2009070120090731.grid" "totals_2009080120090831.grid"
  [477] "totals_2009090120090930.grid" "totals_2009100120091031.grid"
  [479] "totals_2009110120091130.grid" "totals_2009120120091231.grid"
  [481] "totals_2010010120100131.grid" "totals_2010020120100228.grid"
  [483] "totals_2010030120100331.grid" "totals_2010040120100430.grid"
  [485] "totals_2010050120100531.grid" "totals_2010060120100630.grid"
  [487] "totals_2010070120100731.grid" "totals_2010080120100831.grid"
  [489] "totals_2010090120100930.grid" "totals_2010100120101031.grid"
  [491] "totals_2010110120101130.grid" "totals_2010120120101231.grid"
  [493] "totals_2011010120110131.grid" "totals_2011020120110228.grid"
  [495] "totals_2011030120110331.grid" "totals_2011040120110430.grid"
  [497] "totals_2011050120110531.grid" "totals_2011060120110630.grid"
  [499] "totals_2011070120110731.grid" "totals_2011080120110831.grid"
  [501] "totals_2011090120110930.grid" "totals_2011100120111031.grid"
  [503] "totals_2011110120111130.grid" "totals_2011120120111231.grid"
  [505] "totals_2012010120120131.grid" "totals_2012020120120229.grid"
  [507] "totals_2012030120120331.grid" "totals_2012040120120430.grid"
  [509] "totals_2012050120120531.grid" "totals_2012060120120630.grid"
  [511] "totals_2012070120120731.grid" "totals_2012080120120831.grid"
  [513] "totals_2012090120120930.grid" "totals_2012100120121031.grid"
  [515] "totals_2012110120121130.grid" "totals_2012120120121231.grid"
  [517] "totals_2013010120130131.grid" "totals_2013020120130228.grid"
  [519] "totals_2013030120130331.grid" "totals_2013040120130430.grid"
  [521] "totals_2013050120130531.grid" "totals_2013060120130630.grid"
  [523] "totals_2013070120130731.grid" "totals_2013080120130831.grid"
  [525] "totals_2013090120130930.grid" "totals_2013100120131031.grid"
  [527] "totals_2013110120131130.grid" "totals_2013120120131231.grid"
  [529] "totals_2014010120140131.grid"
  > i = 1
  >   #i <- 1 ## for stepping thru
  >   gridname <- cfiles[[i]]
  >   r <- raster(gridname)
  >   e <- extract(r, shp, df=T)
  >   e1 <- shp
  >   e1@data$values <- e[,2]
  >   e1@data$gridname <- gridname
  > e1@data
         lon       lat values                     gridname
  1 145.4185 -16.17003   70.9 totals_1970010119700131.grid
  > cfiles
    [1] "totals_1970010119700131.grid" "totals_1970020119700228.grid"
    [3] "totals_1970030119700331.grid" "totals_1970040119700430.grid"
    [5] "totals_1970050119700531.grid" "totals_1970060119700630.grid"
    [7] "totals_1970070119700731.grid" "totals_1970080119700831.grid"
    [9] "totals_1970090119700930.grid" "totals_1970100119701031.grid"
   [11] "totals_1970110119701130.grid" "totals_1970120119701231.grid"
   [13] "totals_1971010119710131.grid" "totals_1971020119710228.grid"
   [15] "totals_1971030119710331.grid" "totals_1971040119710430.grid"
   [17] "totals_1971050119710531.grid" "totals_1971060119710630.grid"
   [19] "totals_1971070119710731.grid" "totals_1971080119710831.grid"
   [21] "totals_1971090119710930.grid" "totals_1971100119711031.grid"
   [23] "totals_1971110119711130.grid" "totals_1971120119711231.grid"
   [25] "totals_1972010119720131.grid" "totals_1972020119720229.grid"
   [27] "totals_1972030119720331.grid" "totals_1972040119720430.grid"
   [29] "totals_1972050119720531.grid" "totals_1972060119720630.grid"
   [31] "totals_1972070119720731.grid" "totals_1972080119720831.grid"
   [33] "totals_1972090119720930.grid" "totals_1972100119721031.grid"
   [35] "totals_1972110119721130.grid" "totals_1972120119721231.grid"
   [37] "totals_1973010119730131.grid" "totals_1973020119730228.grid"
   [39] "totals_1973030119730331.grid" "totals_1973040119730430.grid"
   [41] "totals_1973050119730531.grid" "totals_1973060119730630.grid"
   [43] "totals_1973070119730731.grid" "totals_1973080119730831.grid"
   [45] "totals_1973090119730930.grid" "totals_1973100119731031.grid"
   [47] "totals_1973110119731130.grid" "totals_1973120119731231.grid"
   [49] "totals_1974010119740131.grid" "totals_1974020119740228.grid"
   [51] "totals_1974030119740331.grid" "totals_1974040119740430.grid"
   [53] "totals_1974050119740531.grid" "totals_1974060119740630.grid"
   [55] "totals_1974070119740731.grid" "totals_1974080119740831.grid"
   [57] "totals_1974090119740930.grid" "totals_1974100119741031.grid"
   [59] "totals_1974110119741130.grid" "totals_1974120119741231.grid"
   [61] "totals_1975010119750131.grid" "totals_1975020119750228.grid"
   [63] "totals_1975030119750331.grid" "totals_1975040119750430.grid"
   [65] "totals_1975050119750531.grid" "totals_1975060119750630.grid"
   [67] "totals_1975070119750731.grid" "totals_1975080119750831.grid"
   [69] "totals_1975090119750930.grid" "totals_1975100119751031.grid"
   [71] "totals_1975110119751130.grid" "totals_1975120119751231.grid"
   [73] "totals_1976010119760131.grid" "totals_1976020119760229.grid"
   [75] "totals_1976030119760331.grid" "totals_1976040119760430.grid"
   [77] "totals_1976050119760531.grid" "totals_1976060119760630.grid"
   [79] "totals_1976070119760731.grid" "totals_1976080119760831.grid"
   [81] "totals_1976090119760930.grid" "totals_1976100119761031.grid"
   [83] "totals_1976110119761130.grid" "totals_1976120119761231.grid"
   [85] "totals_1977010119770131.grid" "totals_1977020119770228.grid"
   [87] "totals_1977030119770331.grid" "totals_1977040119770430.grid"
   [89] "totals_1977050119770531.grid" "totals_1977060119770630.grid"
   [91] "totals_1977070119770731.grid" "totals_1977080119770831.grid"
   [93] "totals_1977090119770930.grid" "totals_1977100119771031.grid"
   [95] "totals_1977110119771130.grid" "totals_1977120119771231.grid"
   [97] "totals_1978010119780131.grid" "totals_1978020119780228.grid"
   [99] "totals_1978030119780331.grid" "totals_1978040119780430.grid"
  [101] "totals_1978050119780531.grid" "totals_1978060119780630.grid"
  [103] "totals_1978070119780731.grid" "totals_1978080119780831.grid"
  [105] "totals_1978090119780930.grid" "totals_1978100119781031.grid"
  [107] "totals_1978110119781130.grid" "totals_1978120119781231.grid"
  [109] "totals_1979010119790131.grid" "totals_1979020119790228.grid"
  [111] "totals_1979030119790331.grid" "totals_1979040119790430.grid"
  [113] "totals_1979050119790531.grid" "totals_1979060119790630.grid"
  [115] "totals_1979070119790731.grid" "totals_1979080119790831.grid"
  [117] "totals_1979090119790930.grid" "totals_1979100119791031.grid"
  [119] "totals_1979110119791130.grid" "totals_1979120119791231.grid"
  [121] "totals_1980010119800131.grid" "totals_1980020119800229.grid"
  [123] "totals_1980030119800331.grid" "totals_1980040119800430.grid"
  [125] "totals_1980050119800531.grid" "totals_1980060119800630.grid"
  [127] "totals_1980070119800731.grid" "totals_1980080119800831.grid"
  [129] "totals_1980090119800930.grid" "totals_1980100119801031.grid"
  [131] "totals_1980110119801130.grid" "totals_1980120119801231.grid"
  [133] "totals_1981010119810131.grid" "totals_1981020119810228.grid"
  [135] "totals_1981030119810331.grid" "totals_1981040119810430.grid"
  [137] "totals_1981050119810531.grid" "totals_1981060119810630.grid"
  [139] "totals_1981070119810731.grid" "totals_1981080119810831.grid"
  [141] "totals_1981090119810930.grid" "totals_1981100119811031.grid"
  [143] "totals_1981110119811130.grid" "totals_1981120119811231.grid"
  [145] "totals_1982010119820131.grid" "totals_1982020119820228.grid"
  [147] "totals_1982030119820331.grid" "totals_1982040119820430.grid"
  [149] "totals_1982050119820531.grid" "totals_1982060119820630.grid"
  [151] "totals_1982070119820731.grid" "totals_1982080119820831.grid"
  [153] "totals_1982090119820930.grid" "totals_1982100119821031.grid"
  [155] "totals_1982110119821130.grid" "totals_1982120119821231.grid"
  [157] "totals_1983010119830131.grid" "totals_1983020119830228.grid"
  [159] "totals_1983030119830331.grid" "totals_1983040119830430.grid"
  [161] "totals_1983050119830531.grid" "totals_1983060119830630.grid"
  [163] "totals_1983070119830731.grid" "totals_1983080119830831.grid"
  [165] "totals_1983090119830930.grid" "totals_1983100119831031.grid"
  [167] "totals_1983110119831130.grid" "totals_1983120119831231.grid"
  [169] "totals_1984010119840131.grid" "totals_1984020119840229.grid"
  [171] "totals_1984030119840331.grid" "totals_1984040119840430.grid"
  [173] "totals_1984050119840531.grid" "totals_1984060119840630.grid"
  [175] "totals_1984070119840731.grid" "totals_1984080119840831.grid"
  [177] "totals_1984090119840930.grid" "totals_1984100119841031.grid"
  [179] "totals_1984110119841130.grid" "totals_1984120119841231.grid"
  [181] "totals_1985010119850131.grid" "totals_1985020119850228.grid"
  [183] "totals_1985030119850331.grid" "totals_1985040119850430.grid"
  [185] "totals_1985050119850531.grid" "totals_1985060119850630.grid"
  [187] "totals_1985070119850731.grid" "totals_1985080119850831.grid"
  [189] "totals_1985090119850930.grid" "totals_1985100119851031.grid"
  [191] "totals_1985110119851130.grid" "totals_1985120119851231.grid"
  [193] "totals_1986010119860131.grid" "totals_1986020119860228.grid"
  [195] "totals_1986030119860331.grid" "totals_1986040119860430.grid"
  [197] "totals_1986050119860531.grid" "totals_1986060119860630.grid"
  [199] "totals_1986070119860731.grid" "totals_1986080119860831.grid"
  [201] "totals_1986090119860930.grid" "totals_1986100119861031.grid"
  [203] "totals_1986110119861130.grid" "totals_1986120119861231.grid"
  [205] "totals_1987010119870131.grid" "totals_1987020119870228.grid"
  [207] "totals_1987030119870331.grid" "totals_1987040119870430.grid"
  [209] "totals_1987050119870531.grid" "totals_1987060119870630.grid"
  [211] "totals_1987070119870731.grid" "totals_1987080119870831.grid"
  [213] "totals_1987090119870930.grid" "totals_1987100119871031.grid"
  [215] "totals_1987110119871130.grid" "totals_1987120119871231.grid"
  [217] "totals_1988010119880131.grid" "totals_1988020119880229.grid"
  [219] "totals_1988030119880331.grid" "totals_1988040119880430.grid"
  [221] "totals_1988050119880531.grid" "totals_1988060119880630.grid"
  [223] "totals_1988070119880731.grid" "totals_1988080119880831.grid"
  [225] "totals_1988090119880930.grid" "totals_1988100119881031.grid"
  [227] "totals_1988110119881130.grid" "totals_1988120119881231.grid"
  [229] "totals_1989010119890131.grid" "totals_1989020119890228.grid"
  [231] "totals_1989030119890331.grid" "totals_1989040119890430.grid"
  [233] "totals_1989050119890531.grid" "totals_1989060119890630.grid"
  [235] "totals_1989070119890731.grid" "totals_1989080119890831.grid"
  [237] "totals_1989090119890930.grid" "totals_1989100119891031.grid"
  [239] "totals_1989110119891130.grid" "totals_1989120119891231.grid"
  [241] "totals_1990010119900131.grid" "totals_1990020119900228.grid"
  [243] "totals_1990030119900331.grid" "totals_1990040119900430.grid"
  [245] "totals_1990050119900531.grid" "totals_1990060119900630.grid"
  [247] "totals_1990070119900731.grid" "totals_1990080119900831.grid"
  [249] "totals_1990090119900930.grid" "totals_1990100119901031.grid"
  [251] "totals_1990110119901130.grid" "totals_1990120119901231.grid"
  [253] "totals_1991010119910131.grid" "totals_1991020119910228.grid"
  [255] "totals_1991030119910331.grid" "totals_1991040119910430.grid"
  [257] "totals_1991050119910531.grid" "totals_1991060119910630.grid"
  [259] "totals_1991070119910731.grid" "totals_1991080119910831.grid"
  [261] "totals_1991090119910930.grid" "totals_1991100119911031.grid"
  [263] "totals_1991110119911130.grid" "totals_1991120119911231.grid"
  [265] "totals_1992010119920131.grid" "totals_1992020119920229.grid"
  [267] "totals_1992030119920331.grid" "totals_1992040119920430.grid"
  [269] "totals_1992050119920531.grid" "totals_1992060119920630.grid"
  [271] "totals_1992070119920731.grid" "totals_1992080119920831.grid"
  [273] "totals_1992090119920930.grid" "totals_1992100119921031.grid"
  [275] "totals_1992110119921130.grid" "totals_1992120119921231.grid"
  [277] "totals_1993010119930131.grid" "totals_1993020119930228.grid"
  [279] "totals_1993030119930331.grid" "totals_1993040119930430.grid"
  [281] "totals_1993050119930531.grid" "totals_1993060119930630.grid"
  [283] "totals_1993070119930731.grid" "totals_1993080119930831.grid"
  [285] "totals_1993090119930930.grid" "totals_1993100119931031.grid"
  [287] "totals_1993110119931130.grid" "totals_1993120119931231.grid"
  [289] "totals_1994010119940131.grid" "totals_1994020119940228.grid"
  [291] "totals_1994030119940331.grid" "totals_1994040119940430.grid"
  [293] "totals_1994050119940531.grid" "totals_1994060119940630.grid"
  [295] "totals_1994070119940731.grid" "totals_1994080119940831.grid"
  [297] "totals_1994090119940930.grid" "totals_1994100119941031.grid"
  [299] "totals_1994110119941130.grid" "totals_1994120119941231.grid"
  [301] "totals_1995010119950131.grid" "totals_1995020119950228.grid"
  [303] "totals_1995030119950331.grid" "totals_1995040119950430.grid"
  [305] "totals_1995050119950531.grid" "totals_1995060119950630.grid"
  [307] "totals_1995070119950731.grid" "totals_1995080119950831.grid"
  [309] "totals_1995090119950930.grid" "totals_1995100119951031.grid"
  [311] "totals_1995110119951130.grid" "totals_1995120119951231.grid"
  [313] "totals_1996010119960131.grid" "totals_1996020119960229.grid"
  [315] "totals_1996030119960331.grid" "totals_1996040119960430.grid"
  [317] "totals_1996050119960531.grid" "totals_1996060119960630.grid"
  [319] "totals_1996070119960731.grid" "totals_1996080119960831.grid"
  [321] "totals_1996090119960930.grid" "totals_1996100119961031.grid"
  [323] "totals_1996110119961130.grid" "totals_1996120119961231.grid"
  [325] "totals_1997010119970131.grid" "totals_1997020119970228.grid"
  [327] "totals_1997030119970331.grid" "totals_1997040119970430.grid"
  [329] "totals_1997050119970531.grid" "totals_1997060119970630.grid"
  [331] "totals_1997070119970731.grid" "totals_1997080119970831.grid"
  [333] "totals_1997090119970930.grid" "totals_1997100119971031.grid"
  [335] "totals_1997110119971130.grid" "totals_1997120119971231.grid"
  [337] "totals_1998010119980131.grid" "totals_1998020119980228.grid"
  [339] "totals_1998030119980331.grid" "totals_1998040119980430.grid"
  [341] "totals_1998050119980531.grid" "totals_1998060119980630.grid"
  [343] "totals_1998070119980731.grid" "totals_1998080119980831.grid"
  [345] "totals_1998090119980930.grid" "totals_1998100119981031.grid"
  [347] "totals_1998110119981130.grid" "totals_1998120119981231.grid"
  [349] "totals_1999010119990131.grid" "totals_1999020119990228.grid"
  [351] "totals_1999030119990331.grid" "totals_1999040119990430.grid"
  [353] "totals_1999050119990531.grid" "totals_1999060119990630.grid"
  [355] "totals_1999070119990731.grid" "totals_1999080119990831.grid"
  [357] "totals_1999090119990930.grid" "totals_1999100119991031.grid"
  [359] "totals_1999110119991130.grid" "totals_1999120119991231.grid"
  [361] "totals_2000010120000131.grid" "totals_2000020120000229.grid"
  [363] "totals_2000030120000331.grid" "totals_2000040120000430.grid"
  [365] "totals_2000050120000531.grid" "totals_2000060120000630.grid"
  [367] "totals_2000070120000731.grid" "totals_2000080120000831.grid"
  [369] "totals_2000090120000930.grid" "totals_2000100120001031.grid"
  [371] "totals_2000110120001130.grid" "totals_2000120120001231.grid"
  [373] "totals_2001010120010131.grid" "totals_2001020120010228.grid"
  [375] "totals_2001030120010331.grid" "totals_2001040120010430.grid"
  [377] "totals_2001050120010531.grid" "totals_2001060120010630.grid"
  [379] "totals_2001070120010731.grid" "totals_2001080120010831.grid"
  [381] "totals_2001090120010930.grid" "totals_2001100120011031.grid"
  [383] "totals_2001110120011130.grid" "totals_2001120120011231.grid"
  [385] "totals_2002010120020131.grid" "totals_2002020120020228.grid"
  [387] "totals_2002030120020331.grid" "totals_2002040120020430.grid"
  [389] "totals_2002050120020531.grid" "totals_2002060120020630.grid"
  [391] "totals_2002070120020731.grid" "totals_2002080120020831.grid"
  [393] "totals_2002090120020930.grid" "totals_2002100120021031.grid"
  [395] "totals_2002110120021130.grid" "totals_2002120120021231.grid"
  [397] "totals_2003010120030131.grid" "totals_2003020120030228.grid"
  [399] "totals_2003030120030331.grid" "totals_2003040120030430.grid"
  [401] "totals_2003050120030531.grid" "totals_2003060120030630.grid"
  [403] "totals_2003070120030731.grid" "totals_2003080120030831.grid"
  [405] "totals_2003090120030930.grid" "totals_2003100120031031.grid"
  [407] "totals_2003110120031130.grid" "totals_2003120120031231.grid"
  [409] "totals_2004010120040131.grid" "totals_2004020120040229.grid"
  [411] "totals_2004030120040331.grid" "totals_2004040120040430.grid"
  [413] "totals_2004050120040531.grid" "totals_2004060120040630.grid"
  [415] "totals_2004070120040731.grid" "totals_2004080120040831.grid"
  [417] "totals_2004090120040930.grid" "totals_2004100120041031.grid"
  [419] "totals_2004110120041130.grid" "totals_2004120120041231.grid"
  [421] "totals_2005010120050131.grid" "totals_2005020120050228.grid"
  [423] "totals_2005030120050331.grid" "totals_2005040120050430.grid"
  [425] "totals_2005050120050531.grid" "totals_2005060120050630.grid"
  [427] "totals_2005070120050731.grid" "totals_2005080120050831.grid"
  [429] "totals_2005090120050930.grid" "totals_2005100120051031.grid"
  [431] "totals_2005110120051130.grid" "totals_2005120120051231.grid"
  [433] "totals_2006010120060131.grid" "totals_2006020120060228.grid"
  [435] "totals_2006030120060331.grid" "totals_2006040120060430.grid"
  [437] "totals_2006050120060531.grid" "totals_2006060120060630.grid"
  [439] "totals_2006070120060731.grid" "totals_2006080120060831.grid"
  [441] "totals_2006090120060930.grid" "totals_2006100120061031.grid"
  [443] "totals_2006110120061130.grid" "totals_2006120120061231.grid"
  [445] "totals_2007010120070131.grid" "totals_2007020120070228.grid"
  [447] "totals_2007030120070331.grid" "totals_2007040120070430.grid"
  [449] "totals_2007050120070531.grid" "totals_2007060120070630.grid"
  [451] "totals_2007070120070731.grid" "totals_2007080120070831.grid"
  [453] "totals_2007090120070930.grid" "totals_2007100120071031.grid"
  [455] "totals_2007110120071130.grid" "totals_2007120120071231.grid"
  [457] "totals_2008010120080131.grid" "totals_2008020120080229.grid"
  [459] "totals_2008030120080331.grid" "totals_2008040120080430.grid"
  [461] "totals_2008050120080531.grid" "totals_2008060120080630.grid"
  [463] "totals_2008070120080731.grid" "totals_2008080120080831.grid"
  [465] "totals_2008090120080930.grid" "totals_2008100120081031.grid"
  [467] "totals_2008110120081130.grid" "totals_2008120120081231.grid"
  [469] "totals_2009010120090131.grid" "totals_2009020120090228.grid"
  [471] "totals_2009030120090331.grid" "totals_2009040120090430.grid"
  [473] "totals_2009050120090531.grid" "totals_2009060120090630.grid"
  [475] "totals_2009070120090731.grid" "totals_2009080120090831.grid"
  [477] "totals_2009090120090930.grid" "totals_2009100120091031.grid"
  [479] "totals_2009110120091130.grid" "totals_2009120120091231.grid"
  [481] "totals_2010010120100131.grid" "totals_2010020120100228.grid"
  [483] "totals_2010030120100331.grid" "totals_2010040120100430.grid"
  [485] "totals_2010050120100531.grid" "totals_2010060120100630.grid"
  [487] "totals_2010070120100731.grid" "totals_2010080120100831.grid"
  [489] "totals_2010090120100930.grid" "totals_2010100120101031.grid"
  [491] "totals_2010110120101130.grid" "totals_2010120120101231.grid"
  [493] "totals_2011010120110131.grid" "totals_2011020120110228.grid"
  [495] "totals_2011030120110331.grid" "totals_2011040120110430.grid"
  [497] "totals_2011050120110531.grid" "totals_2011060120110630.grid"
  [499] "totals_2011070120110731.grid" "totals_2011080120110831.grid"
  [501] "totals_2011090120110930.grid" "totals_2011100120111031.grid"
  [503] "totals_2011110120111130.grid" "totals_2011120120111231.grid"
  [505] "totals_2012010120120131.grid" "totals_2012020120120229.grid"
  [507] "totals_2012030120120331.grid" "totals_2012040120120430.grid"
  [509] "totals_2012050120120531.grid" "totals_2012060120120630.grid"
  [511] "totals_2012070120120731.grid" "totals_2012080120120831.grid"
  [513] "totals_2012090120120930.grid" "totals_2012100120121031.grid"
  [515] "totals_2012110120121130.grid" "totals_2012120120121231.grid"
  [517] "totals_2013010120130131.grid" "totals_2013020120130228.grid"
  [519] "totals_2013030120130331.grid" "totals_2013040120130430.grid"
  [521] "totals_2013050120130531.grid" "totals_2013060120130630.grid"
  [523] "totals_2013070120130731.grid" "totals_2013080120130831.grid"
  [525] "totals_2013090120130930.grid" "totals_2013100120131031.grid"
  [527] "totals_2013110120131130.grid" "totals_2013120120131231.grid"
  [529] "totals_2014010120140131.grid"
  > cfiles[1:10]
   [1] "totals_1970010119700131.grid" "totals_1970020119700228.grid"
   [3] "totals_1970030119700331.grid" "totals_1970040119700430.grid"
   [5] "totals_1970050119700531.grid" "totals_1970060119700630.grid"
   [7] "totals_1970070119700731.grid" "totals_1970080119700831.grid"
   [9] "totals_1970090119700930.grid" "totals_1970100119701031.grid"
  > locn <- geocode("prospect dam")
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=prospect+dam&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  > locn
         lon       lat
  1 26.90208 -32.00395
  > locn <- geocode("prospect reservoir nsw")
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=prospect+reservoir+nsw&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  > locn
         lon       lat
  1 150.8929 -33.82107
  > shp <- SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
  +                               proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  >   gridname <- cfiles[[i]]
  >   r <- raster(gridname)
  >   e <- extract(r, shp, df=T)
  >   e1 <- shp
  >   e1@data$values <- e[,2]
  >   e1@data$gridname <- gridname
  > e1@data 
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  > locn <- geocode(c("prospect reservoir nsw", "parkes nsw"))
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=prospect+reservoir+nsw&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=parkes+nsw&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  > locn
         lon       lat
  1 150.8929 -33.82107
  2 148.1748 -33.13786
  > shp <- SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
  +                               proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  >   e1 <- shp
  >   e1@data$values <- e[,2]
  >   e1@data$gridname <- gridname
  > e1@data 
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 148.1748 -33.13786  110.2 totals_1970010119700131.grid
  > locn <- geocode(c("prospect reservoir nsw", "west wyalong nsw"))
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=prospect+reservoir+nsw&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=west+wyalong+nsw&sensor=false
  Google Maps API Terms of Service : http://developers.google.com/maps/terms
  > locn
         lon       lat
  1 150.8929 -33.82107
  2 147.2053 -33.92350
  > shp <- SpatialPointsDataFrame(cbind(locn$lon,locn$lat),locn,
  +                               proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  >   e <- extract(r, shp, df=T)
  >   e1 <- shp
  >   e1@data$values <- e[,2]
  >   e1@data$gridname <- gridname
  > e1@data
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 147.2053 -33.92350   19.1 totals_1970010119700131.grid
  > dir(pattern = "csv")
  character(0)
  > for (i in seq_len(length(cfiles))) {
  +   gridname <- cfiles[[i]]
  +   r <- raster(gridname)
  +   e <- extract(r, shp, df=T)
  +   e1 <- shp
  +   e1@data$values <- e[,2]
  +   e1@data$gridname <- gridname
  +   write.table(e1@data,"output.csv",
  +     col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE)
  + }
  > dat <- read.csv("output.csv")
  > head(dat)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 147.2053 -33.92350   19.1 totals_1970010119700131.grid
  3 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  4 147.2053 -33.92350    2.0 totals_1970020119700228.grid
  5 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  6 147.2053 -33.92350   45.2 totals_1970030119700331.grid
  > library(sqldf)
  Loading required package: gsubfn
  Loading required package: proto
  Loading required package: RSQLite
  Loading required package: DBI
  > qc <- sqldf("select * from dat where lon = 150.8929")
  Loading required package: tcltk
  > head(qc)
  [1] lon      lat      values   gridname
  <0 rows> (or 0-length row.names)
  > qc <- sqldf("select * from dat where lon > 150")
  > head(qc)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  3 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  4 150.8929 -33.82107   28.1 totals_1970040119700430.grid
  5 150.8929 -33.82107   16.2 totals_1970050119700531.grid
  6 150.8929 -33.82107   16.9 totals_1970060119700630.grid
  > tail(qc)
           lon       lat values                     gridname
  524 150.8929 -33.82107    7.8 totals_2013080120130831.grid
  525 150.8929 -33.82107   18.2 totals_2013090120130930.grid
  526 150.8929 -33.82107    7.6 totals_2013100120131031.grid
  527 150.8929 -33.82107  156.3 totals_2013110120131130.grid
  528 150.8929 -33.82107   31.1 totals_2013120120131231.grid
  529 150.8929 -33.82107   19.5 totals_2014010120140131.grid
  > qc2 <- read.table("clipboard", header  = F)
  > head(qc2)
            V1    V2   V3 V4   V5 V6
  1 IDCJAC0001 67019 1970  1 95.6  Y
  2 IDCJAC0001 67019 1970  2 38.9  Y
  3 IDCJAC0001 67019 1970  3 75.6  Y
  4 IDCJAC0001 67019 1970  4 25.6  Y
  5 IDCJAC0001 67019 1970  5 17.1  Y
  6 IDCJAC0001 67019 1970  6 16.0  Y
  > qc3 <- cbind(qc, qc2)
  Error in data.frame(..., check.names = FALSE) : 
    arguments imply differing number of rows: 529, 527
  > tail(qc2)
              V1    V2   V3 V4    V5 V6
  522 IDCJAC0001 67019 2013  8   8.6  Y
  523 IDCJAC0001 67019 2013  9  20.9  Y
  524 IDCJAC0001 67019 2013 10   9.7  Y
  525 IDCJAC0001 67019 2013 11 179.1  Y
  526 IDCJAC0001 67019 2013 12  32.3  Y
  527 IDCJAC0001 67019 2014  1  20.9  N
  > dat$raster_layer <- as.character(dat$gridname)
  > dat$date <- matrix(unlist(strsplit(dat$raster_layer, "_")), ncol = 2, byrow=TRUE)[,2]
  > head(dat)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 147.2053 -33.92350   19.1 totals_1970010119700131.grid
  3 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  4 147.2053 -33.92350    2.0 totals_1970020119700228.grid
  5 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  6 147.2053 -33.92350   45.2 totals_1970030119700331.grid
                    raster_layer                  date
  1 totals_1970010119700131.grid 1970010119700131.grid
  2 totals_1970010119700131.grid 1970010119700131.grid
  3 totals_1970020119700228.grid 1970020119700228.grid
  4 totals_1970020119700228.grid 1970020119700228.grid
  5 totals_1970030119700331.grid 1970030119700331.grid
  6 totals_1970030119700331.grid 1970030119700331.grid
  > dat$date <- gsub(".grid","",dat$date)
  > head(dat )
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 147.2053 -33.92350   19.1 totals_1970010119700131.grid
  3 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  4 147.2053 -33.92350    2.0 totals_1970020119700228.grid
  5 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  6 147.2053 -33.92350   45.2 totals_1970030119700331.grid
                    raster_layer             date
  1 totals_1970010119700131.grid 1970010119700131
  2 totals_1970010119700131.grid 1970010119700131
  3 totals_1970020119700228.grid 1970020119700228
  4 totals_1970020119700228.grid 1970020119700228
  5 totals_1970030119700331.grid 1970030119700331
  6 totals_1970030119700331.grid 1970030119700331
  > dat$date <- paste(substr(dat$date,1,4), substr(dat$date,5,6), substr(dat$date,7,8), sep = "-")
  > head(dat )
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 147.2053 -33.92350   19.1 totals_1970010119700131.grid
  3 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  4 147.2053 -33.92350    2.0 totals_1970020119700228.grid
  5 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  6 147.2053 -33.92350   45.2 totals_1970030119700331.grid
                    raster_layer       date
  1 totals_1970010119700131.grid 1970-01-01
  2 totals_1970010119700131.grid 1970-01-01
  3 totals_1970020119700228.grid 1970-02-01
  4 totals_1970020119700228.grid 1970-02-01
  5 totals_1970030119700331.grid 1970-03-01
  6 totals_1970030119700331.grid 1970-03-01
  > dat$year <- substr(dat$date,1,4)
  > dat$month <- substr(dat$date,5,6)
  > qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
    qc.month = qc2.V4")
  qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
  +   qc.month = qc2.V4")
  Error in sqliteSendQuery(con, statement, bind.data) : 
    error in statement: no such column: qc.year
  > qc <- sqldf("select * from dat where lon > 150")
  > qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
    qc.month = qc2.V4")
  qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
  +   qc.month = qc2.V4")
  > head(qc3)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  3 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  4 150.8929 -33.82107   28.1 totals_1970040119700430.grid
  5 150.8929 -33.82107   16.2 totals_1970050119700531.grid
  6 150.8929 -33.82107   16.9 totals_1970060119700630.grid
                    raster_layer       date year month   V1 V2 V3 V4 V5   V6
  1 totals_1970010119700131.grid 1970-01-01 1970    -0 <NA> NA NA NA NA <NA>
  2 totals_1970020119700228.grid 1970-02-01 1970    -0 <NA> NA NA NA NA <NA>
  3 totals_1970030119700331.grid 1970-03-01 1970    -0 <NA> NA NA NA NA <NA>
  4 totals_1970040119700430.grid 1970-04-01 1970    -0 <NA> NA NA NA NA <NA>
  5 totals_1970050119700531.grid 1970-05-01 1970    -0 <NA> NA NA NA NA <NA>
  6 totals_1970060119700630.grid 1970-06-01 1970    -0 <NA> NA NA NA NA <NA>
  > dat$month <- substr(dat$date,6,7)
  > qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
    qc.month = qc2.V4")
  qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
  +   qc.month = qc2.V4")
  > head(qc3)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  3 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  4 150.8929 -33.82107   28.1 totals_1970040119700430.grid
  5 150.8929 -33.82107   16.2 totals_1970050119700531.grid
  6 150.8929 -33.82107   16.9 totals_1970060119700630.grid
                    raster_layer       date year month   V1 V2 V3 V4 V5   V6
  1 totals_1970010119700131.grid 1970-01-01 1970    -0 <NA> NA NA NA NA <NA>
  2 totals_1970020119700228.grid 1970-02-01 1970    -0 <NA> NA NA NA NA <NA>
  3 totals_1970030119700331.grid 1970-03-01 1970    -0 <NA> NA NA NA NA <NA>
  4 totals_1970040119700430.grid 1970-04-01 1970    -0 <NA> NA NA NA NA <NA>
  5 totals_1970050119700531.grid 1970-05-01 1970    -0 <NA> NA NA NA NA <NA>
  6 totals_1970060119700630.grid 1970-06-01 1970    -0 <NA> NA NA NA NA <NA>
  > qc <- sqldf("select * from dat where lon > 150")
  > qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
    qc.month = qc2.V4")
  qc3 <- sqldf("select * from qc left join qc2 on qc.year = qc2.V3 and
  +   qc.month = qc2.V4")
  > head(qc3)
         lon       lat values                     gridname
  1 150.8929 -33.82107  110.2 totals_1970010119700131.grid
  2 150.8929 -33.82107   45.8 totals_1970020119700228.grid
  3 150.8929 -33.82107   69.6 totals_1970030119700331.grid
  4 150.8929 -33.82107   28.1 totals_1970040119700430.grid
  5 150.8929 -33.82107   16.2 totals_1970050119700531.grid
  6 150.8929 -33.82107   16.9 totals_1970060119700630.grid
                    raster_layer       date year month         V1    V2   V3 V4
  1 totals_1970010119700131.grid 1970-01-01 1970    01 IDCJAC0001 67019 1970  1
  2 totals_1970020119700228.grid 1970-02-01 1970    02 IDCJAC0001 67019 1970  2
  3 totals_1970030119700331.grid 1970-03-01 1970    03 IDCJAC0001 67019 1970  3
  4 totals_1970040119700430.grid 1970-04-01 1970    04 IDCJAC0001 67019 1970  4
  5 totals_1970050119700531.grid 1970-05-01 1970    05 IDCJAC0001 67019 1970  5
  6 totals_1970060119700630.grid 1970-06-01 1970    06 IDCJAC0001 67019 1970  6
      V5 V6
  1 95.6  Y
  2 38.9  Y
  3 75.6  Y
  4 25.6  Y
  5 17.1  Y
  6 16.0  Y
  > with(qc3, plot(V5, values))
  > with(qc3, plot(date, values, type = "l"))
  Error in plot.window(...) : need finite 'xlim' values
  In addition: Warning messages:
  1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
  2: In min(x) : no non-missing arguments to min; returning Inf
  3: In max(x) : no non-missing arguments to max; returning -Inf
  > with(qc3, plot(as.Date(date), values, type = "l"))
  > with(qc3, lines(as.Date(date), V5, col = "blue"))
  > windows()
  Error: could not find function "windows"
  > par(mfrow = c(1,2))
  > with(qc3, plot(V5, values))
  > with(qc3, plot(as.Date(date), values, type = "l"))
  > with(qc3, lines(as.Date(date), V5, col = "blue"))
  > qc3[which(qc3$V5 == ""),]
   [1] lon          lat          values       gridname     raster_layer
   [6] date         year         month        V1           V2          
  [11] V3           V4           V5           V6          
  <0 rows> (or 0-length row.names)
  > qc3[which(qc3$V5 == NA),]
   [1] lon          lat          values       gridname     raster_layer
   [6] date         year         month        V1           V2          
  [11] V3           V4           V5           V6          
  <0 rows> (or 0-length row.names)
  > qc3[is.na(qc3$V5),]
           lon       lat values                     gridname
  395 150.8929 -33.82107   15.5 totals_2002110120021130.grid
  397 150.8929 -33.82107   20.5 totals_2003010120030131.grid
                      raster_layer       date year month   V1 V2 V3 V4 V5   V6
  395 totals_2002110120021130.grid 2002-11-01 2002    11 <NA> NA NA NA NA <NA>
  397 totals_2003010120030131.grid 2003-01-01 2003    01 <NA> NA NA NA NA <NA>
  > history(max.show = 25)
  Error in .External2(C_savehistory, file) : no history available to save
  > savehistory()
  Error in .External2(C_savehistory, file) : no history available to save
  >
  
  require(HutchinsonDroughtIndex)
  
  qc4 <- sqldf("select * from qc3 where year < 2014")
  tail(qc4)
  qc4$rain <- qc4$values
  indat <- qc4[,c("date","year","month","rain")]
  str(indat)
  indat$year <- as.numeric(indat$year)
  indat$month <- as.numeric(indat$month)
  
  drt <- drought_index_stations(data=indat,years=length(names(table(qc4$year))))
  head(drt)
  tail(drt)
  str(drt)
   qc3=drt[drt$year>=1979 & drt$year < 1984,]
  qc3$date <- as.Date(qc3$date)
  par(mfrow=c(4,1),mar=c(2.5,2,1.5,1))
   plot(qc3$date,qc3$rain,type='l',main='Prospect Reservoir (67019) NSW: raw monthly rainfall')
   #points(qc3$date,qc3$rain)
   
   lines(qc3$date,qc3$sixmnthtot/6, lwd = 2) #,type='l',main='6-monthly total rainfall')
   points(qc3$date,qc3$sixmnthtot/6)
   
   plot(qc3$date,qc3$index,type='l',main='Rescaled percentiles -4 to +4, -1 is Palmer Index Mild Drought',ylim=c(-4,4))
   points(qc3$date,qc3$index)
   segments(min(qc3$date),-1,max(qc3$date),-1)
   segments(min(qc3$date),0,max(qc3$date),0,lty=2)
   plot(qc3$date,qc3$count,type='l',main='Counts below -1 threshold, count of 5 or more is a drought')
   points(qc3$date,qc3$count)
   segments(min(qc3$date),5,max(qc3$date),5)
   
   plot(qc3$date,qc3$count2,type='l',main='Enhanced counts of months if already passed count of 5 and percentiles less than 50%')
   points(qc3$date,qc3$count2)
   segments(min(qc3$date),5,max(qc3$date),5)
  
